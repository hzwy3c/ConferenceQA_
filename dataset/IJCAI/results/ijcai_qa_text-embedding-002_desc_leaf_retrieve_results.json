{"What is the title of the paper with the id 4090?": {"IJCAI2023>>calls>>Journal Track>>Accepted Papers List>>J5950>>authors>>authors_5>>Subbarao Kambhampati": 0.16932958364486694, "IJCAI2023>>program>>Main Track>>1626>>authors>>authors_1>>Peizheng Li": 0.1696520447731018, "IJCAI2023>>program>>Main Track>>1493>>authors>>authors_3>>Xinning Zhou": 0.17447489500045776, "IJCAI2023>>program>>Main Track>>4480>>title>>Adversarial Contention Resolution Games": 0.17450356483459473, "IJCAI2023>>program>>Main Track>>1493>>authors>>authors_5>>Dong Yan": 0.17941319942474365, "IJCAI2023>>program>>Main Track>>4969>>authors>>authors_1>>Dabin Zhang": 0.1794368028640747, "IJCAI2023>>program>>Main Track>>1626>>authors>>authors_2>>Shuxiao Ding": 0.1803973913192749, "IJCAI2023>>program>>Main Track>>1379>>authors>>authors_1>>James Kotary": 0.1826791763305664, "IJCAI2023>>program>>Main Track>>4078>>authors>>authors_3>>Stefan Szeider": 0.1828169822692871, "IJCAI2023>>program>>Main Track>>398>>authors>>authors_3>>Zhongang Qi": 0.18339169025421143, "IJCAI2023>>program>>Doctoral Consortium Track>>DC5898>>authors>>authors_1>>Wiebke Hutiri": 0.18561607599258423, "IJCAI2023>>program>>Main Track>>5012>>authors>>authors_4>>Yidong Chen": 0.18567776679992676, "IJCAI2023>>program>>Main Track>>4969>>authors>>authors_2>>Ronghui Xu": 0.18575447797775269, "IJCAI2023>>program>>Journal Track>>J5939>>authors>>authors_1>>Elias Schede": 0.18601661920547485, "IJCAI2023>>program>>Main Track>>4206>>authors>>authors_2>>Hendrik Molter": 0.18685823678970337, "IJCAI2023>>program>>Main Track>>729>>authors>>authors_1>>Luca Marzari": 0.1869562268257141, "IJCAI2023>>program>>Main Track>>1817>>authors>>authors_1>>Kayla Boggess": 0.18698763847351074, "IJCAI2023>>program>>Main Track>>2229>>title>>On Lower Bounds for Maximin Share Guarantees": 0.1871035099029541, "IJCAI2023>>program>>Main Track>>5176>>authors>>authors_3>>Yasheng Wang": 0.18736135959625244}, "Who are the authors of the paper titled 'Singularformer: Learning to Decompose Self-Attention to Linearize the Complexity of Transformer'?": {"IJCAI2023>>program>>Main Track>>4090>>title>>Singularformer: Learning to Decompose Self-Attention to Linearize the Complexity of Transformer": 0.10782837867736816, "IJCAI2023>>program>>Main Track>>4090>>abstract>>Transformers achieve excellent performance in a variety of domains since they can capture long-distance dependencies through the self-attention mechanism. However, self-attention is computationally costly due to its quadratic complexity and high memory consumption. In this paper, we propose a novel Transformer variant (Singularformer) that uses neural networks to learn the singular value decomposition process of the attention matrix to design a linear-complexity and memory-efficient global self-attention mechanism. Specifically, we decompose the attention matrix into the product of three matrix factors based on singular value decomposition and design neural networks to learn these matrix factors, then the associative law of matrix multiplication is used to linearize the calculation of self-attention. The above procedure allows us to compute self-attention as two-dimensional reduction processes in the first and second token dimensional spaces, followed by a multi-head self-attention computational process on the first dimensional reduced token features. Experimental results on 8 real-world datasets demonstrate that Singularformer performs favorably against the other Transformer variants with lower time and space complexity. Our source code is publicly available at https://github.com/CSUBioGroup/Singularformer.": 0.13651418685913086, "IJCAI2023>>program>>Survey Track>>SV5660>>authors>>authors_2>>Kun Zhou": 0.17418205738067627}, "What is the main aim of the proposed Transformer variant named Singularformer?": {"IJCAI2023>>program>>Main Track>>4090>>abstract>>Transformers achieve excellent performance in a variety of domains since they can capture long-distance dependencies through the self-attention mechanism. However, self-attention is computationally costly due to its quadratic complexity and high memory consumption. In this paper, we propose a novel Transformer variant (Singularformer) that uses neural networks to learn the singular value decomposition process of the attention matrix to design a linear-complexity and memory-efficient global self-attention mechanism. Specifically, we decompose the attention matrix into the product of three matrix factors based on singular value decomposition and design neural networks to learn these matrix factors, then the associative law of matrix multiplication is used to linearize the calculation of self-attention. The above procedure allows us to compute self-attention as two-dimensional reduction processes in the first and second token dimensional spaces, followed by a multi-head self-attention computational process on the first dimensional reduced token features. Experimental results on 8 real-world datasets demonstrate that Singularformer performs favorably against the other Transformer variants with lower time and space complexity. Our source code is publicly available at https://github.com/CSUBioGroup/Singularformer.": 0.13402163982391357, "IJCAI2023>>program>>Main Track>>4090>>title>>Singularformer: Learning to Decompose Self-Attention to Linearize the Complexity of Transformer": 0.13509780168533325}, "What are the advantages of the proposed Transformer variant according to experimental results?": {"IJCAI2023>>program>>Main Track>>2490>>abstract>>Transformers achieve promising performance in document understanding because of their high effectiveness and still suffer from quadratic computational complexity dependency on the sequence length. General efficient transformers are challenging to be directly adapted to model document. They are unable to handle the layout representation in documents, e.g. word, line and paragraph, on different granularity levels and seem hard to achieve a good trade-off between efficiency and performance. To tackle the concerns, we propose Fast-StrucTexT, an efficient multi-modal framework based on the StrucTexT algorithm with an hourglass transformer architecture, for visual document understanding. Specifically, we design a modality-guided dynamic token merging block to make the model learn multi-granularity representation and prunes redundant tokens. Additionally, we present a multi-modal interaction module called Symmetry Cross-Attention (SCA) to consider multi-modal fusion and efficiently guide the token mergence. The SCA allows one modality input as query to calculate cross attention with another modality in a dual phase. Extensive experiments on FUNSD, SROIE, and CORD datasets demonstrate that our model achieves the state-of-the-art performance and almost 1.9x faster inference time than the state-of-the-art methods.": 0.1617264747619629, "IJCAI2023>>program>>Survey Track>>SV5579>>abstract>>Transformers have achieved superior performances in many tasks in natural language processing and computer vision, which also triggered great interest in the time series community. Among multiple advantages of Transformers, the ability to capture long-range dependencies and interactions is especially attractive for time series modeling, leading to exciting progress in various time series applications. In this paper, we systematically review Transformer schemes for time series modeling by highlighting their strengths as well as limitations. In particular, we examine the development of time series Transformers in two perspectives. From the perspective of network structure, we summarize the adaptations and modifications that have been made to Transformers in order to accommodate the challenges in time series analysis. From the perspective of applications, we categorize time series Transformers based on common tasks including forecasting, anomaly detection, and classification. Empirically, we perform robust analysis, model size analysis, and seasonal-trend decomposition analysis to study how Transformers perform in time series. Finally, we discuss and suggest future directions to provide useful research guidance.": 0.17224568128585815}, "In which category does the Singularformer paper fall under?": {"IJCAI2023>>program>>Main Track>>4090>>abstract>>Transformers achieve excellent performance in a variety of domains since they can capture long-distance dependencies through the self-attention mechanism. However, self-attention is computationally costly due to its quadratic complexity and high memory consumption. In this paper, we propose a novel Transformer variant (Singularformer) that uses neural networks to learn the singular value decomposition process of the attention matrix to design a linear-complexity and memory-efficient global self-attention mechanism. Specifically, we decompose the attention matrix into the product of three matrix factors based on singular value decomposition and design neural networks to learn these matrix factors, then the associative law of matrix multiplication is used to linearize the calculation of self-attention. The above procedure allows us to compute self-attention as two-dimensional reduction processes in the first and second token dimensional spaces, followed by a multi-head self-attention computational process on the first dimensional reduced token features. Experimental results on 8 real-world datasets demonstrate that Singularformer performs favorably against the other Transformer variants with lower time and space complexity. Our source code is publicly available at https://github.com/CSUBioGroup/Singularformer.": 0.1624530553817749, "IJCAI2023>>program>>Main Track>>4090>>title>>Singularformer: Learning to Decompose Self-Attention to Linearize the Complexity of Transformer": 0.1646978259086609, "IJCAI2023>>program>>Best Papers from Sister Conferences Track>>SC7>>authors>>authors_1>>Dario Simionato": 0.17263013124465942, "IJCAI2023>>program>>Best Papers from Sister Conferences Track>>SC10>>authors>>authors_2>>Matthias Lanzinger": 0.18005502223968506}, "What is the title of the paper presented in the Main Track 2045 of IJCAI2023 program?": {"IJCAI2023>>program>>Main Track>>1045>>authors>>authors_3>>Elisheva S. Shamash": 0.08137255907058716, "IJCAI2023>>program>>Main Track>>5195>>authors>>authors_3>>Thiago D. Simão": 0.08351242542266846, "IJCAI2023>>program>>Main Track>>4586>>authors>>authors_5>>Shinnosuke Takamichi": 0.08547818660736084, "IJCAI2023>>program>>Main Track>>863>>authors>>authors_5>>Kai Chen": 0.08788871765136719, "IJCAI2023>>program>>Main Track>>3832>>authors>>authors_6>>Felix Ulrich-Oltean": 0.08918577432632446, "IJCAI2023>>program>>Main Track>>2178>>authors>>authors_3>>Ruben Solozabal Ochoa de Retana": 0.09028881788253784, "IJCAI2023>>program>>Main Track>>1621>>authors>>authors_3>>Jörg Hoffmann": 0.09106791019439697, "IJCAI2023>>program>>Main Track>>1798>>authors>>authors_4>>Chenxi Ma": 0.09148144721984863, "IJCAI2023>>program>>Main Track>>200>>authors>>authors_4>>Yiran Chen": 0.09188461303710938, "IJCAI2023>>program>>Main Track>>2230>>authors>>authors_3>>Sheila McIlraith": 0.09265047311782837, "IJCAI2023>>program>>Main Track>>4586>>authors>>authors_1>>Takaaki Saeki": 0.09271496534347534, "IJCAI2023>>program>>Main Track>>2705>>authors>>authors_5>>Dangyang Chen": 0.09281408786773682, "IJCAI2023>>program>>Main Track>>1540>>authors>>authors_4>>Hao Wang": 0.09335589408874512, "IJCAI2023>>program>>Main Track>>4090>>authors>>authors_1>>Yifan Wu": 0.09337413311004639, "IJCAI2023>>program>>Main Track>>4376>>authors>>authors_1>>Zhaiming Shen": 0.09371918439865112, "IJCAI2023>>program>>Main Track>>2754>>authors>>authors_5>>Bin Cui": 0.09384799003601074, "IJCAI2023>>program>>Main Track>>1756>>authors>>authors_4>>Conrad Schecker": 0.09464395046234131, "IJCAI2023>>program>>Main Track>>902>>authors>>authors_1>>Tong Liu": 0.0950135588645935, "IJCAI2023>>program>>Main Track>>2276>>authors>>authors_2>>Di Jin": 0.09571599960327148, "IJCAI2023>>program>>Main Track>>2716>>authors>>authors_5>>Siya Qiu": 0.09618568420410156}, "Who are the authors of the paper presented in the Main Track 2045?": {"IJCAI2023>>program>>Main Track>>1856>>authors>>authors_2>>Kate Larson": 0.11314630508422852, "IJCAI2023>>program>>Main Track>>2038>>authors>>authors_5>>Xi Wu": 0.11581361293792725, "IJCAI2023>>program>>Main Track>>1856>>authors>>authors_1>>David Radke": 0.11802732944488525, "IJCAI2023>>program>>Main Track>>4969>>authors>>authors_1>>Dabin Zhang": 0.11972099542617798, "IJCAI2023>>program>>Main Track>>863>>authors>>authors_8>>Jie Chen": 0.12332892417907715, "IJCAI2023>>program>>Main Track>>4454>>authors>>authors_4>>Kurt Mehlhorn": 0.12338894605636597, "IJCAI2023>>program>>Main Track>>2045>>authors>>authors_5>>Yipeng Zhou": 0.12483334541320801, "IJCAI2023>>program>>Main Track>>4062>>authors>>authors_3>>Pascal Lenzner": 0.1249881386756897, "IJCAI2023>>program>>Main Track>>2045>>authors>>authors_1>>Jiahao Liu": 0.12536132335662842, "IJCAI2023>>program>>Main Track>>1045>>authors>>authors_3>>Elisheva S. Shamash": 0.12654268741607666, "IJCAI2023>>program>>Main Track>>4586>>authors>>authors_5>>Shinnosuke Takamichi": 0.12692558765411377, "IJCAI2023>>program>>Main Track>>2230>>authors>>authors_3>>Sheila McIlraith": 0.12721681594848633, "IJCAI2023>>program>>Main Track>>2587>>authors>>authors_5>>Gongping Yang": 0.12735015153884888, "IJCAI2023>>program>>Main Track>>5176>>authors>>authors_3>>Yasheng Wang": 0.12818193435668945, "IJCAI2023>>program>>Main Track>>1698>>authors>>authors_1>>Yi Gao": 0.12841010093688965, "IJCAI2023>>program>>Main Track>>5164>>authors>>authors_4>>Jianyong Wang": 0.12855994701385498, "IJCAI2023>>program>>Main Track>>4580>>authors>>authors_1>>Carlos Hernández": 0.12859582901000977, "IJCAI2023>>program>>Main Track>>4484>>authors>>authors_1>>Luca Becchetti": 0.12875372171401978, "IJCAI2023>>program>>Main Track>>1426>>authors>>authors_3>>Yu-Shuen Wang": 0.12891238927841187, "IJCAI2023>>program>>Main Track>>3934>>authors>>authors_4>>Luc De Raedt": 0.1296299695968628}, "What is the main problem that the paper 'FedDWA: Personalized Federated Learning with Dynamic Weight Adjustment' addresses?": {"IJCAI2023>>program>>Main Track>>2045>>title>>FedDWA: Personalized Federated Learning with Dynamic Weight Adjustment": 0.10468494892120361, "IJCAI2023>>program>>Main Track>>2045>>abstract>>Different from conventional federated learning, personalized federated learning (PFL) is able to train a customized model for each individual client according to its unique requirement. The mainstream approach is to adopt a kind of weighted aggregation method to generate personalized models, in which weights are determined by the loss value or model parameters among different clients. However, such kinds of methods require clients to download others’ models. It not only sheer increases communication traffic but also potentially infringes data privacy. In this paper, we propose a new PFL algorithm called FedDWA (Federated Learning with Dynamic Weight Adjustment) to address the above problem, which leverages the parameter server (PS) to compute personalized aggregation weights based on collected models from clients. In this way, FedDWA can capture similarities between clients with much less communication overhead. More specifically, we formulate the PFL problem as an optimization problem by minimizing the distance between personalized models and  guidance models, so as to  customize aggregation weights for each client. Guidance models are obtained by  the local one-step ahead adaptation on individual clients. Finally,  we conduct extensive experiments using five real datasets and the results demonstrate that FedDWA can significantly reduce the communication traffic and achieve much higher model accuracy than the state-of-the-art approaches.": 0.10482031106948853}, "What is the proposed solution in the paper to reduce communication traffic in personalized federated learning?": {"IJCAI2023>>program>>Main Track>>1390>>abstract>>Split learning is a simple solution for Vertical Federated Learning (VFL), which has drawn substantial attention in both research and application due to its simplicity and efficiency. However, communication efficiency is still a crucial issue for split learning. In this paper, we investigate multiple communication reduction methods for split learning, including cut layer size reduction, top-k sparsification, quantization, and L1 regularization. Through analysis of the cut layer size reduction and top-k sparsification, we further propose randomized top-k sparsification, to make the model generalize and converge better. This is done by selecting top-k elements with a large probability while also having a small probability to select non-top-k elements. Empirical results show that compared with other communication-reduction methods, our proposed randomized top-k sparsification achieves a better model performance under the same compression level.": 0.15757006406784058, "IJCAI2023>>program>>Main Track>>3414>>abstract>>Federated recommendation is a new Internet service architecture that aims to provide privacy-preserving recommendation services in federated settings. Existing solutions are used to combine distributed recommendation algorithms and privacy-preserving mechanisms. Thus it inherently takes the form of heavyweight models at the server and hinders the deployment of on-device intelligent models to end-users. This paper proposes a novel Personalized Federated Recommendation (PFedRec) framework to learn many user-specific lightweight models to be deployed on smart devices rather than a heavyweight model on a server. Moreover, we propose a new dual personalization mechanism to effectively learn fine-grained personalization on both users and items. The overall learning process is formulated into a unified federated optimization framework. Specifically, unlike previous methods that share exactly the same item embeddings across users in a federated system, dual personalization allows mild finetuning of item embeddings for each user to generate user-specific views for item representations which can be integrated into existing federated recommendation methods to gain improvements immediately. Experiments on multiple benchmark datasets have demonstrated the effectiveness of PFedRec and the dual personalization mechanism. Moreover, we provide visualizations and in-depth analysis of the personalization techniques in item embedding, which shed novel insights on the design of recommender systems in federated settings. The code is available.": 0.1588168740272522, "IJCAI2023>>program>>Main Track>>1390>>title>>Reducing Communication for Split Learning by Randomized Top-k Sparsification": 0.16193807125091553}, "What is the primary keyword associated with the paper presented in the Main Track 2045?": {"IJCAI2023>>program>>Main Track>>3195>>keywords>>keywords_2>>Computer Vision -> CV: Neural generative models, auto encoders, GANs": 0.11666446924209595, "IJCAI2023>>program>>Main Track>>4255>>keywords>>keywords_1>>Machine Learning -> ML: Robustness": 0.11857932806015015, "IJCAI2023>>program>>Main Track>>4504>>authors>>authors_2>>Tobias Geibinger": 0.11953502893447876, "IJCAI2023>>program>>Main Track>>704>>keywords>>keywords_1>>Machine Learning -> ML: Federated learning": 0.12075167894363403, "IJCAI2023>>program>>Main Track>>5155>>keywords>>keywords_2>>Machine Learning -> ML: Time series and data streams": 0.1216549277305603, "IJCAI2023>>program>>Main Track>>1850>>keywords>>keywords_2>>Computer Vision -> CV: Video analysis and understanding": 0.12209081649780273, "IJCAI2023>>program>>Main Track>>5155>>keywords>>keywords_1>>Machine Learning -> ML: Federated learning": 0.12241500616073608, "IJCAI2023>>program>>Main Track>>2077>>keywords>>keywords_2>>Computer Vision -> CV: Recognition (object detection, categorization)": 0.12252718210220337, "IJCAI2023>>program>>Main Track>>4799>>keywords>>keywords_3>>Knowledge Representation and Reasoning -> KRR: Causality": 0.12395751476287842, "IJCAI2023>>program>>Main Track>>3525>>keywords>>keywords_1>>Natural Language Processing -> NLP: Language models": 0.12400639057159424, "IJCAI2023>>program>>Main Track>>4226>>keywords>>keywords_1>>Computer Vision -> CV: Machine learning for vision": 0.1249549388885498, "IJCAI2023>>program>>Main Track>>4383>>keywords>>keywords_2>>Machine Learning -> ML: Clustering": 0.12502801418304443, "IJCAI2023>>program>>Main Track>>3832>>keywords>>keywords_3>>Machine Learning -> ML: Classification": 0.12522464990615845, "IJCAI2023>>program>>Main Track>>204>>keywords>>keywords_2>>Machine Learning -> ML: Reinforcement learning": 0.12539398670196533, "IJCAI2023>>program>>Main Track>>1009>>keywords>>keywords_1>>Machine Learning -> ML: Optimization": 0.12600278854370117, "IJCAI2023>>program>>Main Track>>4124>>keywords>>keywords_2>>Data Mining -> DM: Class imbalance and unequal cost": 0.1260165572166443, "IJCAI2023>>program>>Main Track>>204>>keywords>>keywords_3>>Uncertainty in AI -> UAI: Sequential decision making": 0.1260356307029724}, "What is the title of the paper with ID 1588?": {"IJCAI2023>>program>>Main Track>>1626>>authors>>authors_1>>Peizheng Li": 0.15450584888458252, "IJCAI2023>>program>>Main Track>>1493>>authors>>authors_3>>Xinning Zhou": 0.1599806547164917, "IJCAI2023>>program>>Main Track>>1493>>authors>>authors_5>>Dong Yan": 0.16148388385772705, "IJCAI2023>>calls>>Journal Track>>Accepted Papers List>>J5950>>authors>>authors_5>>Subbarao Kambhampati": 0.16380321979522705, "IJCAI2023>>program>>Main Track>>1588>>authors>>authors_4>>Li Guo": 0.16552549600601196, "IJCAI2023>>program>>Main Track>>1626>>authors>>authors_2>>Shuxiao Ding": 0.16581469774246216, "IJCAI2023>>program>>Main Track>>1856>>authors>>authors_2>>Kate Larson": 0.16950833797454834, "IJCAI2023>>program>>Main Track>>1588>>authors>>authors_7>>Xun Chen": 0.17211288213729858, "IJCAI2023>>program>>Main Track>>1593>>authors>>authors_2>>Tao Dai": 0.17223197221755981, "IJCAI2023>>program>>Main Track>>398>>authors>>authors_3>>Zhongang Qi": 0.1722988486289978, "IJCAI2023>>program>>Doctoral Consortium Track>>DC5898>>authors>>authors_1>>Wiebke Hutiri": 0.17261219024658203, "IJCAI2023>>program>>Journal Track>>J5758>>authors>>authors_2>>Matthew E. Taylor": 0.17262840270996094, "IJCAI2023>>program>>Main Track>>4482>>title>>Fair Division of a Graph into Compact Bundles": 0.17328321933746338, "IJCAI2023>>program>>Main Track>>1384>>authors>>authors_1>>Xixuan Hao": 0.17336517572402954, "IJCAI2023>>program>>Main Track>>4969>>authors>>authors_1>>Dabin Zhang": 0.1742350459098816, "IJCAI2023>>program>>Main Track>>2358>>authors>>authors_3>>Yuxin Wang": 0.17439329624176025, "IJCAI2023>>program>>Main Track>>1379>>authors>>authors_1>>James Kotary": 0.17481791973114014, "IJCAI2023>>program>>Main Track>>4480>>title>>Adversarial Contention Resolution Games": 0.17510783672332764, "IJCAI2023>>program>>Main Track>>1593>>keywords>>keywords_2>>Computer Vision -> CV: Applications": 0.1752147078514099, "IJCAI2023>>program>>Main Track>>1856>>authors>>authors_3>>Tim Brecht": 0.17575305700302124}, "Who are the authors of the paper titled 'Accurate MRI Reconstruction via Multi-Domain Recurrent Networks'?": {"IJCAI2023>>program>>Main Track>>4454>>authors>>authors_4>>Kurt Mehlhorn": 0.16413456201553345, "IJCAI2023>>program>>Main Track>>1379>>authors>>authors_1>>James Kotary": 0.1692800521850586, "IJCAI2023>>program>>Main Track>>536>>authors>>authors_1>>Rao Fu": 0.16941970586776733, "IJCAI2023>>program>>Journal Track>>J5924>>authors>>authors_6>>Daniele Magazzeni": 0.17058658599853516, "IJCAI2023>>program>>Main Track>>731>>authors>>authors_2>>Massih-Reza Amini": 0.1707475185394287, "IJCAI2023>>program>>Survey Track>>SV5648>>authors>>authors_5>>Pradeep K. Murukannaiah": 0.17163598537445068, "IJCAI2023>>program>>Demonstrations Track>>DM5732>>authors>>authors_3>>Huiying Ren": 0.17247211933135986, "IJCAI2023>>program>>Demonstrations Track>>DM5696>>authors>>authors_2>>Rongge Guo": 0.17301726341247559, "IJCAI2023>>program>>Survey Track>>SV5593>>authors>>authors_8>>Qing Li": 0.1731928586959839, "IJCAI2023>>program>>Main Track>>1384>>authors>>authors_1>>Xixuan Hao": 0.17428255081176758, "IJCAI2023>>program>>Survey Track>>SV5660>>authors>>authors_2>>Kun Zhou": 0.17451053857803345, "IJCAI2023>>program>>Demonstrations Track>>DM5731>>authors>>authors_4>>Prasenjit Mitra": 0.17458927631378174, "IJCAI2023>>program>>Survey Track>>SV5630>>authors>>authors_1>>Zhichun Guo": 0.1752747893333435, "IJCAI2023>>program>>Journal Track>>J5758>>authors>>authors_1>>Sriram Ganapathi Subramanian": 0.17536336183547974, "IJCAI2023>>program>>Demonstrations Track>>DM5742>>authors>>authors_3>>Biplav Srivastava": 0.175390362739563, "IJCAI2023>>program>>Demonstrations Track>>DM5718>>authors>>authors_1>>Sowmith Nandan Rachuri": 0.175392746925354, "IJCAI2023>>program>>Demonstrations Track>>DM5731>>authors>>authors_2>>Shreya Ghosh": 0.1756327748298645, "IJCAI2023>>program>>Main Track>>2929>>authors>>authors_5>>Eric Rice": 0.17577803134918213}, "What is the key issue addressed by the paper ID 1588 in the field of MRI reconstruction?": {"IJCAI2023>>program>>Main Track>>1588>>abstract>>In recent years, deep convolutional neural networks (CNNs) have become dominant in MRI reconstruction from undersampled k-space. However, most existing CNNs methods reconstruct the undersampled images either in the spatial domain or in the frequency domain, and neglecting the correlation between these two domains. This hinders the further reconstruction performance improvement. To tackle this issue, in this work, we propose a new multi-domain recurrent network (MDR-Net) with multi-domain learning (MDL) blocks as its basic units to reconstruct the undersampled MR image progressively. Specifically, the MDL block interactively processes the local spatial features and the global frequency information to facilitate complementary learning, leading to fine-grained features generation. Furthermore, we introduce an effective frequency-based loss to narrow the frequency spectrum gap, compensating for over-smoothness caused by the widely used spatial reconstruction loss. Extensive experiments on public fastMRI datasets demonstrate that our MDR-Net consistently outperforms other competitive methods and is able to provide more details.": 0.15333908796310425, "IJCAI2023>>program>>Main Track>>1588>>title>>Accurate MRI Reconstruction via Multi-Domain Recurrent Networks": 0.15451741218566895, "IJCAI2023>>program>>Main Track>>596>>title>>Deep Unfolding Convolutional Dictionary Model for Multi-Contrast MRI Super-resolution and Reconstruction": 0.15488749742507935}, "Which technique is proposed by the authors in the paper 'Accurate MRI Reconstruction via Multi-Domain Recurrent Networks' to tackle the identified issue?": {"IJCAI2023>>program>>Main Track>>1588>>abstract>>In recent years, deep convolutional neural networks (CNNs) have become dominant in MRI reconstruction from undersampled k-space. However, most existing CNNs methods reconstruct the undersampled images either in the spatial domain or in the frequency domain, and neglecting the correlation between these two domains. This hinders the further reconstruction performance improvement. To tackle this issue, in this work, we propose a new multi-domain recurrent network (MDR-Net) with multi-domain learning (MDL) blocks as its basic units to reconstruct the undersampled MR image progressively. Specifically, the MDL block interactively processes the local spatial features and the global frequency information to facilitate complementary learning, leading to fine-grained features generation. Furthermore, we introduce an effective frequency-based loss to narrow the frequency spectrum gap, compensating for over-smoothness caused by the widely used spatial reconstruction loss. Extensive experiments on public fastMRI datasets demonstrate that our MDR-Net consistently outperforms other competitive methods and is able to provide more details.": 0.10290282964706421, "IJCAI2023>>program>>Main Track>>596>>title>>Deep Unfolding Convolutional Dictionary Model for Multi-Contrast MRI Super-resolution and Reconstruction": 0.15110886096954346, "IJCAI2023>>program>>Main Track>>1798>>abstract>>In recent years, many convolutional neural network-based models are designed for JPEG artifacts reduction, and have achieved notable progress. However, few methods are suitable for extreme low-bitrate image compression artifacts reduction. The main challenge is that the highly compressed image loses too much information, resulting in reconstructing high-quality image difficultly. To address this issue, we propose a multimodal fusion learning method for text-guided JPEG artifacts reduction, in which the corresponding text description not only provides the potential prior information of the highly compressed image, but also serves as supplementary information to assist in image deblocking. We fuse image features and text semantic features from the global and local perspectives respectively, and design a contrastive loss built upon contrastive learning to produce visually pleasing results. Extensive experiments, including a user study, prove that our method can obtain better deblocking results compared to the state-of-the-art methods.": 0.1580493450164795, "IJCAI2023>>program>>Main Track>>1588>>title>>Accurate MRI Reconstruction via Multi-Domain Recurrent Networks": 0.1583237648010254}, "What are the keywords of the paper with ID 1588?": {"IJCAI2023>>program>>Main Track>>1593>>keywords>>keywords_2>>Computer Vision -> CV: Applications": 0.14032793045043945, "IJCAI2023>>calls>>Main Track>>Accepted Papers List>>141>>keywords>>keywords_1>>Data Mining -> DM: Mining graphs": 0.14546197652816772, "IJCAI2023>>program>>Main Track>>1493>>keywords>>keywords_1>>Machine Learning -> ML: Reinforcement learning": 0.1482219099998474, "IJCAI2023>>program>>Main Track>>1596>>keywords>>keywords_2>>Computer Vision -> CV: Image and video retrieval": 0.1483287215232849, "IJCAI2023>>program>>Main Track>>1593>>keywords>>keywords_1>>Computer Vision -> CV: Recognition (object detection, categorization)": 0.15056508779525757, "IJCAI2023>>program>>Main Track>>4480>>keywords>>keywords_1>>Game Theory and Economic Paradigms -> GTEP: Noncooperative games": 0.15133613348007202, "IJCAI2023>>program>>Doctoral Consortium Track>>DC5898>>keywords>>keywords_4>>AI Ethics, Trust, Fairness -> ETF: Societal impact of AI": 0.15152859687805176, "IJCAI2023>>program>>Main Track>>1621>>keywords>>keywords_1>>Multidisciplinary Topics and Applications -> MDA: Software engineering": 0.1515491008758545, "IJCAI2023>>program>>Best Papers from Sister Conferences Track>>SC12>>keywords>>keywords_4>>Sister Conferences Best Papers -> Search": 0.15210086107254028, "IJCAI2023>>program>>Journal Track>>J5946>>keywords>>keywords_3>>Multidisciplinary Topics and Applications -> MDA: Security and privacy": 0.15294915437698364, "IJCAI2023>>program>>Main Track>>1596>>keywords>>keywords_1>>Computer Vision -> CV: Vision and language": 0.15422165393829346, "IJCAI2023>>program>>Main Track>>4665>>keywords>>keywords_2>>AI Ethics, Trust, Fairness -> ETF: Explainability and interpretability": 0.15518897771835327, "IJCAI2023>>program>>Journal Track>>J5928>>keywords>>keywords_1>>Multidisciplinary Topics and Applications -> MDA: Security and privacy": 0.15556728839874268, "IJCAI2023>>program>>Main Track>>1778>>keywords>>keywords_2>>Robotics -> ROB: Multi-robot systems": 0.15603631734848022, "IJCAI2023>>program>>Main Track>>1542>>keywords>>keywords_2>>Computer Vision -> CV: Vision and language": 0.1560373306274414}, "What is the title of the paper with ID 2869 in the main track of IJCAI2023?": {"IJCAI2023>>program>>Main Track>>3863>>authors>>authors_7>>Hua Wu": 0.10175079107284546, "IJCAI2023>>program>>Main Track>>5195>>authors>>authors_3>>Thiago D. Simão": 0.10197824239730835, "IJCAI2023>>program>>Main Track>>2178>>authors>>authors_3>>Ruben Solozabal Ochoa de Retana": 0.10232841968536377, "IJCAI2023>>program>>Main Track>>1493>>authors>>authors_5>>Dong Yan": 0.1042054295539856, "IJCAI2023>>program>>Journal Track>>J5939>>authors>>authors_1>>Elias Schede": 0.10448157787322998, "IJCAI2023>>program>>Main Track>>2929>>authors>>authors_1>>Haipeng Chen": 0.10823452472686768, "IJCAI2023>>program>>Main Track>>4206>>authors>>authors_2>>Hendrik Molter": 0.10943001508712769, "IJCAI2023>>program>>Main Track>>3832>>authors>>authors_6>>Felix Ulrich-Oltean": 0.1099400520324707, "IJCAI2023>>program>>Main Track>>4276>>authors>>authors_8>>Bettina Könighofer": 0.11032795906066895, "IJCAI2023>>program>>Main Track>>5012>>authors>>authors_4>>Yidong Chen": 0.11089754104614258, "IJCAI2023>>program>>Main Track>>4580>>authors>>authors_1>>Carlos Hernández": 0.11174201965332031, "IJCAI2023>>program>>Main Track>>4276>>authors>>authors_4>>Katrine Bjørner": 0.11187821626663208, "IJCAI2023>>program>>Main Track>>200>>authors>>authors_4>>Yiran Chen": 0.11206227540969849, "IJCAI2023>>program>>Main Track>>5281>>authors>>authors_2>>Hao Chen": 0.11235737800598145, "IJCAI2023>>program>>Main Track>>863>>authors>>authors_5>>Kai Chen": 0.11260682344436646, "IJCAI2023>>program>>Main Track>>1493>>authors>>authors_6>>Jun Zhu": 0.11269503831863403, "IJCAI2023>>program>>Main Track>>3497>>authors>>authors_2>>Jun Yuan": 0.11319583654403687, "IJCAI2023>>program>>Main Track>>3873>>authors>>authors_3>>Michael Wooldridge": 0.11332970857620239, "IJCAI2023>>program>>Main Track>>4586>>authors>>authors_5>>Shinnosuke Takamichi": 0.11347746849060059, "IJCAI2023>>program>>Main Track>>4376>>authors>>authors_1>>Zhaiming Shen": 0.11357808113098145}, "Who are the authors of the paper titled 'Teacher Assistant-Based Knowledge Distillation Extracting Multi-level Features on Single Channel Sleep EEG'?": {"IJCAI2023>>program>>Survey Track>>SV5660>>authors>>authors_2>>Kun Zhou": 0.17032665014266968, "IJCAI2023>>program>>Survey Track>>SV5593>>authors>>authors_1>>Chengyi Liu": 0.17273789644241333, "IJCAI2023>>program>>Demonstrations Track>>DM5705>>authors>>authors_5>>Kavitha Srinivas": 0.17396199703216553, "IJCAI2023>>program>>Main Track>>1379>>authors>>authors_1>>James Kotary": 0.17415034770965576, "IJCAI2023>>program>>Survey Track>>SV5648>>authors>>authors_5>>Pradeep K. Murukannaiah": 0.17498934268951416, "IJCAI2023>>program>>Main Track>>1834>>authors>>authors_3>>Christel Baier": 0.17502093315124512, "IJCAI2023>>calls>>Journal Track>>Accepted Papers List>>J5950>>authors>>authors_5>>Subbarao Kambhampati": 0.17684394121170044, "IJCAI2023>>program>>Survey Track>>SV5487>>authors>>authors_2>>Zongxiong Chen": 0.17774349451065063, "IJCAI2023>>program>>Main Track>>4454>>authors>>authors_4>>Kurt Mehlhorn": 0.17803800106048584, "IJCAI2023>>program>>Journal Track>>J5924>>authors>>authors_6>>Daniele Magazzeni": 0.17848724126815796, "IJCAI2023>>program>>Survey Track>>SV5630>>authors>>authors_11>>Nitesh V. Chawla": 0.17881351709365845, "IJCAI2023>>program>>Survey Track>>SV5630>>authors>>authors_1>>Zhichun Guo": 0.17882364988327026, "IJCAI2023>>program>>Demonstrations Track>>DM5712>>authors>>authors_1>>Yong Shan": 0.1795821189880371, "IJCAI2023>>program>>Survey Track>>SV5614>>authors>>authors_1>>Arnaud Deza": 0.17984378337860107, "IJCAI2023>>program>>Survey Track>>SV5660>>authors>>authors_3>>Wayne Xin Zhao": 0.17990612983703613, "IJCAI2023>>program>>Main Track>>1626>>authors>>authors_1>>Peizheng Li": 0.18003058433532715, "IJCAI2023>>program>>Journal Track>>J5758>>authors>>authors_2>>Matthew E. Taylor": 0.18019193410873413, "IJCAI2023>>program>>Survey Track>>SV5593>>authors>>authors_8>>Qing Li": 0.1804831624031067}, "What is the main focus of the study presented in the paper with ID 2869 at IJCAI2023?": {"IJCAI2023>>program>>Main Track>>5195>>authors>>authors_3>>Thiago D. Simão": 0.12140119075775146, "IJCAI2023>>program>>Main Track>>3863>>authors>>authors_7>>Hua Wu": 0.12172156572341919, "IJCAI2023>>program>>Main Track>>11>>keywords>>keywords_2>>Machine Learning -> ML: Deep reinforcement learning": 0.1283245086669922, "IJCAI2023>>program>>Main Track>>4226>>keywords>>keywords_2>>Computer Vision -> CV: Applications": 0.12999963760375977, "IJCAI2023>>program>>Main Track>>2225>>abstract>>Spoken language understanding (SLU), one of the key enabling technologies for human-computer interaction in IoT devices, provides an easy-to-use user interface. Human speech can contain a lot of user-sensitive information, such as gender, identity, and sensitive content. New types of security and privacy breaches have thus emerged. Users do not want to expose their personal sensitive information to malicious attacks by untrusted third parties. Thus, the SLU system needs to ensure that a potential malicious attacker cannot deduce the sensitive attributes of the users, while it should avoid greatly compromising the SLU accuracy. To address the above challenge, this paper proposes a novel SLU multi-task privacy-preserving model to prevent both the speech recognition (ASR) and identity recognition (IR) attacks. The model uses the hidden layer separation technique so that SLU information is distributed only in a specific portion of the hidden layer, and the other two types of information are removed to obtain a privacy-secure hidden layer. In order to achieve good balance between efficiency and privacy, we introduce a new mechanism of model pre-training, namely joint adversarial training, to further enhance the user privacy. Experiments over two SLU datasets show that the proposed method can reduce the accuracy of both the ASR and IR attacks close to that of a random guess, while leaving the SLU performance largely unaffected.": 0.13059532642364502, "IJCAI2023>>program>>Main Track>>3959>>keywords>>keywords_1>>Computer Vision -> CV: Adversarial learning, adversarial attack and defense methods": 0.13170528411865234, "IJCAI2023>>program>>Main Track>>2178>>authors>>authors_3>>Ruben Solozabal Ochoa de Retana": 0.13178062438964844, "IJCAI2023>>program>>Main Track>>1633>>authors>>authors_3>>John Dickerson": 0.1318281888961792}, "What specific problem does SleepKD intend to solve?": {"IJCAI2023>>program>>Main Track>>2869>>abstract>>Sleep stage classification is of great significance to the diagnosis of sleep disorders. However, existing sleep stage classification models based on deep learning are usually relatively large in size (wider and deeper), which makes them hard to be deployed on wearable devices. Therefore, it is a challenge to lighten the existing sleep stage classification models. In this paper, we propose a novel general knowledge distillation framework for sleep stage classification tasks called SleepKD. Our SleepKD, composed of the multi-level module, teacher assistant module, and other knowledge distillation modules, aims to lighten large-scale sleep stage classification models. Specifically, the multi-level module is able to transfer the multi-level knowledge extracted from sleep signals by the teacher model (large-scale model) to the student model (lightweight model). Moreover, the teacher assistant module bridges the large gap between the teacher and student network, and further improves the distillation. We evaluate our method on two public sleep datasets (Sleep-EDF and ISRUC-III). Compared to the baseline methods, the results show that our knowledge distillation framework achieves state-of-the-art performance. SleepKD can significantly lighten the sleep model while maintaining its classification performance. The source code is available at https://github.com/HychaoWang/SleepKD.": 0.17233121395111084, "IJCAI2023>>program>>Main Track>>2869>>title>>Teacher Assistant-Based Knowledge Distillation Extracting Multi-level Features on Single Channel Sleep EEG": 0.1890752911567688}, "In what areas is the study presented in the paper with ID 2869 at IJCAI2023 relevant?": {"IJCAI2023>>program>>Main Track>>3863>>authors>>authors_7>>Hua Wu": 0.12872356176376343, "IJCAI2023>>program>>Main Track>>5195>>authors>>authors_3>>Thiago D. Simão": 0.13294774293899536, "IJCAI2023>>program>>Journal Track>>J5941>>authors>>authors_2>>Andrew Searns": 0.132962167263031, "IJCAI2023>>program>>Main Track>>4339>>abstract>>In a number of different fields, including Engeneering, Chemistry and Physics, the design of technological tools and device structures is increasingly supported by deep-learning based methods, which provide suggestions on crucial architectural choices based on the properties that these tools and structures should exhibit. The paper proposes a novel architecture, named GIDnet, to address this inverse design problem, which is based on exploring a suitably defined latent space associated with the possible designs. Among its distinguishing features, GIDnet is capable of identifying the most appropriate starting point for the exploration and of likely converging into a point corresponding to a design that is a feasible one. Results of a thorough experimental activity evidence that GIDnet outperforms earlier approaches in the literature.": 0.133672297000885, "IJCAI2023>>program>>Main Track>>4766>>keywords>>keywords_1>>Multidisciplinary Topics and Applications -> MDA: Bioinformatics": 0.13397109508514404, "IJCAI2023>>program>>Main Track>>4419>>title>>Learning to Act for Perceiving in Partially Unknown Environments": 0.13401347398757935}, "What is the title of the program 1145 in the Main Track of IJCAI2023?": {"IJCAI2023>>program>>Main Track>>1045>>authors>>authors_3>>Elisheva S. Shamash": 0.0827215313911438, "IJCAI2023>>program>>Main Track>>4376>>authors>>authors_1>>Zhaiming Shen": 0.0870656967163086, "IJCAI2023>>program>>Main Track>>1756>>authors>>authors_4>>Conrad Schecker": 0.08716672658920288, "IJCAI2023>>program>>Main Track>>1798>>authors>>authors_4>>Chenxi Ma": 0.08883500099182129, "IJCAI2023>>program>>Main Track>>1621>>authors>>authors_3>>Jörg Hoffmann": 0.09034079313278198, "IJCAI2023>>program>>Main Track>>774>>authors>>authors_5>>Ran Yi": 0.09043341875076294, "IJCAI2023>>program>>Main Track>>5145>>authors>>authors_1>>Chenghao Liu": 0.09104728698730469, "IJCAI2023>>program>>Main Track>>4090>>authors>>authors_1>>Yifan Wu": 0.09113043546676636, "IJCAI2023>>program>>Main Track>>2705>>authors>>authors_5>>Dangyang Chen": 0.09180140495300293, "IJCAI2023>>program>>Main Track>>4586>>authors>>authors_5>>Shinnosuke Takamichi": 0.09205186367034912, "IJCAI2023>>program>>Main Track>>2230>>authors>>authors_3>>Sheila McIlraith": 0.09252381324768066, "IJCAI2023>>program>>Main Track>>621>>authors>>authors_1>>Xiang Li": 0.09327733516693115, "IJCAI2023>>program>>Main Track>>200>>authors>>authors_4>>Yiran Chen": 0.09327954053878784, "IJCAI2023>>program>>Main Track>>1542>>authors>>authors_1>>Liqi Yan": 0.09351855516433716, "IJCAI2023>>program>>Main Track>>1067>>authors>>authors_4>>Jun Zhou": 0.09372293949127197, "IJCAI2023>>program>>Main Track>>5195>>authors>>authors_3>>Thiago D. Simão": 0.09378111362457275, "IJCAI2023>>program>>Main Track>>2178>>authors>>authors_3>>Ruben Solozabal Ochoa de Retana": 0.09378427267074585, "IJCAI2023>>program>>Main Track>>1542>>authors>>authors_4>>Dongfang Liu": 0.09402471780776978, "IJCAI2023>>program>>Main Track>>774>>authors>>authors_3>>Yabiao Wang": 0.09424614906311035, "IJCAI2023>>program>>Main Track>>3832>>authors>>authors_6>>Felix Ulrich-Oltean": 0.09433382749557495}, "Who are the authors of the program 1145 in IJCAI2023?": {"IJCAI2023>>program>>Main Track>>345>>authors>>authors_2>>Zhiwei He": 0.09113419055938721, "IJCAI2023>>program>>Main Track>>4132>>authors>>authors_4>>Jiancheng Lv": 0.09352582693099976, "IJCAI2023>>program>>Main Track>>1067>>authors>>authors_4>>Jun Zhou": 0.09511452913284302, "IJCAI2023>>program>>Main Track>>2754>>authors>>authors_3>>Xupeng Miao": 0.09523040056228638, "IJCAI2023>>program>>Main Track>>1274>>authors>>authors_4>>Zhongwen Rao": 0.09594970941543579, "IJCAI2023>>program>>Main Track>>1045>>authors>>authors_1>>Takayuki Osogami": 0.09668844938278198, "IJCAI2023>>program>>Main Track>>2759>>authors>>authors_1>>Alessandro Daniele": 0.09678423404693604, "IJCAI2023>>program>>Main Track>>1067>>authors>>authors_2>>Fan Liu": 0.09707802534103394, "IJCAI2023>>program>>Main Track>>4051>>authors>>authors_2>>Arunabha Sen": 0.09735119342803955, "IJCAI2023>>program>>Main Track>>1813>>authors>>authors_5>>Naijia Wang": 0.0978437066078186, "IJCAI2023>>program>>Main Track>>3848>>authors>>authors_6>>Diederik M. Roijers": 0.09839850664138794, "IJCAI2023>>program>>Main Track>>1155>>authors>>authors_2>>Yazhou Ren": 0.09892940521240234, "IJCAI2023>>program>>Main Track>>5014>>authors>>authors_7>>Yanjie Fu": 0.09914970397949219, "IJCAI2023>>program>>Main Track>>774>>authors>>authors_4>>Chengjie Wang": 0.09920859336853027, "IJCAI2023>>program>>Main Track>>3540>>authors>>authors_3>>Xuefeng Jiang": 0.09955871105194092, "IJCAI2023>>program>>Main Track>>604>>authors>>authors_8>>Jie Chen": 0.10007810592651367, "IJCAI2023>>program>>Main Track>>4655>>authors>>authors_7>>George Skretas": 0.10046440362930298, "IJCAI2023>>program>>Main Track>>3667>>authors>>authors_7>>Cigdem Aslay": 0.10088139772415161, "IJCAI2023>>program>>Main Track>>408>>authors>>authors_3>>Zejian Li": 0.10089850425720215, "IJCAI2023>>program>>Main Track>>1834>>authors>>authors_3>>Christel Baier": 0.10128974914550781}, "What is the main focus of the paper presented in program 1145 at IJCAI 2023?": {"IJCAI2023>>program>>Main Track>>5195>>authors>>authors_3>>Thiago D. Simão": 0.09517228603363037, "IJCAI2023>>program>>Main Track>>1621>>authors>>authors_3>>Jörg Hoffmann": 0.09841477870941162, "IJCAI2023>>program>>Main Track>>200>>authors>>authors_4>>Yiran Chen": 0.09850704669952393, "IJCAI2023>>program>>Main Track>>1045>>authors>>authors_3>>Elisheva S. Shamash": 0.10038304328918457, "IJCAI2023>>program>>Main Track>>863>>authors>>authors_5>>Kai Chen": 0.10152691602706909, "IJCAI2023>>program>>Main Track>>3832>>authors>>authors_6>>Felix Ulrich-Oltean": 0.102444589138031, "IJCAI2023>>program>>Main Track>>1067>>authors>>authors_4>>Jun Zhou": 0.10283869504928589, "IJCAI2023>>program>>Main Track>>2144>>authors>>authors_8>>Min-Ling Zhang": 0.10498929023742676, "IJCAI2023>>program>>Main Track>>4090>>authors>>authors_1>>Yifan Wu": 0.10532605648040771, "IJCAI2023>>program>>Main Track>>1756>>authors>>authors_4>>Conrad Schecker": 0.10556095838546753, "IJCAI2023>>program>>Main Track>>4586>>authors>>authors_5>>Shinnosuke Takamichi": 0.10561883449554443, "IJCAI2023>>program>>Main Track>>4376>>authors>>authors_1>>Zhaiming Shen": 0.10650402307510376, "IJCAI2023>>program>>Main Track>>1603>>authors>>authors_2>>Maria Polukarov": 0.10845452547073364, "IJCAI2023>>program>>Main Track>>1633>>authors>>authors_3>>John Dickerson": 0.10868465900421143, "IJCAI2023>>program>>Main Track>>2184>>authors>>authors_3>>Kun Wei": 0.10868823528289795, "IJCAI2023>>program>>Main Track>>1540>>authors>>authors_7>>Enhong Chen": 0.10889959335327148, "IJCAI2023>>program>>Main Track>>4051>>authors>>authors_2>>Arunabha Sen": 0.10910874605178833, "IJCAI2023>>program>>Main Track>>11>>keywords>>keywords_2>>Machine Learning -> ML: Deep reinforcement learning": 0.10920131206512451, "IJCAI2023>>program>>Main Track>>3863>>authors>>authors_7>>Hua Wu": 0.10949844121932983, "IJCAI2023>>program>>Main Track>>5171>>authors>>authors_4>>Houqiang Li": 0.11016297340393066}, "What new approach does the paper in program 1145 introduce in the field of natural language processing?": {"IJCAI2023>>program>>Main Track>>2816>>keywords>>keywords_1>>Natural Language Processing -> NLP: Language models": 0.12734287977218628, "IJCAI2023>>program>>Demonstrations Track>>DM5722>>keywords>>keywords_3>>Natural Language Processing -> NLP: Language models": 0.12786102294921875, "IJCAI2023>>program>>Main Track>>1145>>keywords>>keywords_2>>Natural Language Processing -> NLP: Language models": 0.13354307413101196, "IJCAI2023>>program>>Main Track>>5012>>keywords>>keywords_1>>Natural Language Processing -> NLP: Machine translation and multilinguality": 0.1339244246482849, "IJCAI2023>>program>>Demonstrations Track>>DM5686>>keywords>>keywords_9>>Natural Language Processing -> NLP: Speech": 0.1357266902923584, "IJCAI2023>>program>>Main Track>>1145>>keywords>>keywords_1>>Natural Language Processing -> NLP: Question answering": 0.13635748624801636, "IJCAI2023>>program>>Survey Track>>SV5639>>keywords>>keywords_4>>Survey -> Natural Language Processing": 0.1369560956954956, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5757>>keywords>>keywords_1>>AI for Good -> Natural Language Processing": 0.13711810111999512, "IJCAI2023>>program>>Demonstrations Track>>DM5722>>keywords>>keywords_1>>Natural Language Processing -> NLP: Tools": 0.1383683681488037, "IJCAI2023>>program>>Main Track>>2358>>title>>Linguistic More: Taking a Further Step toward Efficient and Accurate Scene Text Recognition": 0.13871252536773682, "IJCAI2023>>program>>Demonstrations Track>>DM5686>>keywords>>keywords_7>>Natural Language Processing -> NLP: Applications": 0.1391240358352661, "IJCAI2023>>program>>Main Track>>3260>>keywords>>keywords_1>>Natural Language Processing -> NLP: Summarization": 0.13963854312896729, "IJCAI2023>>program>>Demonstrations Track>>DM5722>>keywords>>keywords_5>>Natural Language Processing -> NLP: Named entities": 0.1406170129776001, "IJCAI2023>>program>>Demonstrations Track>>DM5722>>keywords>>keywords_4>>Natural Language Processing -> NLP: Sentiment analysis, stylistic analysis, and argument mining": 0.14068138599395752, "IJCAI2023>>program>>Main Track>>4586>>keywords>>keywords_1>>Natural Language Processing -> NLP: Speech": 0.141127347946167, "IJCAI2023>>program>>Main Track>>4148>>keywords>>keywords_1>>Natural Language Processing -> NLP: Applications": 0.14124035835266113}, "What are the keywords associated with the paper presented in program 1145 at IJCAI 2023?": {"IJCAI2023>>program>>Best Papers from Sister Conferences Track>>SC25>>keywords>>keywords_1>>Sister Conferences Best Papers -> Machine Learning": 0.08738338947296143, "IJCAI2023>>program>>Best Papers from Sister Conferences Track>>SC12>>keywords>>keywords_1>>Sister Conferences Best Papers -> Constraint Satisfaction and Optimization": 0.0880017876625061, "IJCAI2023>>program>>Best Papers from Sister Conferences Track>>SC3>>keywords>>keywords_3>>Sister Conferences Best Papers -> Humans and AI": 0.08933389186859131, "IJCAI2023>>program>>Best Papers from Sister Conferences Track>>SC3>>keywords>>keywords_2>>Sister Conferences Best Papers -> Data Mining": 0.09049886465072632, "IJCAI2023>>program>>Best Papers from Sister Conferences Track>>SC3>>keywords>>keywords_1>>Sister Conferences Best Papers -> AI Ethics, Trust, Fairness": 0.09091764688491821, "IJCAI2023>>program>>Main Track>>4226>>keywords>>keywords_2>>Computer Vision -> CV: Applications": 0.09108424186706543, "IJCAI2023>>program>>Main Track>>451>>keywords>>keywords_3>>Machine Learning -> ML: Probabilistic machine learning": 0.09131073951721191, "IJCAI2023>>program>>Best Papers from Sister Conferences Track>>SC12>>keywords>>keywords_4>>Sister Conferences Best Papers -> Search": 0.09327000379562378, "IJCAI2023>>program>>Best Papers from Sister Conferences Track>>SC12>>keywords>>keywords_2>>Sister Conferences Best Papers -> Knowledge Representation and Reasoning": 0.09358131885528564, "IJCAI2023>>program>>Main Track>>1593>>keywords>>keywords_2>>Computer Vision -> CV: Applications": 0.09405887126922607, "IJCAI2023>>program>>Survey Track>>SV5487>>keywords>>keywords_2>>Survey -> Computer Vision": 0.09432882070541382, "IJCAI2023>>program>>Journal Track>>J5943>>keywords>>keywords_7>>Machine Learning -> ML: Symbolic methods": 0.09490960836410522, "IJCAI2023>>program>>Main Track>>5195>>authors>>authors_3>>Thiago D. Simão": 0.09669691324234009, "IJCAI2023>>program>>Main Track>>4168>>keywords>>keywords_1>>Computer Vision -> CV: Computational photography": 0.09711772203445435, "IJCAI2023>>program>>Main Track>>4122>>keywords>>keywords_1>>Game Theory and Economic Paradigms -> GTEP: Fair division": 0.09717059135437012}, "What is the title of the paper with code 460 in the IJCAI 2023 Main Track?": {"IJCAI2023>>program>>Main Track>>4580>>authors>>authors_1>>Carlos Hernández": 0.09394800662994385, "IJCAI2023>>program>>Main Track>>4766>>authors>>authors_4>>Abdoulaye Banire Diallo": 0.09884119033813477, "IJCAI2023>>program>>Main Track>>2178>>authors>>authors_3>>Ruben Solozabal Ochoa de Retana": 0.0993884801864624, "IJCAI2023>>program>>Main Track>>902>>authors>>authors_1>>Tong Liu": 0.10016787052154541, "IJCAI2023>>program>>Main Track>>648>>authors>>authors_8>>Yue Qi": 0.10025262832641602, "IJCAI2023>>program>>Main Track>>4206>>authors>>authors_2>>Hendrik Molter": 0.10042428970336914, "IJCAI2023>>program>>Main Track>>5195>>authors>>authors_3>>Thiago D. Simão": 0.10122251510620117, "IJCAI2023>>program>>Main Track>>2038>>authors>>authors_1>>Zhongjing Du": 0.10250258445739746, "IJCAI2023>>program>>Main Track>>4376>>authors>>authors_1>>Zhaiming Shen": 0.10270315408706665, "IJCAI2023>>program>>Main Track>>4586>>authors>>authors_5>>Shinnosuke Takamichi": 0.10276460647583008, "IJCAI2023>>program>>Main Track>>4607>>authors>>authors_1>>Nima Motamed": 0.10336130857467651, "IJCAI2023>>program>>Main Track>>863>>authors>>authors_5>>Kai Chen": 0.10464376211166382, "IJCAI2023>>program>>Main Track>>1798>>authors>>authors_4>>Chenxi Ma": 0.10475730895996094, "IJCAI2023>>program>>Main Track>>4276>>authors>>authors_8>>Bettina Könighofer": 0.10612165927886963, "IJCAI2023>>program>>Main Track>>4586>>authors>>authors_1>>Takaaki Saeki": 0.10678797960281372, "IJCAI2023>>program>>Main Track>>1493>>authors>>authors_5>>Dong Yan": 0.1068316102027893, "IJCAI2023>>program>>Main Track>>4732>>authors>>authors_1>>Martin Svatoš": 0.10695034265518188, "IJCAI2023>>program>>Journal Track>>J5939>>authors>>authors_1>>Elias Schede": 0.10714048147201538, "IJCAI2023>>program>>Main Track>>3497>>authors>>authors_2>>Jun Yuan": 0.1080288290977478, "IJCAI2023>>program>>Main Track>>3863>>authors>>authors_7>>Hua Wu": 0.10808324813842773}, "Who are the authors of the paper titled 'ContrastMotion: Self-supervised Scene Motion Learning for Large-Scale LiDAR Point Clouds'?": {"IJCAI2023>>program>>Survey Track>>SV5660>>authors>>authors_2>>Kun Zhou": 0.17496538162231445, "IJCAI2023>>program>>Survey Track>>SV5630>>authors>>authors_1>>Zhichun Guo": 0.17801052331924438, "IJCAI2023>>program>>Survey Track>>SV5593>>authors>>authors_1>>Chengyi Liu": 0.17875158786773682, "IJCAI2023>>program>>Survey Track>>SV5593>>authors>>authors_8>>Qing Li": 0.17885911464691162, "IJCAI2023>>program>>Main Track>>1626>>authors>>authors_1>>Peizheng Li": 0.1789090633392334, "IJCAI2023>>program>>Main Track>>863>>authors>>authors_7>>Chang Liu": 0.17917996644973755, "IJCAI2023>>program>>Survey Track>>SV5487>>authors>>authors_2>>Zongxiong Chen": 0.17962390184402466, "IJCAI2023>>program>>Main Track>>4454>>authors>>authors_4>>Kurt Mehlhorn": 0.18144512176513672, "IJCAI2023>>program>>Main Track>>1834>>authors>>authors_3>>Christel Baier": 0.18155109882354736, "IJCAI2023>>program>>Survey Track>>SV5660>>authors>>authors_3>>Wayne Xin Zhao": 0.18216288089752197, "IJCAI2023>>program>>Main Track>>1384>>authors>>authors_1>>Xixuan Hao": 0.1832331418991089, "IJCAI2023>>program>>Main Track>>398>>authors>>authors_3>>Zhongang Qi": 0.18326729536056519, "IJCAI2023>>program>>Survey Track>>SV5460>>authors>>authors_2>>Jing Liu": 0.18330180644989014, "IJCAI2023>>program>>Journal Track>>J5924>>authors>>authors_6>>Daniele Magazzeni": 0.18351173400878906, "IJCAI2023>>program>>Survey Track>>SV5593>>authors>>authors_5>>Hang Li": 0.18378490209579468, "IJCAI2023>>calls>>Main Track>>Accepted Papers List>>141>>authors>>authors_3>>Lun Du": 0.18382304906845093, "IJCAI2023>>program>>Main Track>>4062>>authors>>authors_3>>Pascal Lenzner": 0.18386560678482056, "IJCAI2023>>program>>Main Track>>863>>authors>>authors_8>>Jie Chen": 0.18405473232269287, "IJCAI2023>>program>>Main Track>>731>>authors>>authors_2>>Massih-Reza Amini": 0.18438869714736938, "IJCAI2023>>program>>Survey Track>>SV5569>>authors>>authors_4>>He Zhang": 0.18458008766174316}, "What are the keywords of paper 460?": {"IJCAI2023>>program>>Main Track>>460>>keywords>>keywords_3>>Computer Vision -> CV: Scene analysis and understanding": 0.16458815336227417, "IJCAI2023>>program>>Main Track>>4226>>keywords>>keywords_1>>Computer Vision -> CV: Machine learning for vision": 0.16946041584014893, "IJCAI2023>>program>>Best Papers from Sister Conferences Track>>SC12>>keywords>>keywords_4>>Sister Conferences Best Papers -> Search": 0.17006516456604004, "IJCAI2023>>program>>Main Track>>4480>>keywords>>keywords_1>>Game Theory and Economic Paradigms -> GTEP: Noncooperative games": 0.17121076583862305, "IJCAI2023>>program>>Main Track>>3663>>keywords>>keywords_1>>Search -> S: Combinatorial search and optimisation": 0.17192208766937256, "IJCAI2023>>program>>Journal Track>>J5934>>keywords>>keywords_1>>Constraint Satisfaction and Optimization -> CSO: Satisfiabilty": 0.1741318702697754, "IJCAI2023>>calls>>Journal Track>>Accepted Papers List>>J5950>>keywords>>keywords_1>>Planning": 0.17442506551742554, "IJCAI2023>>program>>Main Track>>460>>keywords>>keywords_1>>Computer Vision -> CV: Motion and tracking": 0.1751512885093689, "IJCAI2023>>program>>Survey Track>>SV5660>>keywords>>keywords_1>>Survey -> Natural Language Processing": 0.17555475234985352, "IJCAI2023>>program>>Journal Track>>J5934>>keywords>>keywords_2>>Constraint Satisfaction and Optimization -> CSO: Solvers and tools": 0.17575037479400635, "IJCAI2023>>program>>Main Track>>4068>>keywords>>keywords_1>>Game Theory and Economic Paradigms -> GTEP: Computational social choice": 0.17677247524261475, "IJCAI2023>>calls>>Main Track>>Accepted Papers List>>141>>keywords>>keywords_1>>Data Mining -> DM: Mining graphs": 0.17708927392959595, "IJCAI2023>>program>>Journal Track>>J5943>>keywords>>keywords_4>>Constraint Satisfaction and Optimization -> CSO: Satisfiabilty": 0.17745012044906616, "IJCAI2023>>program>>Journal Track>>J5930>>keywords>>keywords_3>>Robotics -> ROB: Motion and path planning": 0.17776578664779663, "IJCAI2023>>program>>Best Papers from Sister Conferences Track>>SC12>>keywords>>keywords_1>>Sister Conferences Best Papers -> Constraint Satisfaction and Optimization": 0.1778469681739807}, "Can you provide a brief about the paper 'ContrastMotion: Self-supervised Scene Motion Learning for Large-Scale LiDAR Point Clouds'?": {"IJCAI2023>>program>>Main Track>>460>>title>>ContrastMotion: Self-supervised Scene Motion Learning for Large-Scale LiDAR Point Clouds": 0.09055256843566895, "IJCAI2023>>program>>Main Track>>460>>abstract>>In this paper, we propose a novel self-supervised motion estimator for LiDAR-based autonomous driving via BEV representation. Different from usually adopted self-supervised strategies for data-level structure consistency, we predict scene motion via feature-level consistency between pillars in consecutive frames, which can eliminate the effect caused by noise points and view-changing point clouds in dynamic scenes. Specifically, we propose Soft Discriminative Loss that provides the network with more pseudo-supervised signals to learn discriminative and robust features in a contrastive learning manner. We also propose Gated Multi-Frame Fusion block that learns valid compensation between point cloud frames automatically to enhance feature extraction. Finally, pillar association is proposed to predict pillar correspondence probabilities based on feature distance, and whereby further predicts scene motion. Extensive experiments show the effectiveness and superiority of our ContrastMotion on both scene flow and motion prediction tasks.": 0.10030156373977661, "IJCAI2023>>program>>Main Track>>953>>abstract>>For many driving safety applications, it is of great importance to accurately register LiDAR point clouds generated on distant moving vehicles. However, such point clouds have extremely different point density and sensor perspective on the same object, making registration on such point clouds very hard. In this paper, we propose a novel feature extraction framework, called APR, for online distant point cloud registration. Specifically, APR leverages an autoencoder design, where the autoencoder reconstructs a denser aggregated point cloud with several frames instead of the original single input point cloud. Our design forces the encoder to extract features with rich local geometry information based on one single input point cloud. Such features are then used for online distant point cloud registration. We conduct extensive experiments against state-of-the-art (SOTA) feature extractors on KITTI and nuScenes datasets. Results show that APR outperforms all other extractors by a large margin, increasing average registration recall of SOTA extractors by 7.1% on LoKITTI and 4.6% on LoNuScenes. Code is available at https://github.com/liuQuan98/APR.": 0.15163582563400269}, "In the proposed method of the paper 'ContrastMotion: Self-supervised Scene Motion Learning for Large-Scale LiDAR Point Clouds', what are the strategies used to learn valid compensation between point cloud frames?": {"IJCAI2023>>program>>Main Track>>460>>abstract>>In this paper, we propose a novel self-supervised motion estimator for LiDAR-based autonomous driving via BEV representation. Different from usually adopted self-supervised strategies for data-level structure consistency, we predict scene motion via feature-level consistency between pillars in consecutive frames, which can eliminate the effect caused by noise points and view-changing point clouds in dynamic scenes. Specifically, we propose Soft Discriminative Loss that provides the network with more pseudo-supervised signals to learn discriminative and robust features in a contrastive learning manner. We also propose Gated Multi-Frame Fusion block that learns valid compensation between point cloud frames automatically to enhance feature extraction. Finally, pillar association is proposed to predict pillar correspondence probabilities based on feature distance, and whereby further predicts scene motion. Extensive experiments show the effectiveness and superiority of our ContrastMotion on both scene flow and motion prediction tasks.": 0.08842974901199341, "IJCAI2023>>program>>Main Track>>460>>title>>ContrastMotion: Self-supervised Scene Motion Learning for Large-Scale LiDAR Point Clouds": 0.09734660387039185, "IJCAI2023>>program>>Main Track>>953>>abstract>>For many driving safety applications, it is of great importance to accurately register LiDAR point clouds generated on distant moving vehicles. However, such point clouds have extremely different point density and sensor perspective on the same object, making registration on such point clouds very hard. In this paper, we propose a novel feature extraction framework, called APR, for online distant point cloud registration. Specifically, APR leverages an autoencoder design, where the autoencoder reconstructs a denser aggregated point cloud with several frames instead of the original single input point cloud. Our design forces the encoder to extract features with rich local geometry information based on one single input point cloud. Such features are then used for online distant point cloud registration. We conduct extensive experiments against state-of-the-art (SOTA) feature extractors on KITTI and nuScenes datasets. Results show that APR outperforms all other extractors by a large margin, increasing average registration recall of SOTA extractors by 7.1% on LoKITTI and 4.6% on LoNuScenes. Code is available at https://github.com/liuQuan98/APR.": 0.15159368515014648}, "What is the title of the paper with id J5685 in Journal Track?": {"IJCAI2023>>program>>Journal Track>>J5939>>authors>>authors_1>>Elias Schede": 0.12908124923706055, "IJCAI2023>>calls>>Journal Track>>Accepted Papers List>>J5950>>authors>>authors_5>>Subbarao Kambhampati": 0.13940149545669556, "IJCAI2023>>program>>Journal Track>>J5758>>authors>>authors_1>>Sriram Ganapathi Subramanian": 0.14015954732894897, "IJCAI2023>>program>>Journal Track>>J5925>>authors>>authors_2>>Mouaz Al-Mallah": 0.14054006338119507, "IJCAI2023>>program>>Journal Track>>J5552>>authors>>authors_1>>Nieves Montes": 0.1447528600692749, "IJCAI2023>>program>>Journal Track>>J5758>>authors>>authors_2>>Matthew E. Taylor": 0.14490175247192383, "IJCAI2023>>program>>Journal Track>>J5927>>authors>>authors_2>>Janardhan Rao Doppa": 0.14543873071670532, "IJCAI2023>>program>>Journal Track>>J5685>>authors>>authors_3>>David Martins de Matos": 0.1490437388420105, "IJCAI2023>>program>>Main Track>>1493>>authors>>authors_5>>Dong Yan": 0.149416983127594, "IJCAI2023>>program>>Journal Track>>J5935>>authors>>authors_1>>Patrick Rodler": 0.14964592456817627, "IJCAI2023>>program>>Journal Track>>J5586>>authors>>authors_2>>Pavel Naumov": 0.15152305364608765, "IJCAI2023>>program>>Journal Track>>J5950>>authors>>authors_2>>Hankz Hankui Zhuo": 0.15174269676208496, "IJCAI2023>>program>>Main Track>>1626>>authors>>authors_1>>Peizheng Li": 0.15194040536880493, "IJCAI2023>>calls>>Journal Track>>Accepted Papers List>>J5950>>authors>>authors_1>>Kebing Jin": 0.1527308225631714, "IJCAI2023>>program>>Journal Track>>J5685>>authors>>authors_1>>Eugénio Ribeiro": 0.15307319164276123, "IJCAI2023>>program>>Journal Track>>J5552>>authors>>authors_2>>Nardine Osman": 0.1532486081123352, "IJCAI2023>>calls>>Journal Track>>Accepted Papers List>>J5950>>keywords>>keywords_3>>Mixed Planning": 0.15382534265518188, "IJCAI2023>>program>>Main Track>>1493>>authors>>authors_6>>Jun Zhu": 0.15397214889526367}, "Who is the second author of the paper with id J5685?": {"IJCAI2023>>program>>Journal Track>>J5758>>authors>>authors_2>>Matthew E. Taylor": 0.13565289974212646, "IJCAI2023>>calls>>Journal Track>>Accepted Papers List>>J5950>>authors>>authors_3>>Zhanhao Xiao": 0.14347368478775024, "IJCAI2023>>calls>>Journal Track>>Accepted Papers List>>J5950>>authors>>authors_5>>Subbarao Kambhampati": 0.14566141366958618, "IJCAI2023>>program>>Journal Track>>J5758>>authors>>authors_1>>Sriram Ganapathi Subramanian": 0.14652609825134277, "IJCAI2023>>program>>Journal Track>>J5685>>authors>>authors_3>>David Martins de Matos": 0.14858222007751465, "IJCAI2023>>program>>Journal Track>>J5931>>authors>>authors_2>>Nariaki Kitamura": 0.14979934692382812, "IJCAI2023>>program>>Main Track>>108>>authors>>authors_2>>Peter J. Stuckey": 0.15097588300704956, "IJCAI2023>>program>>Main Track>>1626>>authors>>authors_1>>Peizheng Li": 0.15172749757766724, "IJCAI2023>>program>>Main Track>>4760>>authors>>authors_2>>Andrzej Kaczmarczyk": 0.15358811616897583, "IJCAI2023>>program>>Main Track>>1834>>authors>>authors_2>>Simon Jantsch": 0.1543569564819336, "IJCAI2023>>program>>Main Track>>1856>>authors>>authors_2>>Kate Larson": 0.15520024299621582, "IJCAI2023>>program>>Journal Track>>J5919>>authors>>authors_3>>Pascal Van Hentenryck": 0.1556549072265625, "IJCAI2023>>program>>Survey Track>>SV5487>>authors>>authors_2>>Zongxiong Chen": 0.15682601928710938, "IJCAI2023>>program>>Journal Track>>J5685>>authors>>authors_1>>Eugénio Ribeiro": 0.15938472747802734, "IJCAI2023>>program>>Journal Track>>J5924>>authors>>authors_6>>Daniele Magazzeni": 0.1602628231048584, "IJCAI2023>>calls>>Journal Track>>Accepted Papers List>>J5950>>authors>>authors_1>>Kebing Jin": 0.16028344631195068, "IJCAI2023>>program>>Main Track>>398>>authors>>authors_3>>Zhongang Qi": 0.16070276498794556, "IJCAI2023>>program>>Main Track>>3964>>authors>>authors_2>>Bojie Shen": 0.16071486473083496}, "What is the main aim of the paper's proposed method?": {"IJCAI2023>>calls>>Main Track>>Accepted Papers List>>141>>abstract>>Recent years have witnessed the great potential of attention mechanism in graph representation learning. However, while variants of attention-based GNNs are setting new benchmarks for numerous real-world datasets, recent works have pointed out that their induced attentions are less robust and generalizable against noisy graphs due to lack of direct supervision. In this paper, we present a new framework which utilizes the tool of causality to provide a powerful supervision signal for the learning process of attention functions. Specifically, we estimate the direct causal effect of attention to the final prediction, and then maximize such effect to guide attention attending to more meaningful neighbors. Our method can serve as a plug-and-play module for any canonical attention-based GNNs in an end-to-end fashion. Extensive experiments on a wide range of real-world datasets demonstrate the effectiveness of our method, which outperforms the state-of-the-art methods by a large margin.": 0.1853540539741516, "IJCAI2023>>program>>Main Track>>5293>>abstract>>We consider a multi-issue election setting over a set of possibly interdependent issues with the goal of achieving proportional representation of the views of the electorate. To this end, we employ a proportionality criterion suggested by Skowron and Gorecki [2022], that guarantees fair representation for all groups of voters of sufficient size. For this criterion, there exist rules that perform well in the case where all the issues have a binary domain and are independent of each other. In particular, this has been shown for Proportional Approval Voting (PAV) and for the Method of Equal Shares (MES). In this paper, we go two steps further: we generalize these guarantees for issues with a non-binary domain, and, most importantly, we consider extensions to elections with dependencies among issues, where we identify restrictions that lead to analogous results. To achieve this, we define appropriate generalizations of PAV and MES to handle conditional ballots. In addition to proportionality considerations, we also examine the computational properties of the conditional version of MES. Our findings indicate that the conditional case poses additional challenges and differs significantly from the unconditional one, both in terms of proportionality guarantees and computational complexity.": 0.19072061777114868, "IJCAI2023>>program>>Main Track>>863>>authors>>authors_8>>Jie Chen": 0.19161075353622437, "IJCAI2023>>program>>Main Track>>1493>>authors>>authors_5>>Dong Yan": 0.19185715913772583}, "In which area does the paper with id J5685 contribute? Give one of the fields provided": {"IJCAI2023>>program>>Journal Track>>J5758>>authors>>authors_1>>Sriram Ganapathi Subramanian": 0.1796594262123108, "IJCAI2023>>program>>Journal Track>>J5685>>authors>>authors_3>>David Martins de Matos": 0.181249737739563, "IJCAI2023>>calls>>Journal Track>>Accepted Papers List>>J5950>>authors>>authors_5>>Subbarao Kambhampati": 0.18526339530944824, "IJCAI2023>>program>>Journal Track>>J5685>>authors>>authors_1>>Eugénio Ribeiro": 0.18684566020965576, "IJCAI2023>>program>>Main Track>>1626>>authors>>authors_1>>Peizheng Li": 0.187350332736969, "IJCAI2023>>program>>Journal Track>>J5927>>authors>>authors_2>>Janardhan Rao Doppa": 0.18790459632873535, "IJCAI2023>>program>>Journal Track>>J5925>>authors>>authors_2>>Mouaz Al-Mallah": 0.18874382972717285, "IJCAI2023>>program>>Journal Track>>J5938>>authors>>authors_1>>Charles K. Assaad": 0.19031298160552979, "IJCAI2023>>program>>Journal Track>>J5935>>authors>>authors_1>>Patrick Rodler": 0.1903533935546875, "IJCAI2023>>program>>Best Papers from Sister Conferences Track>>SC7>>authors>>authors_1>>Dario Simionato": 0.19055354595184326, "IJCAI2023>>program>>Main Track>>4653>>authors>>authors_3>>Min Chi": 0.190956711769104, "IJCAI2023>>program>>Main Track>>4969>>authors>>authors_1>>Dabin Zhang": 0.19126224517822266, "IJCAI2023>>program>>Journal Track>>J5758>>authors>>authors_2>>Matthew E. Taylor": 0.19135749340057373, "IJCAI2023>>program>>Journal Track>>J5919>>authors>>authors_3>>Pascal Van Hentenryck": 0.1919020414352417, "IJCAI2023>>program>>Journal Track>>J5939>>authors>>authors_1>>Elias Schede": 0.19278615713119507, "IJCAI2023>>program>>Journal Track>>J5923>>authors>>authors_1>>Loïs Vanhée": 0.19292044639587402, "IJCAI2023>>program>>Journal Track>>J5552>>authors>>authors_1>>Nieves Montes": 0.19303220510482788, "IJCAI2023>>program>>Journal Track>>J5552>>authors>>authors_2>>Nardine Osman": 0.1932496428489685}, "The authors of paper J5685 propose a new approach to recognize general-purpose communicative functions. What type of network does this approach use and why?": {"IJCAI2023>>program>>Journal Track>>J5685>>abstract>>From the perspective of a dialog system, the identification of the intention behind the segments in a dialog is important, as it provides cues regarding the information present in the segments and how they should be interpreted. The ISO 24617-2 standard for dialog act annotation defines a hierarchically organized set of general-purpose communicative functions that correspond to different intentions that are relevant in the context of a dialog. In this paper, we explore the automatic recognition of these functions. To do so, we propose to adapt existing approaches to dialog act recognition, so that they can deal with the hierarchical classification problem. More specifically, we propose the use of an end-to-end hierarchical network with cascading outputs and maximum a posteriori path estimation to predict the communicative function at each level of the hierarchy, preserve the dependencies between the functions in the path, and decide at which level to stop. Additionally, we rely on transfer learning processes to address the data scarcity problem. Our experiments on the DialogBank show that this approach outperforms both flat and hierarchical approaches based on multiple classifiers and that each of its components plays an important role in the recognition of general-purpose communicative functions.": 0.10141050815582275, "IJCAI2023>>program>>Journal Track>>J5685>>title>>Automatic Recognition of the General-Purpose Communicative Functions Defined by the ISO 24617-2 Standard for Dialog Act Annotation (Extended Abstract)": 0.14967972040176392}, "What is the title of the program with the code 4961 at the IJCAI2023 Main Track?": {"IJCAI2023>>program>>Main Track>>4276>>authors>>authors_8>>Bettina Könighofer": 0.09253895282745361, "IJCAI2023>>program>>Main Track>>4766>>authors>>authors_4>>Abdoulaye Banire Diallo": 0.09282928705215454, "IJCAI2023>>program>>Main Track>>5311>>authors>>authors_4>>Yakob Kahane": 0.09309440851211548, "IJCAI2023>>program>>Main Track>>2038>>authors>>authors_1>>Zhongjing Du": 0.09421330690383911, "IJCAI2023>>program>>Main Track>>1756>>authors>>authors_4>>Conrad Schecker": 0.09429746866226196, "IJCAI2023>>program>>Main Track>>4206>>authors>>authors_2>>Hendrik Molter": 0.09577786922454834, "IJCAI2023>>program>>Main Track>>485>>authors>>authors_4>>Guojie Song": 0.09586644172668457, "IJCAI2023>>program>>Main Track>>4672>>authors>>authors_6>>Jonas Schmidt": 0.09589630365371704, "IJCAI2023>>program>>Main Track>>4376>>authors>>authors_1>>Zhaiming Shen": 0.09589934349060059, "IJCAI2023>>program>>Main Track>>2471>>authors>>authors_2>>Boris Konev": 0.09602069854736328, "IJCAI2023>>program>>Main Track>>4607>>authors>>authors_1>>Nima Motamed": 0.09647345542907715, "IJCAI2023>>program>>Main Track>>5012>>authors>>authors_4>>Yidong Chen": 0.0966116189956665, "IJCAI2023>>program>>Main Track>>4090>>authors>>authors_1>>Yifan Wu": 0.09782785177230835, "IJCAI2023>>program>>Main Track>>4636>>authors>>authors_9>>James T. Neal": 0.09801137447357178, "IJCAI2023>>program>>Main Track>>4691>>authors>>authors_1>>Anaëlle Wilczynski": 0.0980481505393982, "IJCAI2023>>program>>Main Track>>1542>>authors>>authors_4>>Dongfang Liu": 0.0986860990524292, "IJCAI2023>>program>>Main Track>>3482>>authors>>authors_5>>Truyen Tran": 0.0987427830696106, "IJCAI2023>>program>>Main Track>>2471>>authors>>authors_6>>Michael Zakharyaschev": 0.09880310297012329, "IJCAI2023>>program>>Main Track>>2471>>authors>>authors_3>>Vladislav Ryzhikov": 0.09884560108184814}, "Who are the authors of the program 'Capturing the Long-Distance Dependency in the Control Flow Graph via Structural-Guided Attention for Bug Localization'?": {"IJCAI2023>>program>>Main Track>>5155>>authors>>authors_2>>Guodong Long": 0.1585676670074463, "IJCAI2023>>program>>Main Track>>4454>>authors>>authors_4>>Kurt Mehlhorn": 0.16526341438293457, "IJCAI2023>>program>>Main Track>>1834>>authors>>authors_2>>Simon Jantsch": 0.16788184642791748, "IJCAI2023>>program>>Main Track>>1223>>authors>>authors_2>>Jose Campos": 0.16794651746749878, "IJCAI2023>>program>>Main Track>>1834>>authors>>authors_3>>Christel Baier": 0.1680811643600464, "IJCAI2023>>program>>Main Track>>580>>authors>>authors_2>>Yuesheng Zhu": 0.16822832822799683, "IJCAI2023>>program>>Main Track>>3414>>authors>>authors_2>>Guodong Long": 0.16866737604141235, "IJCAI2023>>program>>Main Track>>2587>>authors>>authors_5>>Gongping Yang": 0.16876733303070068, "IJCAI2023>>program>>Main Track>>1716>>authors>>authors_4>>Caihua Liu": 0.16888093948364258, "IJCAI2023>>program>>Survey Track>>SV5554>>authors>>authors_1>>Longbing Cao": 0.16941362619400024, "IJCAI2023>>program>>Main Track>>3395>>authors>>authors_1>>Haolong Xiang": 0.16960716247558594, "IJCAI2023>>program>>Main Track>>1736>>authors>>authors_1>>Feng Wu": 0.16991156339645386, "IJCAI2023>>program>>Journal Track>>J5758>>authors>>authors_1>>Sriram Ganapathi Subramanian": 0.17083436250686646, "IJCAI2023>>program>>Survey Track>>SV5569>>authors>>authors_3>>Longxiang Gao": 0.17092609405517578, "IJCAI2023>>program>>Journal Track>>J5924>>authors>>authors_6>>Daniele Magazzeni": 0.1714116930961609, "IJCAI2023>>program>>Main Track>>3863>>authors>>authors_8>>Haifeng Wang": 0.17153435945510864, "IJCAI2023>>program>>Main Track>>251>>authors>>authors_2>>Feng Zhao": 0.17197787761688232, "IJCAI2023>>program>>Survey Track>>SV5569>>authors>>authors_2>>Yong Xiang": 0.17205548286437988, "IJCAI2023>>program>>Main Track>>4168>>authors>>authors_4>>Sheng-Jun Huang": 0.17207717895507812, "IJCAI2023>>program>>Survey Track>>SV5648>>authors>>authors_5>>Pradeep K. Murukannaiah": 0.17215251922607422}, "What problem does the program with the code 4961 at the IJCAI2023 Main Track aim to address?": {"IJCAI2023>>program>>Main Track>>4691>>authors>>authors_1>>Anaëlle Wilczynski": 0.10963088274002075, "IJCAI2023>>program>>Main Track>>2471>>authors>>authors_2>>Boris Konev": 0.10978835821151733, "IJCAI2023>>program>>Main Track>>3482>>authors>>authors_5>>Truyen Tran": 0.11057573556900024, "IJCAI2023>>program>>Main Track>>4636>>authors>>authors_9>>James T. Neal": 0.11089861392974854, "IJCAI2023>>program>>Main Track>>4276>>authors>>authors_8>>Bettina Könighofer": 0.11125147342681885, "IJCAI2023>>program>>Main Track>>1756>>authors>>authors_4>>Conrad Schecker": 0.11133354902267456, "IJCAI2023>>program>>Main Track>>4206>>authors>>authors_2>>Hendrik Molter": 0.11174553632736206, "IJCAI2023>>program>>Main Track>>1798>>authors>>authors_4>>Chenxi Ma": 0.11288440227508545, "IJCAI2023>>program>>Main Track>>4672>>authors>>authors_6>>Jonas Schmidt": 0.11299145221710205, "IJCAI2023>>program>>Main Track>>2471>>authors>>authors_6>>Michael Zakharyaschev": 0.11368274688720703, "IJCAI2023>>program>>Main Track>>3948>>authors>>authors_3>>Yongmei Liu": 0.11372208595275879, "IJCAI2023>>program>>Main Track>>4607>>authors>>authors_1>>Nima Motamed": 0.11399275064468384, "IJCAI2023>>program>>Main Track>>621>>authors>>authors_1>>Xiang Li": 0.11401933431625366, "IJCAI2023>>program>>Main Track>>4376>>authors>>authors_1>>Zhaiming Shen": 0.11411941051483154, "IJCAI2023>>program>>Main Track>>2847>>authors>>authors_6>>Tian Gao": 0.11414915323257446, "IJCAI2023>>program>>Main Track>>906>>authors>>authors_3>>Sutanay Choudhury": 0.11439800262451172}, "What is the novel bug localization model proposed in the program 'Capturing the Long-Distance Dependency in the Control Flow Graph via Structural-Guided Attention for Bug Localization'?": {"IJCAI2023>>program>>Main Track>>4961>>title>>Capturing the Long-Distance Dependency in the Control Flow Graph via Structural-Guided Attention for Bug Localization": 0.07980120182037354, "IJCAI2023>>program>>Main Track>>4961>>abstract>>To alleviate the burden of software maintenance, bug localization, which aims to automatically locate the buggy source files based on the bug report, has drawn significant attention in the software mining community. Recent studies indicate that the program structure in source code carries more semantics reflecting the program behavior, which is beneficial for bug localization. Benefiting from the rich structural information in the Control Flow Graph (CFG), CFG-based bug localization methods have achieved the state-of-the-art performance. Existing CFG-based methods extract the semantic feature from the CFG via the graph neural network. However, the step-wise feature propagation in the graph neural network suffers from the problem of information loss when the propagation distance is long, while the long-distance dependency is rather common in the CFG. In this paper, we argue that the long-distance dependency is crucial for feature extraction from the CFG, and propose a novel bug localization model named sgAttention. In sgAttention, a particularly designed structural-guided attention is employed to globally capture the information in the CFG, where features of irrelevant nodes are masked for each node to facilitate better feature extraction from the CFG. Experimental results on four widely-used open-source software projects indicate that sgAttention averagely improves the state-of-the-art bug localization methods by 32.9\\% and 29.2\\% and the state-of-the-art pre-trained models by 5.8\\%  and 4.9\\% in terms of MAP and MRR, respectively.": 0.08427238464355469}, "What are the research areas of the program with the code 4961, presented at the IJCAI2023 Main Track?": {"IJCAI2023>>program>>Main Track>>4636>>authors>>authors_9>>James T. Neal": 0.09208375215530396, "IJCAI2023>>program>>Main Track>>4484>>authors>>authors_4>>Francesco Pasquale": 0.0930793285369873, "IJCAI2023>>program>>Main Track>>4376>>authors>>authors_1>>Zhaiming Shen": 0.09476858377456665, "IJCAI2023>>program>>Main Track>>3863>>authors>>authors_7>>Hua Wu": 0.09520727396011353, "IJCAI2023>>program>>Main Track>>3482>>authors>>authors_5>>Truyen Tran": 0.09535163640975952, "IJCAI2023>>program>>Main Track>>396>>authors>>authors_4>>Chi-Man Pun": 0.09611833095550537, "IJCAI2023>>program>>Main Track>>1621>>authors>>authors_3>>Jörg Hoffmann": 0.09624946117401123, "IJCAI2023>>program>>Main Track>>2178>>authors>>authors_3>>Ruben Solozabal Ochoa de Retana": 0.09798145294189453, "IJCAI2023>>program>>Main Track>>4206>>authors>>authors_2>>Hendrik Molter": 0.09812581539154053, "IJCAI2023>>program>>Main Track>>1756>>authors>>authors_4>>Conrad Schecker": 0.09824782609939575, "IJCAI2023>>program>>Main Track>>4580>>authors>>authors_1>>Carlos Hernández": 0.09840530157089233, "IJCAI2023>>program>>Main Track>>906>>authors>>authors_3>>Sutanay Choudhury": 0.09975689649581909}, "What is the title of the main track program 2099 in IJCAI 2023 conference?": {"IJCAI2023>>program>>Main Track>>2929>>authors>>authors_1>>Haipeng Chen": 0.07750654220581055, "IJCAI2023>>program>>Main Track>>4090>>authors>>authors_1>>Yifan Wu": 0.07884424924850464, "IJCAI2023>>program>>Main Track>>200>>authors>>authors_4>>Yiran Chen": 0.0801236629486084, "IJCAI2023>>program>>Main Track>>1045>>authors>>authors_3>>Elisheva S. Shamash": 0.08226871490478516, "IJCAI2023>>program>>Main Track>>3832>>authors>>authors_6>>Felix Ulrich-Oltean": 0.08543741703033447, "IJCAI2023>>program>>Main Track>>4376>>authors>>authors_1>>Zhaiming Shen": 0.0857124924659729, "IJCAI2023>>program>>Main Track>>2178>>authors>>authors_3>>Ruben Solozabal Ochoa de Retana": 0.0865250825881958, "IJCAI2023>>program>>Main Track>>1621>>authors>>authors_3>>Jörg Hoffmann": 0.08868032693862915, "IJCAI2023>>program>>Main Track>>1798>>authors>>authors_4>>Chenxi Ma": 0.08894860744476318, "IJCAI2023>>program>>Main Track>>1756>>authors>>authors_4>>Conrad Schecker": 0.08949548006057739, "IJCAI2023>>program>>Main Track>>2038>>authors>>authors_1>>Zhongjing Du": 0.09033751487731934, "IJCAI2023>>program>>Main Track>>863>>authors>>authors_5>>Kai Chen": 0.09071218967437744, "IJCAI2023>>program>>Main Track>>580>>authors>>authors_5>>Gerald Schaefer": 0.09142565727233887, "IJCAI2023>>program>>Main Track>>2276>>authors>>authors_2>>Di Jin": 0.09152686595916748, "IJCAI2023>>program>>Main Track>>1798>>authors>>authors_1>>Xuhao Jiang": 0.09161800146102905, "IJCAI2023>>program>>Main Track>>621>>authors>>authors_1>>Xiang Li": 0.09184759855270386, "IJCAI2023>>program>>Main Track>>1067>>authors>>authors_4>>Jun Zhou": 0.09194928407669067, "IJCAI2023>>program>>Main Track>>2705>>authors>>authors_5>>Dangyang Chen": 0.09212017059326172, "IJCAI2023>>program>>Main Track>>906>>authors>>authors_3>>Sutanay Choudhury": 0.092274010181427, "IJCAI2023>>program>>Main Track>>2144>>authors>>authors_8>>Min-Ling Zhang": 0.092312753200531}, "Who are the authors of the program 2099 in the IJCAI 2023 conference?": {"IJCAI2023>>program>>Main Track>>194>>authors>>authors_1>>Chuanyang Hu": 0.10031276941299438, "IJCAI2023>>program>>Main Track>>1045>>authors>>authors_1>>Takayuki Osogami": 0.10104018449783325, "IJCAI2023>>program>>Main Track>>1813>>authors>>authors_5>>Naijia Wang": 0.10113215446472168, "IJCAI2023>>program>>Main Track>>408>>authors>>authors_3>>Zejian Li": 0.10173046588897705, "IJCAI2023>>program>>Main Track>>200>>authors>>authors_4>>Yiran Chen": 0.10247749090194702, "IJCAI2023>>program>>Main Track>>5281>>authors>>authors_2>>Hao Chen": 0.10274052619934082, "IJCAI2023>>program>>Main Track>>1067>>authors>>authors_4>>Jun Zhou": 0.10286909341812134, "IJCAI2023>>program>>Main Track>>1067>>authors>>authors_2>>Fan Liu": 0.10308325290679932, "IJCAI2023>>program>>Main Track>>2038>>authors>>authors_2>>Xu Jiang": 0.1032450795173645, "IJCAI2023>>program>>Main Track>>2754>>authors>>authors_3>>Xupeng Miao": 0.1035149097442627, "IJCAI2023>>program>>Main Track>>4132>>authors>>authors_4>>Jiancheng Lv": 0.1041364073753357, "IJCAI2023>>program>>Main Track>>4090>>authors>>authors_1>>Yifan Wu": 0.10420536994934082, "IJCAI2023>>program>>Main Track>>2759>>authors>>authors_1>>Alessandro Daniele": 0.10428345203399658, "IJCAI2023>>program>>Main Track>>2929>>authors>>authors_3>>Wei Qiu": 0.10458999872207642, "IJCAI2023>>program>>Main Track>>2929>>authors>>authors_1>>Haipeng Chen": 0.10559862852096558, "IJCAI2023>>program>>Main Track>>3832>>authors>>authors_6>>Felix Ulrich-Oltean": 0.10647231340408325, "IJCAI2023>>program>>Main Track>>5014>>authors>>authors_7>>Yanjie Fu": 0.10651832818984985, "IJCAI2023>>program>>Main Track>>863>>authors>>authors_5>>Kai Chen": 0.10694265365600586, "IJCAI2023>>program>>Main Track>>1099>>authors>>authors_2>>Tian-Jing Zhang": 0.10713982582092285, "IJCAI2023>>program>>Main Track>>345>>authors>>authors_2>>Zhiwei He": 0.10714125633239746}, "What is the major challenge identified in the program 2099 at the IJCAI 2023 conference?": {"IJCAI2023>>program>>Main Track>>200>>authors>>authors_4>>Yiran Chen": 0.11420285701751709, "IJCAI2023>>program>>Competitions and Challenges>>Competitions and Challenges_11>>Description>>Research on 3D mapping plays a vital role in improving our quality of life through numerous smart city applications. However, acquiring 3D data can be expensive, rendering frequent or large-scale data updates impractical. To address this issue, AI technologies are explored to infer 3D information from easily accessible 2D images. Under this backdrop, AI Singapore, in collaboration with Singapore Land Authority, is launching a Visual Localisation Challenge, with an aim to address the problem of “How can AI be used to accurately extract camera pose data from 2D images?”. By leveraging advancements in visual odometry, we hope to push the boundaries of 3D learning and make it more affordable and sustainable for a broader range of real-world applications.": 0.12033110857009888, "IJCAI2023>>program>>Main Track>>5195>>authors>>authors_3>>Thiago D. Simão": 0.12151461839675903, "IJCAI2023>>program>>Competitions and Challenges>>Competitions and Challenges_1>>Timeline of the competition>>Timeline of the competition_1>>April 15th,2023:Call for participation": 0.12244254350662231, "IJCAI2023>>program>>Main Track>>4090>>authors>>authors_1>>Yifan Wu": 0.1232190728187561, "IJCAI2023>>program>>Main Track>>2144>>authors>>authors_8>>Min-Ling Zhang": 0.12368780374526978, "IJCAI2023>>program>>Main Track>>3863>>authors>>authors_7>>Hua Wu": 0.12411749362945557, "IJCAI2023>>program>>Main Track>>2184>>authors>>authors_3>>Kun Wei": 0.12421160936355591, "IJCAI2023>>program>>Main Track>>2929>>authors>>authors_1>>Haipeng Chen": 0.12440317869186401}, "What is the main idea of the solution provided for robust RL in the program 2099 at the IJCAI 2023 conference?": {"IJCAI2023>>program>>Main Track>>2099>>abstract>>Robust reinforcement learning (RL) has been a challenging problem due to the gap between simulation and the real world. Existing efforts typically address the robust RL problem by solving a max-min problem. The main idea is to maximize the cumulative reward under the worst-possible perturbations. However, the worst-case optimization either leads to overly conservative solutions or unstable training process, which further affects the policy robustness and generalization performance. In this paper, we tackle this problem from both formulation definition and algorithm design. First, we formulate the robust RL as a max-expectation optimization problem, where the goal is to find an optimal policy under both the worst cases and the non-worst cases. Then, we propose a novel framework DRRL to solve the max-expectation optimization. Given our definition of the feasible tasks, a task generation and sequencing mechanism is introduced to dynamically output tasks at appropriate difficulty level for the current policy. With these progressive tasks, DRRL realizes dynamic multi-task learning to improve the policy robustness and the training stability. Finally, extensive experiments demonstrate that the proposed method exhibits significant performance on the unmanned CarRacing game and multiple high-dimensional MuJoCo environments.": 0.11249428987503052, "IJCAI2023>>program>>Main Track>>2099>>title>>Robust Reinforcement Learning via Progressive Task Sequence": 0.11665701866149902, "IJCAI2023>>program>>Journal Track>>J5919>>title>>Reinforcement Learning from Optimization Proxy for Ride-Hailing Vehicle Relocation (Extended Abstract)": 0.12588131427764893, "IJCAI2023>>program>>Main Track>>4991>>title>>A Hierarchical Approach to Population Training for Human-AI Collaboration": 0.12749743461608887}, "What are the keywords associated with program 2099 in the IJCAI 2023 conference?": {"IJCAI2023>>program>>Main Track>>2160>>keywords>>keywords_1>>Computer Vision -> CV: 3D computer vision": 0.09056681394577026, "IJCAI2023>>program>>Main Track>>451>>keywords>>keywords_3>>Machine Learning -> ML: Probabilistic machine learning": 0.09207534790039062, "IJCAI2023>>program>>Main Track>>4090>>authors>>authors_1>>Yifan Wu": 0.09385353326797485, "IJCAI2023>>program>>Main Track>>2160>>keywords>>keywords_2>>Computer Vision -> CV: Transfer, low-shot, semi- and un- supervised learning": 0.09451687335968018, "IJCAI2023>>program>>Main Track>>683>>keywords>>keywords_1>>Computer Vision -> CV: Computational photography": 0.0950467586517334, "IJCAI2023>>program>>Main Track>>2099>>keywords>>keywords_2>>Agent-based and Multi-agent Systems -> MAS: Agent theories and models": 0.0950692892074585, "IJCAI2023>>program>>Best Papers from Sister Conferences Track>>SC12>>keywords>>keywords_2>>Sister Conferences Best Papers -> Knowledge Representation and Reasoning": 0.09564143419265747, "IJCAI2023>>program>>Main Track>>827>>keywords>>keywords_1>>Computer Vision -> CV: 3D computer vision": 0.09588682651519775, "IJCAI2023>>program>>Main Track>>4090>>keywords>>keywords_1>>Machine Learning -> ML: Attention models": 0.09646010398864746, "IJCAI2023>>program>>Main Track>>4629>>authors>>authors_3>>Victor Gutierrez-Basulto": 0.09662055969238281, "IJCAI2023>>program>>Main Track>>4516>>keywords>>keywords_2>>Knowledge Representation and Reasoning -> KRR: Preference modelling and preference-based reasoning": 0.09708654880523682, "IJCAI2023>>program>>Main Track>>451>>keywords>>keywords_1>>Computer Vision -> CV: 3D computer vision": 0.09845191240310669, "IJCAI2023>>program>>Main Track>>4783>>keywords>>keywords_2>>Knowledge Representation and Reasoning -> KRR: Preference modelling and preference-based reasoning": 0.09873002767562866, "IJCAI2023>>program>>Main Track>>648>>keywords>>keywords_2>>Computer Vision -> CV: Representation learning": 0.09940510988235474, "IJCAI2023>>program>>Journal Track>>J5943>>keywords>>keywords_7>>Machine Learning -> ML: Symbolic methods": 0.09946668148040771, "IJCAI2023>>program>>Journal Track>>J5935>>keywords>>keywords_4>>Knowledge Representation and Reasoning -> KRR: Semantic Web": 0.09976601600646973}, "What is the title of the paper with the ID 1836 in the main track at IJCAI2023?": {"IJCAI2023>>program>>Main Track>>5195>>authors>>authors_3>>Thiago D. Simão": 0.09318983554840088, "IJCAI2023>>program>>Main Track>>2178>>authors>>authors_3>>Ruben Solozabal Ochoa de Retana": 0.09399336576461792, "IJCAI2023>>program>>Main Track>>3863>>authors>>authors_7>>Hua Wu": 0.09622979164123535, "IJCAI2023>>program>>Main Track>>4206>>authors>>authors_2>>Hendrik Molter": 0.09878391027450562, "IJCAI2023>>program>>Main Track>>4376>>authors>>authors_1>>Zhaiming Shen": 0.09892851114273071, "IJCAI2023>>program>>Main Track>>4276>>authors>>authors_8>>Bettina Könighofer": 0.09918653964996338, "IJCAI2023>>program>>Journal Track>>J5939>>authors>>authors_1>>Elias Schede": 0.0994499921798706, "IJCAI2023>>program>>Main Track>>1493>>authors>>authors_5>>Dong Yan": 0.10018962621688843, "IJCAI2023>>program>>Main Track>>4766>>authors>>authors_4>>Abdoulaye Banire Diallo": 0.1008002758026123, "IJCAI2023>>program>>Main Track>>4732>>authors>>authors_1>>Martin Svatoš": 0.10132473707199097, "IJCAI2023>>program>>Main Track>>1633>>authors>>authors_3>>John Dickerson": 0.10142397880554199, "IJCAI2023>>program>>Main Track>>1493>>authors>>authors_3>>Xinning Zhou": 0.10278105735778809, "IJCAI2023>>program>>Main Track>>3497>>authors>>authors_2>>Jun Yuan": 0.10304945707321167, "IJCAI2023>>program>>Main Track>>5012>>authors>>authors_4>>Yidong Chen": 0.10319924354553223, "IJCAI2023>>program>>Main Track>>863>>authors>>authors_5>>Kai Chen": 0.10322034358978271, "IJCAI2023>>program>>Main Track>>4276>>authors>>authors_4>>Katrine Bjørner": 0.10330080986022949, "IJCAI2023>>program>>Main Track>>4580>>authors>>authors_1>>Carlos Hernández": 0.10334241390228271, "IJCAI2023>>program>>Main Track>>200>>authors>>authors_4>>Yiran Chen": 0.10396760702133179, "IJCAI2023>>program>>Main Track>>648>>authors>>authors_8>>Yue Qi": 0.10449153184890747, "IJCAI2023>>program>>Main Track>>3832>>authors>>authors_6>>Felix Ulrich-Oltean": 0.10463100671768188}, "Who are the authors of the paper with the ID 1836 in the main track at IJCAI2023?": {"IJCAI2023>>program>>Main Track>>3873>>authors>>authors_3>>Michael Wooldridge": 0.09053093194961548, "IJCAI2023>>program>>Main Track>>1856>>authors>>authors_2>>Kate Larson": 0.09483587741851807, "IJCAI2023>>program>>Main Track>>1493>>authors>>authors_6>>Jun Zhu": 0.09494137763977051, "IJCAI2023>>program>>Main Track>>1834>>authors>>authors_3>>Christel Baier": 0.09613639116287231, "IJCAI2023>>program>>Main Track>>1384>>authors>>authors_1>>Xixuan Hao": 0.09630918502807617, "IJCAI2023>>program>>Main Track>>863>>authors>>authors_8>>Jie Chen": 0.09649646282196045, "IJCAI2023>>program>>Main Track>>4276>>authors>>authors_8>>Bettina Könighofer": 0.09746694564819336, "IJCAI2023>>program>>Main Track>>1493>>authors>>authors_5>>Dong Yan": 0.09761291742324829, "IJCAI2023>>program>>Main Track>>2969>>authors>>authors_4>>Devarajan Sridharan": 0.09777528047561646, "IJCAI2023>>program>>Main Track>>5281>>authors>>authors_2>>Hao Chen": 0.09953969717025757, "IJCAI2023>>program>>Main Track>>4969>>authors>>authors_1>>Dabin Zhang": 0.09978467226028442, "IJCAI2023>>program>>Main Track>>863>>authors>>authors_5>>Kai Chen": 0.10002630949020386, "IJCAI2023>>program>>Main Track>>5164>>authors>>authors_4>>Jianyong Wang": 0.10016083717346191, "IJCAI2023>>program>>Main Track>>1424>>authors>>authors_1>>Shijie Luo": 0.10026943683624268, "IJCAI2023>>program>>Journal Track>>J5939>>authors>>authors_1>>Elias Schede": 0.10068219900131226, "IJCAI2023>>program>>Main Track>>1856>>authors>>authors_1>>David Radke": 0.10070395469665527, "IJCAI2023>>program>>Main Track>>2759>>authors>>authors_1>>Alessandro Daniele": 0.10128170251846313, "IJCAI2023>>program>>Main Track>>604>>authors>>authors_8>>Jie Chen": 0.10140663385391235, "IJCAI2023>>program>>Main Track>>1834>>authors>>authors_4>>Clemens Dubslaff": 0.10157150030136108, "IJCAI2023>>program>>Main Track>>2038>>authors>>authors_5>>Xi Wu": 0.10197311639785767}, "What is the abstract of the paper with the ID 1836 in the main track at IJCAI2023?": {"IJCAI2023>>program>>Main Track>>2178>>authors>>authors_3>>Ruben Solozabal Ochoa de Retana": 0.09251135587692261, "IJCAI2023>>program>>Best Papers from Sister Conferences Track>>SC20>>abstract": 0.10103482007980347, "IJCAI2023>>program>>Main Track>>5195>>authors>>authors_3>>Thiago D. Simão": 0.10186421871185303, "IJCAI2023>>program>>Best Papers from Sister Conferences Track>>SC8>>abstract": 0.10253584384918213, "IJCAI2023>>program>>Main Track>>3863>>authors>>authors_7>>Hua Wu": 0.1041291356086731, "IJCAI2023>>program>>Main Track>>194>>authors>>authors_4>>Xuming He": 0.10503578186035156, "IJCAI2023>>program>>Main Track>>774>>authors>>authors_3>>Yabiao Wang": 0.10512453317642212, "IJCAI2023>>program>>Main Track>>4580>>authors>>authors_1>>Carlos Hernández": 0.10517525672912598, "IJCAI2023>>program>>Main Track>>4376>>authors>>authors_1>>Zhaiming Shen": 0.10533154010772705, "IJCAI2023>>program>>Main Track>>648>>authors>>authors_8>>Yue Qi": 0.10558831691741943, "IJCAI2023>>program>>Main Track>>1493>>authors>>authors_5>>Dong Yan": 0.10590577125549316, "IJCAI2023>>program>>Main Track>>4766>>authors>>authors_4>>Abdoulaye Banire Diallo": 0.10700070858001709, "IJCAI2023>>program>>Best Papers from Sister Conferences Track>>SC26>>abstract": 0.10721927881240845, "IJCAI2023>>program>>Main Track>>2230>>authors>>authors_3>>Sheila McIlraith": 0.10754770040512085, "IJCAI2023>>program>>Main Track>>4206>>authors>>authors_2>>Hendrik Molter": 0.10768026113510132, "IJCAI2023>>program>>Main Track>>863>>authors>>authors_5>>Kai Chen": 0.10776185989379883, "IJCAI2023>>program>>Main Track>>2705>>authors>>authors_5>>Dangyang Chen": 0.10797363519668579, "IJCAI2023>>program>>Best Papers from Sister Conferences Track>>SC24>>abstract": 0.10805362462997437, "IJCAI2023>>program>>Main Track>>4276>>authors>>authors_8>>Bettina Könighofer": 0.10807335376739502, "IJCAI2023>>program>>Main Track>>3873>>authors>>authors_3>>Michael Wooldridge": 0.10809522867202759, "IJCAI2023>>program>>Main Track>>1798>>authors>>authors_4>>Chenxi Ma": 0.10846376419067383}, "What are the keywords of the paper with the ID 1836 in the main track at IJCAI2023?": {"IJCAI2023>>program>>Main Track>>1593>>keywords>>keywords_2>>Computer Vision -> CV: Applications": 0.08718770742416382, "IJCAI2023>>program>>Best Papers from Sister Conferences Track>>SC12>>keywords>>keywords_1>>Sister Conferences Best Papers -> Constraint Satisfaction and Optimization": 0.08894026279449463, "IJCAI2023>>program>>Best Papers from Sister Conferences Track>>SC25>>keywords>>keywords_1>>Sister Conferences Best Papers -> Machine Learning": 0.09014588594436646, "IJCAI2023>>program>>Main Track>>2178>>authors>>authors_3>>Ruben Solozabal Ochoa de Retana": 0.09054732322692871, "IJCAI2023>>program>>Main Track>>4766>>keywords>>keywords_3>>Machine Learning -> ML: Unsupervised learning": 0.09142684936523438, "IJCAI2023>>program>>Main Track>>4226>>keywords>>keywords_2>>Computer Vision -> CV: Applications": 0.0914350152015686, "IJCAI2023>>program>>Best Papers from Sister Conferences Track>>SC3>>keywords>>keywords_2>>Sister Conferences Best Papers -> Data Mining": 0.09174811840057373, "IJCAI2023>>program>>Best Papers from Sister Conferences Track>>SC3>>keywords>>keywords_1>>Sister Conferences Best Papers -> AI Ethics, Trust, Fairness": 0.09202402830123901, "IJCAI2023>>program>>Main Track>>3540>>keywords>>keywords_3>>Machine Learning -> ML: Robustness": 0.09212231636047363, "IJCAI2023>>program>>Best Papers from Sister Conferences Track>>SC3>>keywords>>keywords_3>>Sister Conferences Best Papers -> Humans and AI": 0.0946052074432373, "IJCAI2023>>program>>Main Track>>11>>keywords>>keywords_2>>Machine Learning -> ML: Deep reinforcement learning": 0.09471094608306885, "IJCAI2023>>program>>Main Track>>2911>>keywords>>keywords_1>>Natural Language Processing -> NLP: Text classification": 0.09481912851333618, "IJCAI2023>>program>>Main Track>>3378>>keywords>>keywords_1>>Humans and AI -> HAI: Human-AI collaboration": 0.0958738923072815, "IJCAI2023>>program>>Main Track>>3117>>keywords>>keywords_1>>Uncertainty in AI -> UAI: Bayesian networks": 0.09590363502502441, "IJCAI2023>>program>>Main Track>>4516>>keywords>>keywords_2>>Knowledge Representation and Reasoning -> KRR: Preference modelling and preference-based reasoning": 0.09651267528533936, "IJCAI2023>>program>>Main Track>>5195>>authors>>authors_3>>Thiago D. Simão": 0.0966903567314148}, "What is the main topic addressed by the paper with the ID 1836 in the main track at IJCAI2023?": {"IJCAI2023>>program>>Main Track>>3863>>authors>>authors_7>>Hua Wu": 0.09424692392349243, "IJCAI2023>>program>>Main Track>>5195>>authors>>authors_3>>Thiago D. Simão": 0.09717059135437012, "IJCAI2023>>program>>Main Track>>2178>>authors>>authors_3>>Ruben Solozabal Ochoa de Retana": 0.09806221723556519, "IJCAI2023>>program>>Main Track>>4376>>authors>>authors_1>>Zhaiming Shen": 0.10242438316345215, "IJCAI2023>>program>>Main Track>>1621>>authors>>authors_3>>Jörg Hoffmann": 0.10256141424179077, "IJCAI2023>>program>>Main Track>>11>>keywords>>keywords_2>>Machine Learning -> ML: Deep reinforcement learning": 0.10429149866104126, "IJCAI2023>>program>>Main Track>>3832>>authors>>authors_6>>Felix Ulrich-Oltean": 0.10489940643310547, "IJCAI2023>>program>>Main Track>>1633>>authors>>authors_3>>John Dickerson": 0.10589063167572021, "IJCAI2023>>program>>Main Track>>2230>>authors>>authors_3>>Sheila McIlraith": 0.10628420114517212, "IJCAI2023>>program>>Main Track>>435>>authors>>authors_1>>Weiyan Xie": 0.10636204481124878, "IJCAI2023>>program>>Main Track>>2577>>keywords>>keywords_1>>Knowledge Representation and Reasoning -> KRR: Automated reasoning and theorem proving": 0.10637730360031128, "IJCAI2023>>program>>Main Track>>1883>>authors>>authors_3>>Tao Meng": 0.10679316520690918, "IJCAI2023>>program>>Main Track>>863>>authors>>authors_5>>Kai Chen": 0.10711979866027832, "IJCAI2023>>program>>Main Track>>4586>>authors>>authors_5>>Shinnosuke Takamichi": 0.10738629102706909, "IJCAI2023>>program>>Main Track>>4276>>authors>>authors_4>>Katrine Bjørner": 0.10748666524887085, "IJCAI2023>>program>>Main Track>>1798>>authors>>authors_4>>Chenxi Ma": 0.10755282640457153, "IJCAI2023>>program>>Main Track>>2716>>authors>>authors_5>>Siya Qiu": 0.10777777433395386, "IJCAI2023>>program>>Main Track>>4580>>authors>>authors_1>>Carlos Hernández": 0.1078193187713623, "IJCAI2023>>program>>Main Track>>200>>authors>>authors_4>>Yiran Chen": 0.10812747478485107}, "What is the title of the paper presented in the Special Track on AI for Good?": {"IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5784>>authors>>authors_2>>Philippe Vismara": 0.0742572546005249, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5683>>authors>>authors_1>>Dixin Luo": 0.08710086345672607, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5683>>authors>>authors_2>>Haoran Cheng": 0.0900353193283081, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5784>>authors>>authors_4>>Stéphane de Tourdonnet": 0.09018939733505249, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5757>>authors>>authors_1>>Sujan Dutta": 0.09050828218460083, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5431>>keywords>>keywords_2>>AI for Good -> Multidisciplinary Topics and Applications": 0.09074842929840088, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5757>>authors>>authors_2>>Parth Srivastava": 0.09137272834777832, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5458>>authors>>authors_2>>Fan Li": 0.09201109409332275, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5795>>authors>>authors_2>>Yunhe Feng": 0.0933271050453186, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5787>>authors>>authors_1>>Zhaonian Zhang": 0.09588658809661865, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5808>>authors>>authors_4>>Khalid Elgazzar": 0.09606701135635376, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5814>>authors>>authors_7>>Sandhya Ramalingam": 0.09620684385299683, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5800>>keywords>>keywords_3>>AI for Good -> Search": 0.09627914428710938, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5828>>title>>Limited Resource Allocation in a Non-Markovian World: The Case of Maternal and Child Healthcare": 0.09717881679534912, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5761>>authors>>authors_4>>Jennifer H. Fair": 0.09741675853729248}, "Who are the authors of the paper titled 'GreenFlow: A Computation Allocation Framework for Building Environmentally Sound Recommendation System'?": {"IJCAI2023>>program>>Main Track>>536>>authors>>authors_1>>Rao Fu": 0.16878187656402588, "IJCAI2023>>program>>Main Track>>490>>authors>>authors_2>>Buzhen Huang": 0.17359411716461182, "IJCAI2023>>program>>Journal Track>>J5924>>authors>>authors_6>>Daniele Magazzeni": 0.1760624647140503, "IJCAI2023>>program>>Survey Track>>SV5593>>authors>>authors_8>>Qing Li": 0.17622369527816772, "IJCAI2023>>program>>Demonstrations Track>>DM5705>>authors>>authors_5>>Kavitha Srinivas": 0.17654407024383545, "IJCAI2023>>program>>Survey Track>>SV5593>>authors>>authors_1>>Chengyi Liu": 0.17671877145767212, "IJCAI2023>>program>>Main Track>>3526>>authors>>authors_1>>Xiaolin Zheng": 0.17683899402618408, "IJCAI2023>>program>>Main Track>>4454>>authors>>authors_4>>Kurt Mehlhorn": 0.1771039366722107, "IJCAI2023>>program>>Main Track>>162>>authors>>authors_2>>Jianrong Zhang": 0.17744463682174683, "IJCAI2023>>program>>Main Track>>1626>>authors>>authors_1>>Peizheng Li": 0.17756712436676025, "IJCAI2023>>program>>Main Track>>3073>>authors>>authors_3>>Eklavya Sharma": 0.1776430606842041, "IJCAI2023>>program>>Main Track>>5176>>authors>>authors_3>>Yasheng Wang": 0.17795252799987793, "IJCAI2023>>program>>Main Track>>536>>authors>>authors_3>>Qian Li": 0.1780187487602234, "IJCAI2023>>program>>Main Track>>3873>>authors>>authors_3>>Michael Wooldridge": 0.17839008569717407, "IJCAI2023>>program>>Main Track>>1476>>authors>>authors_1>>Ren-Jian Wang": 0.17863410711288452, "IJCAI2023>>program>>Main Track>>1796>>authors>>authors_1>>Yalin Yu": 0.17875051498413086, "IJCAI2023>>program>>Main Track>>398>>authors>>authors_3>>Zhongang Qi": 0.17878609895706177, "IJCAI2023>>program>>Main Track>>1384>>authors>>authors_1>>Xixuan Hao": 0.17879533767700195, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5850>>authors>>authors_5>>Richa Singh": 0.1788574457168579}, "What is the main proposal in the abstract of the paper titled 'GreenFlow: A Computation Allocation Framework for Building Environmentally Sound Recommendation System'?": {"IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5800>>title>>GreenFlow: A Computation Allocation Framework for Building Environmentally Sound Recommendation System": 0.07205086946487427, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5800>>abstract>>Given the enormous number of users and items, industrial cascade recommendation systems (RS) are continuously expanded in size and complexity to deliver relevant items, such as news, services, and commodities, to the appropriate users. In a real-world scenario with hundreds of thousands requests per second, significant computation is required to infer personalized results for each request, resulting in a massive energy consumption and carbon emission that raises concern. \n\nThis paper proposes GreenFlow, a practical computation allocation framework for RS, that considers both accuracy and carbon emission during inference. For each stage (e.g., recall, pre-ranking, ranking, etc.) of a cascade RS, when a user triggers a request, we define two actions that determine the computation: (1) the trained instances of models with different computational complexity; and (2) the number of items to be inferred in the stage. We refer to the combinations of actions in all stages as action chains. A reward score is estimated for each action chain, followed by dynamic primal-dual optimization considering both the reward and computation budget. Extensive experiments verify the effectiveness of the framework, reducing computation consumption by 41% in an industrial mobile application while maintaining commercial revenue. Moreover, the proposed framework saves approximately 5000kWh of electricity and reduces 3 tons of carbon emissions per day.": 0.10918796062469482}, "What are the general areas of interest or keywords concerning the paper titled 'GreenFlow: A Computation Allocation Framework for Building Environmentally Sound Recommendation System'?": {"IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5800>>title>>GreenFlow: A Computation Allocation Framework for Building Environmentally Sound Recommendation System": 0.08846378326416016, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5800>>abstract>>Given the enormous number of users and items, industrial cascade recommendation systems (RS) are continuously expanded in size and complexity to deliver relevant items, such as news, services, and commodities, to the appropriate users. In a real-world scenario with hundreds of thousands requests per second, significant computation is required to infer personalized results for each request, resulting in a massive energy consumption and carbon emission that raises concern. \n\nThis paper proposes GreenFlow, a practical computation allocation framework for RS, that considers both accuracy and carbon emission during inference. For each stage (e.g., recall, pre-ranking, ranking, etc.) of a cascade RS, when a user triggers a request, we define two actions that determine the computation: (1) the trained instances of models with different computational complexity; and (2) the number of items to be inferred in the stage. We refer to the combinations of actions in all stages as action chains. A reward score is estimated for each action chain, followed by dynamic primal-dual optimization considering both the reward and computation budget. Extensive experiments verify the effectiveness of the framework, reducing computation consumption by 41% in an industrial mobile application while maintaining commercial revenue. Moreover, the proposed framework saves approximately 5000kWh of electricity and reduces 3 tons of carbon emissions per day.": 0.11670058965682983, "IJCAI2023>>program>>Demonstrations Track>>DM5740>>keywords>>keywords_2>>Multidisciplinary Topics and Applications -> MDA: Energy, environment and sustainability": 0.15744483470916748, "IJCAI2023>>program>>Main Track>>4665>>keywords>>keywords_2>>AI Ethics, Trust, Fairness -> ETF: Explainability and interpretability": 0.16291487216949463, "IJCAI2023>>calls>>Journal Track>>Accepted Papers List>>J5950>>keywords>>keywords_1>>Planning": 0.1641252040863037, "IJCAI2023>>program>>Main Track>>4004>>keywords>>keywords_2>>Game Theory and Economic Paradigms -> GTEP: Computational social choice": 0.16883569955825806, "IJCAI2023>>program>>Main Track>>4071>>title>>Generalization through Diversity: Improving Unsupervised Environment Design": 0.1701807975769043}, "What are the environmental benefits of the proposed GreenFlow framework as per the abstract?": {"IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5800>>title>>GreenFlow: A Computation Allocation Framework for Building Environmentally Sound Recommendation System": 0.12952423095703125, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5800>>abstract>>Given the enormous number of users and items, industrial cascade recommendation systems (RS) are continuously expanded in size and complexity to deliver relevant items, such as news, services, and commodities, to the appropriate users. In a real-world scenario with hundreds of thousands requests per second, significant computation is required to infer personalized results for each request, resulting in a massive energy consumption and carbon emission that raises concern. \n\nThis paper proposes GreenFlow, a practical computation allocation framework for RS, that considers both accuracy and carbon emission during inference. For each stage (e.g., recall, pre-ranking, ranking, etc.) of a cascade RS, when a user triggers a request, we define two actions that determine the computation: (1) the trained instances of models with different computational complexity; and (2) the number of items to be inferred in the stage. We refer to the combinations of actions in all stages as action chains. A reward score is estimated for each action chain, followed by dynamic primal-dual optimization considering both the reward and computation budget. Extensive experiments verify the effectiveness of the framework, reducing computation consumption by 41% in an industrial mobile application while maintaining commercial revenue. Moreover, the proposed framework saves approximately 5000kWh of electricity and reduces 3 tons of carbon emissions per day.": 0.1518574357032776}, "What is the title of the demonstration track DM5722?": {"IJCAI2023>>program>>Demonstrations Track>>DM5722>>authors>>authors_5>>Raviraj Joshi": 0.14798355102539062, "IJCAI2023>>program>>Demonstrations Track>>DM5722>>authors>>authors_1>>Vidula Magdum": 0.15302616357803345, "IJCAI2023>>program>>Demonstrations Track>>DM5696>>authors>>authors_4>>Thomas McCluskey": 0.15440881252288818, "IJCAI2023>>program>>Demonstrations Track>>DM5696>>authors>>authors_6>>Mauro Vallati": 0.15441149473190308, "IJCAI2023>>program>>Demonstrations Track>>DM5722>>authors>>authors_2>>Omkar Dhekane": 0.1594611406326294, "IJCAI2023>>program>>Demonstrations Track>>DM5686>>title>>VideoMaster: A Multimodal Micro Game Video Recreator": 0.16053247451782227, "IJCAI2023>>program>>Demonstrations Track>>DM5728>>authors>>authors_2>>Elham Khabiri": 0.16090744733810425, "IJCAI2023>>program>>Demonstrations Track>>DM5691>>authors>>authors_1>>Baihan Lin": 0.16129052639007568, "IJCAI2023>>program>>Demonstrations Track>>DM5686>>authors>>authors_2>>Xiao Chen": 0.16134703159332275, "IJCAI2023>>program>>Demonstrations Track>>DM5712>>authors>>authors_2>>Jinchao Zhang": 0.16199582815170288, "IJCAI2023>>program>>Demonstrations Track>>DM5712>>authors>>authors_5>>Jie Zhou": 0.16205936670303345, "IJCAI2023>>program>>Demonstrations Track>>DM5691>>authors>>authors_2>>Guillermo Cecchi": 0.16259253025054932, "IJCAI2023>>program>>Demonstrations Track>>DM5731>>authors>>authors_2>>Shreya Ghosh": 0.1626143455505371, "IJCAI2023>>program>>Demonstrations Track>>DM5696>>authors>>authors_1>>Saumya Bhatnagar": 0.1634199023246765, "IJCAI2023>>program>>Demonstrations Track>>DM5718>>title>>Modeling the Impact of Policy Interventions for Sustainable Development": 0.1635074019432068, "IJCAI2023>>program>>Demonstrations Track>>DM5729>>keywords>>keywords_4>>Planning and Scheduling -> PS: Markov decisions processes": 0.1641451120376587, "IJCAI2023>>program>>Demonstrations Track>>DM5732>>keywords>>keywords_2>>Multidisciplinary Topics and Applications -> MDA: Other": 0.16491740942001343}, "Who are the authors of the demonstration 'mahaNLP: A Marathi Natural Language Processing Library'?": {"IJCAI2023>>program>>Demonstrations Track>>DM5722>>title>>mahaNLP: A Marathi Natural Language Processing Library": 0.0912466049194336, "IJCAI2023>>program>>Demonstrations Track>>DM5722>>abstract>>We present mahaNLP, an open-source natural language processing (NLP) library specifically built for the Marathi language. It aims to enhance the support for the low-resource Indian language Marathi in the field of NLP. It is an easy-to-use, extensible and modular toolkit for Marathi text analysis built on state-of-the-art transformer models. In comparison to other existing Indic NLP libraries that support basic Marathi processing, this toolkit houses an extensive set of NLP tasks ranging from basic preprocessing tasks to advanced NLP tasks. Additionally, it provides functionality to load datasets for supervised tasks like Marathi sentiment analysis, NER, and Hate speech detection as data frames. This paper focuses on the overview of the mahaNLP framework, its features, and its usage. This work is a part of the L3Cube MahaNLP initiative, more information about it can be found at https://github.com/l3cube-pune/MarathiNLP and the demonstration video and file of mahaNLP are available at https://youtu.be/KxExcwCrTO0 and https://cutt.ly/f1FYQak respectively.": 0.09989970922470093, "IJCAI2023>>program>>Demonstrations Track>>DM5731>>authors>>authors_4>>Prasenjit Mitra": 0.15161657333374023, "IJCAI2023>>program>>Demonstrations Track>>DM5728>>authors>>authors_1>>Anuradha Bhamidipaty": 0.15447944402694702, "IJCAI2023>>program>>Demonstrations Track>>DM5718>>authors>>authors_2>>Arpitha Malavalli": 0.15673357248306274, "IJCAI2023>>program>>Demonstrations Track>>DM5728>>authors>>authors_2>>Elham Khabiri": 0.16374832391738892, "IJCAI2023>>program>>Demonstrations Track>>DM5731>>authors>>authors_2>>Shreya Ghosh": 0.16603469848632812, "IJCAI2023>>program>>Demonstrations Track>>DM5742>>authors>>authors_3>>Biplav Srivastava": 0.16640079021453857, "IJCAI2023>>program>>Demonstrations Track>>DM5742>>authors>>authors_1>>Vishal Pallagani": 0.1665440797805786}, "What is the goal of the mahaNLP library?": {"IJCAI2023>>program>>Demonstrations Track>>DM5722>>title>>mahaNLP: A Marathi Natural Language Processing Library": 0.08878469467163086, "IJCAI2023>>program>>Demonstrations Track>>DM5722>>abstract>>We present mahaNLP, an open-source natural language processing (NLP) library specifically built for the Marathi language. It aims to enhance the support for the low-resource Indian language Marathi in the field of NLP. It is an easy-to-use, extensible and modular toolkit for Marathi text analysis built on state-of-the-art transformer models. In comparison to other existing Indic NLP libraries that support basic Marathi processing, this toolkit houses an extensive set of NLP tasks ranging from basic preprocessing tasks to advanced NLP tasks. Additionally, it provides functionality to load datasets for supervised tasks like Marathi sentiment analysis, NER, and Hate speech detection as data frames. This paper focuses on the overview of the mahaNLP framework, its features, and its usage. This work is a part of the L3Cube MahaNLP initiative, more information about it can be found at https://github.com/l3cube-pune/MarathiNLP and the demonstration video and file of mahaNLP are available at https://youtu.be/KxExcwCrTO0 and https://cutt.ly/f1FYQak respectively.": 0.10284268856048584, "IJCAI2023>>program>>Main Track>>2816>>keywords>>keywords_1>>Natural Language Processing -> NLP: Language models": 0.17598587274551392, "IJCAI2023>>program>>Demonstrations Track>>DM5722>>keywords>>keywords_3>>Natural Language Processing -> NLP: Language models": 0.18054640293121338, "IJCAI2023>>program>>Demonstrations Track>>DM5722>>keywords>>keywords_5>>Natural Language Processing -> NLP: Named entities": 0.18192040920257568}, "What are the features and functionality of the mahaNLP library?": {"IJCAI2023>>program>>Demonstrations Track>>DM5722>>title>>mahaNLP: A Marathi Natural Language Processing Library": 0.08504509925842285, "IJCAI2023>>program>>Demonstrations Track>>DM5722>>abstract>>We present mahaNLP, an open-source natural language processing (NLP) library specifically built for the Marathi language. It aims to enhance the support for the low-resource Indian language Marathi in the field of NLP. It is an easy-to-use, extensible and modular toolkit for Marathi text analysis built on state-of-the-art transformer models. In comparison to other existing Indic NLP libraries that support basic Marathi processing, this toolkit houses an extensive set of NLP tasks ranging from basic preprocessing tasks to advanced NLP tasks. Additionally, it provides functionality to load datasets for supervised tasks like Marathi sentiment analysis, NER, and Hate speech detection as data frames. This paper focuses on the overview of the mahaNLP framework, its features, and its usage. This work is a part of the L3Cube MahaNLP initiative, more information about it can be found at https://github.com/l3cube-pune/MarathiNLP and the demonstration video and file of mahaNLP are available at https://youtu.be/KxExcwCrTO0 and https://cutt.ly/f1FYQak respectively.": 0.10582941770553589, "IJCAI2023>>program>>Main Track>>2816>>keywords>>keywords_1>>Natural Language Processing -> NLP: Language models": 0.17226648330688477, "IJCAI2023>>program>>Demonstrations Track>>DM5722>>keywords>>keywords_5>>Natural Language Processing -> NLP: Named entities": 0.17363935708999634, "IJCAI2023>>program>>Demonstrations Track>>DM5722>>keywords>>keywords_3>>Natural Language Processing -> NLP: Language models": 0.17707979679107666, "IJCAI2023>>program>>Demonstrations Track>>DM5686>>keywords>>keywords_9>>Natural Language Processing -> NLP: Speech": 0.18229061365127563, "IJCAI2023>>program>>Demonstrations Track>>DM5691>>keywords>>keywords_8>>Natural Language Processing -> NLP: Applications": 0.1834545135498047, "IJCAI2023>>program>>Demonstrations Track>>DM5686>>keywords>>keywords_7>>Natural Language Processing -> NLP: Applications": 0.18540364503860474}, "What are the keywords associated with the DM5722 demonstration track?": {"IJCAI2023>>program>>Demonstrations Track>>DM5728>>keywords>>keywords_3>>Natural Language Processing -> NLP: Applications": 0.11521899700164795, "IJCAI2023>>program>>Demonstrations Track>>DM5732>>keywords>>keywords_2>>Multidisciplinary Topics and Applications -> MDA: Other": 0.12021255493164062, "IJCAI2023>>program>>Demonstrations Track>>DM5703>>keywords>>keywords_4>>Machine Learning -> ML: Evaluation": 0.12635689973831177, "IJCAI2023>>program>>Demonstrations Track>>DM5729>>keywords>>keywords_3>>Multidisciplinary Topics and Applications -> MDA: Databases": 0.12801826000213623, "IJCAI2023>>program>>Demonstrations Track>>DM5728>>keywords>>keywords_2>>Data Mining -> DM: Knowledge graphs and knowledge base completion": 0.1287899613380432, "IJCAI2023>>program>>Demonstrations Track>>DM5740>>keywords>>keywords_3>>Machine Learning -> ML: Autoencoders": 0.12925422191619873, "IJCAI2023>>program>>Demonstrations Track>>DM5728>>keywords>>keywords_1>>Data Mining -> DM: Mining heterogenous data": 0.12975645065307617, "IJCAI2023>>program>>Demonstrations Track>>DM5719>>keywords>>keywords_4>>Machine Learning -> ML: Feature extraction, selection and dimensionality reduction": 0.1324756145477295, "IJCAI2023>>program>>Demonstrations Track>>DM5719>>keywords>>keywords_1>>Multidisciplinary Topics and Applications -> MDA: Energy, environment and sustainability": 0.13282018899917603, "IJCAI2023>>program>>Demonstrations Track>>DM5681>>keywords>>keywords_1>>Computer Vision -> CV: Applications": 0.13360202312469482, "IJCAI2023>>program>>Demonstrations Track>>DM5728>>keywords>>keywords_4>>Knowledge Representation and Reasoning -> KRR: Applications": 0.13365864753723145, "IJCAI2023>>program>>Demonstrations Track>>DM5686>>keywords>>keywords_4>>Computer Vision -> CV: Video analysis and understanding": 0.13524264097213745, "IJCAI2023>>program>>Demonstrations Track>>DM5719>>keywords>>keywords_2>>Multidisciplinary Topics and Applications -> MDA: Life sciences": 0.1368657350540161, "IJCAI2023>>program>>Demonstrations Track>>DM5729>>keywords>>keywords_4>>Planning and Scheduling -> PS: Markov decisions processes": 0.13790053129196167}, "What is the title of the paper with the ID 4313 in the Main Track program?": {"IJCAI2023>>program>>Main Track>>1493>>authors>>authors_5>>Dong Yan": 0.11764639616012573, "IJCAI2023>>program>>Main Track>>4276>>authors>>authors_4>>Katrine Bjørner": 0.11870396137237549, "IJCAI2023>>program>>Main Track>>4276>>authors>>authors_8>>Bettina Könighofer": 0.11900418996810913, "IJCAI2023>>program>>Main Track>>4504>>authors>>authors_2>>Tobias Geibinger": 0.12225544452667236, "IJCAI2023>>program>>Main Track>>3934>>authors>>authors_4>>Luc De Raedt": 0.12480229139328003, "IJCAI2023>>program>>Main Track>>5012>>authors>>authors_4>>Yidong Chen": 0.1251395344734192, "IJCAI2023>>program>>Main Track>>3002>>authors>>authors_1>>Francesco Belardinelli": 0.12654930353164673, "IJCAI2023>>program>>Main Track>>3510>>authors>>authors_1>>Yongjuan Che": 0.12810224294662476, "IJCAI2023>>program>>Main Track>>3497>>authors>>authors_2>>Jun Yuan": 0.12862712144851685, "IJCAI2023>>program>>Main Track>>4438>>authors>>authors_4>>Keerthiram Murugesan": 0.1286787986755371, "IJCAI2023>>program>>Main Track>>2471>>authors>>authors_3>>Vladislav Ryzhikov": 0.12926650047302246, "IJCAI2023>>program>>Main Track>>4438>>authors>>authors_3>>Ronny Luss": 0.12962812185287476, "IJCAI2023>>program>>Main Track>>3138>>authors>>authors_4>>Niyati Chhaya": 0.12980598211288452, "IJCAI2023>>program>>Main Track>>4672>>authors>>authors_2>>Vittorio Bilò": 0.1304648518562317}, "Who is the author of the paper 'A Rule-Based Modal View of Causal Reasoning'?": {"IJCAI2023>>program>>Main Track>>1834>>authors>>authors_3>>Christel Baier": 0.18003809452056885, "IJCAI2023>>program>>Main Track>>1856>>authors>>authors_2>>Kate Larson": 0.1803249716758728, "IJCAI2023>>program>>Main Track>>1856>>authors>>authors_1>>David Radke": 0.18190985918045044, "IJCAI2023>>program>>Main Track>>1379>>authors>>authors_1>>James Kotary": 0.18258225917816162, "IJCAI2023>>program>>Main Track>>536>>authors>>authors_1>>Rao Fu": 0.18371927738189697, "IJCAI2023>>program>>Main Track>>1032>>authors>>authors_2>>Marie-Francine Moens": 0.18419378995895386, "IJCAI2023>>program>>Main Track>>729>>authors>>authors_1>>Luca Marzari": 0.18424636125564575, "IJCAI2023>>program>>Main Track>>1834>>authors>>authors_2>>Simon Jantsch": 0.1843428611755371, "IJCAI2023>>program>>Main Track>>398>>authors>>authors_3>>Zhongang Qi": 0.18474483489990234, "IJCAI2023>>program>>Main Track>>1793>>authors>>authors_1>>Siddhartha Banerjee": 0.18562328815460205, "IJCAI2023>>program>>Main Track>>536>>authors>>authors_5>>Pierre Alliez": 0.18598151206970215, "IJCAI2023>>program>>Main Track>>1621>>authors>>authors_1>>Maria Christakis": 0.18821024894714355, "IJCAI2023>>program>>Main Track>>1856>>authors>>authors_3>>Tim Brecht": 0.18827790021896362, "IJCAI2023>>program>>Journal Track>>J5924>>authors>>authors_6>>Daniele Magazzeni": 0.18868768215179443, "IJCAI2023>>program>>Journal Track>>J5758>>authors>>authors_2>>Matthew E. Taylor": 0.18882709741592407, "IJCAI2023>>program>>Main Track>>1384>>authors>>authors_1>>Xixuan Hao": 0.18893951177597046, "IJCAI2023>>program>>Main Track>>1920>>authors>>authors_1>>Cristian-Paul Bara": 0.1895633339881897, "IJCAI2023>>calls>>Journal Track>>Accepted Papers List>>J5950>>authors>>authors_5>>Subbarao Kambhampati": 0.18957513570785522, "IJCAI2023>>program>>Main Track>>3873>>authors>>authors_3>>Michael Wooldridge": 0.18972718715667725, "IJCAI2023>>program>>Main Track>>1593>>authors>>authors_2>>Tao Dai": 0.18988662958145142}, "What is the abstract of the paper with the ID 4313 in the Main Track program?": {"IJCAI2023>>program>>Main Track>>4504>>authors>>authors_2>>Tobias Geibinger": 0.11251604557037354, "IJCAI2023>>program>>Main Track>>1493>>authors>>authors_5>>Dong Yan": 0.11476224660873413, "IJCAI2023>>program>>Main Track>>3934>>authors>>authors_4>>Luc De Raedt": 0.1164979338645935, "IJCAI2023>>program>>Main Track>>3002>>authors>>authors_1>>Francesco Belardinelli": 0.11786776781082153, "IJCAI2023>>program>>Main Track>>4276>>authors>>authors_8>>Bettina Könighofer": 0.1180298924446106, "IJCAI2023>>program>>Main Track>>4853>>abstract>>We give polynomial time algorithms for escaping from high-dimensional saddle points under a moderate number of constraints. Given gradient access to a smooth function $f \\colon \\mathbb R^d \\to \\mathbb R$ we show that (noisy) gradient descent methods can escape from saddle points under a logarithmic number of inequality constraints. This constitutes progress (without reliance on NP-oracles or altering the definitions to only account for certain constraints) on the main open question of the breakthrough work of [Ge et al.`15] who showed an analogous result for unconstrained and equality-constrained problems. Our results hold for both regular and stochastic gradient descent.": 0.11981165409088135, "IJCAI2023>>program>>Main Track>>5012>>authors>>authors_4>>Yidong Chen": 0.12018406391143799, "IJCAI2023>>program>>Main Track>>3171>>abstract>>The recent popularity of Wordle has revived interest in guessing games. We develop a general method for finding optimal strategies for guessing games while avoiding an exhaustive search. Our main contribution are several theorems that build towards a general theory to prove optimality of a strategy for a guessing game. This work is developed to apply to any guessing game, but we use Wordle as an example to present concrete results.": 0.12029105424880981, "IJCAI2023>>program>>Main Track>>4438>>authors>>authors_4>>Keerthiram Murugesan": 0.12056022882461548, "IJCAI2023>>program>>Main Track>>4276>>authors>>authors_4>>Katrine Bjørner": 0.120583176612854, "IJCAI2023>>program>>Main Track>>4172>>title>>An Experimental Comparison of Multiwinner Voting Rules on Approval Elections": 0.12120509147644043, "IJCAI2023>>program>>Main Track>>3510>>authors>>authors_1>>Yongjuan Che": 0.12154513597488403}, "What are the keywords associated with the paper 'A Rule-Based Modal View of Causal Reasoning'?": {"IJCAI2023>>program>>Main Track>>4313>>abstract>>We present a novel rule-based semantics for causal reasoning as well as a number of modal languages interpreted over it. They enable us to represent some fundamental concepts in the theory of causality including causal necessity and possibility, interventionist conditionals and Lewisian conditionals. We provide complexity results for the satisfiability checking and model checking problem for these modal languages. Moreover, we study the relationship between our rule-based semantics and the structural equation modeling (SEM) approach to causal reasoning, as well as between our rule-based semantics for causal conditionals and the standard semantics for belief base change.": 0.0822516679763794, "IJCAI2023>>program>>Main Track>>4313>>title>>A Rule-Based Modal View of Causal Reasoning": 0.12626796960830688, "IJCAI2023>>program>>Main Track>>2836>>keywords>>keywords_2>>Knowledge Representation and Reasoning -> KRR: Causality": 0.13098883628845215, "IJCAI2023>>program>>Doctoral Consortium Track>>DC5895>>keywords>>keywords_1>>Knowledge Representation and Reasoning -> KRR: Causality": 0.13806301355361938, "IJCAI2023>>program>>Best Papers from Sister Conferences Track>>SC10>>title>>MV-Datalog+/-: Effective Rule-based Reasoning with Uncertain Observations": 0.14761078357696533, "IJCAI2023>>program>>Journal Track>>J5949>>keywords>>keywords_1>>Knowledge Representation and Reasoning -> KRR: Non-monotonic reasoning": 0.151472270488739, "IJCAI2023>>program>>Journal Track>>J5949>>keywords>>keywords_3>>Knowledge Representation and Reasoning -> KRR: Semantic Web": 0.15189242362976074, "IJCAI2023>>program>>Doctoral Consortium Track>>DC5906>>keywords>>keywords_3>>Knowledge Representation and Reasoning -> KRR: Causality": 0.151966392993927, "IJCAI2023>>program>>Main Track>>3153>>keywords>>keywords_1>>Knowledge Representation and Reasoning -> KRR: Causality": 0.15218430757522583, "IJCAI2023>>program>>Main Track>>2836>>title>>The Hardness of Reasoning about Probabilities and Causality": 0.1538935899734497, "IJCAI2023>>program>>Main Track>>1875>>keywords>>keywords_2>>Knowledge Representation and Reasoning -> KRR: Case-based reasoning": 0.15401369333267212, "IJCAI2023>>program>>Journal Track>>J5922>>keywords>>keywords_1>>Knowledge Representation and Reasoning -> KRR: Learning and reasoning": 0.15467798709869385}, "In which area does the paper 'A Rule-Based Modal View of Causal Reasoning' fall under in terms of Knowledge Representation and Reasoning?": {"IJCAI2023>>program>>Main Track>>4313>>abstract>>We present a novel rule-based semantics for causal reasoning as well as a number of modal languages interpreted over it. They enable us to represent some fundamental concepts in the theory of causality including causal necessity and possibility, interventionist conditionals and Lewisian conditionals. We provide complexity results for the satisfiability checking and model checking problem for these modal languages. Moreover, we study the relationship between our rule-based semantics and the structural equation modeling (SEM) approach to causal reasoning, as well as between our rule-based semantics for causal conditionals and the standard semantics for belief base change.": 0.09263354539871216, "IJCAI2023>>program>>Main Track>>4313>>title>>A Rule-Based Modal View of Causal Reasoning": 0.13560634851455688, "IJCAI2023>>program>>Main Track>>5014>>keywords>>keywords_2>>Knowledge Representation and Reasoning -> KRR: Learning and reasoning": 0.14197349548339844, "IJCAI2023>>program>>Best Papers from Sister Conferences Track>>SC10>>title>>MV-Datalog+/-: Effective Rule-based Reasoning with Uncertain Observations": 0.14307963848114014, "IJCAI2023>>program>>Main Track>>2836>>keywords>>keywords_2>>Knowledge Representation and Reasoning -> KRR: Causality": 0.14337772130966187, "IJCAI2023>>program>>Main Track>>4826>>abstract>>The ease and speed of spreading misinformation and propaganda on the Web motivate the need to develop trustworthy technology for detecting fallacies in natural language arguments. However, state-of-the-art language modeling methods exhibit a lack of robustness on tasks like logical fallacy classification that require complex reasoning. In this paper, we propose a Case-Based Reasoning method that classifies new cases of logical fallacy by language-modeling-driven retrieval and adaptation of historical cases. We design four complementary strategies to enrich input representation for our model, based on external information about goals, explanations, counterarguments, and argument structure. Our experiments in in-domain and out-of-domain settings indicate that Case-Based Reasoning improves the accuracy and generalizability of language models. Our ablation studies suggest that representations of similar cases have a strong impact on the model performance, that models perform well with fewer retrieved cases, and that the size of the case database has a negligible effect on the performance. Finally, we dive deeper into the relationship between the properties of the retrieved cases and the model performance.": 0.14355289936065674}, "What is the title of the presentation under the Main Track program number 298?": {"IJCAI2023>>program>>Main Track>>1588>>authors>>authors_4>>Li Guo": 0.11603450775146484, "IJCAI2023>>program>>Main Track>>228>>authors>>authors_1>>Md Asifur Rahman": 0.12124985456466675, "IJCAI2023>>program>>Main Track>>398>>authors>>authors_1>>Xuewei Li": 0.1245240569114685, "IJCAI2023>>program>>Main Track>>3002>>authors>>authors_1>>Francesco Belardinelli": 0.12774628400802612, "IJCAI2023>>program>>Main Track>>4504>>authors>>authors_2>>Tobias Geibinger": 0.12786823511123657, "IJCAI2023>>program>>Main Track>>1798>>authors>>authors_1>>Xuhao Jiang": 0.12892544269561768, "IJCAI2023>>program>>Main Track>>3138>>authors>>authors_1>>Kezhen Chen": 0.12911373376846313, "IJCAI2023>>program>>Main Track>>3934>>authors>>authors_4>>Luc De Raedt": 0.130901038646698, "IJCAI2023>>program>>Main Track>>2929>>authors>>authors_1>>Haipeng Chen": 0.13103044033050537, "IJCAI2023>>program>>Main Track>>234>>keywords>>keywords_3>>Planning and Scheduling -> PS: Applications": 0.1315498948097229, "IJCAI2023>>program>>Main Track>>1045>>authors>>authors_3>>Elisheva S. Shamash": 0.13175207376480103, "IJCAI2023>>program>>Main Track>>1588>>authors>>authors_5>>Xueyang Fu": 0.13372719287872314, "IJCAI2023>>program>>Main Track>>3482>>authors>>authors_5>>Truyen Tran": 0.1338123083114624, "IJCAI2023>>program>>Main Track>>2512>>authors>>authors_4>>Kuldeep S. Meel": 0.13414430618286133, "IJCAI2023>>program>>Main Track>>4438>>authors>>authors_1>>Debarun Bhattacharjya": 0.13487845659255981, "IJCAI2023>>program>>Main Track>>2296>>authors>>authors_2>>Zening Zeng": 0.13547247648239136, "IJCAI2023>>program>>Main Track>>3002>>authors>>authors_4>>Vadim Malvone": 0.13571733236312866, "IJCAI2023>>program>>Main Track>>2471>>authors>>authors_3>>Vladislav Ryzhikov": 0.1359095573425293, "IJCAI2023>>program>>Main Track>>892>>authors>>authors_1>>Zesen Cheng": 0.13601291179656982}, "Who are the authors of the presentation under the Main Track program number 298?": {"IJCAI2023>>program>>Main Track>>1588>>authors>>authors_4>>Li Guo": 0.11603260040283203, "IJCAI2023>>program>>Main Track>>4969>>authors>>authors_1>>Dabin Zhang": 0.11721360683441162, "IJCAI2023>>program>>Main Track>>228>>authors>>authors_1>>Md Asifur Rahman": 0.11734950542449951, "IJCAI2023>>program>>Main Track>>1856>>authors>>authors_2>>Kate Larson": 0.11917203664779663, "IJCAI2023>>program>>Main Track>>3002>>authors>>authors_4>>Vadim Malvone": 0.11923950910568237, "IJCAI2023>>program>>Main Track>>2512>>authors>>authors_4>>Kuldeep S. Meel": 0.11995190382003784, "IJCAI2023>>program>>Main Track>>3934>>authors>>authors_4>>Luc De Raedt": 0.12031936645507812, "IJCAI2023>>program>>Main Track>>398>>authors>>authors_1>>Xuewei Li": 0.12299203872680664, "IJCAI2023>>program>>Main Track>>1817>>authors>>authors_1>>Kayla Boggess": 0.1230115294456482, "IJCAI2023>>program>>Main Track>>2969>>authors>>authors_4>>Devarajan Sridharan": 0.1233750581741333, "IJCAI2023>>program>>Main Track>>1798>>authors>>authors_1>>Xuhao Jiang": 0.12372833490371704, "IJCAI2023>>program>>Main Track>>375>>authors>>authors_1>>Jianxiong Tang": 0.12412095069885254, "IJCAI2023>>program>>Main Track>>2296>>authors>>authors_2>>Zening Zeng": 0.1248619556427002, "IJCAI2023>>program>>Main Track>>3140>>authors>>authors_2>>Saqib Ameen": 0.12520772218704224, "IJCAI2023>>program>>Main Track>>1856>>authors>>authors_1>>David Radke": 0.12558740377426147, "IJCAI2023>>program>>Main Track>>3002>>authors>>authors_1>>Francesco Belardinelli": 0.12562501430511475, "IJCAI2023>>program>>Main Track>>1045>>authors>>authors_3>>Elisheva S. Shamash": 0.12713301181793213, "IJCAI2023>>program>>Main Track>>4586>>authors>>authors_6>>Hiroshi Saruwatari": 0.12732994556427002, "IJCAI2023>>program>>Main Track>>4276>>authors>>authors_8>>Bettina Könighofer": 0.12738049030303955}, "What is the abstract of the presentation under the Main Track program number 298, and what field does it pertain to?": {"IJCAI2023>>program>>Main Track>>1588>>authors>>authors_4>>Li Guo": 0.12025713920593262, "IJCAI2023>>program>>Main Track>>4504>>authors>>authors_2>>Tobias Geibinger": 0.1202925443649292, "IJCAI2023>>program>>Main Track>>3171>>abstract>>The recent popularity of Wordle has revived interest in guessing games. We develop a general method for finding optimal strategies for guessing games while avoiding an exhaustive search. Our main contribution are several theorems that build towards a general theory to prove optimality of a strategy for a guessing game. This work is developed to apply to any guessing game, but we use Wordle as an example to present concrete results.": 0.1259942650794983, "IJCAI2023>>program>>Main Track>>4172>>title>>An Experimental Comparison of Multiwinner Voting Rules on Approval Elections": 0.12608134746551514, "IJCAI2023>>program>>Main Track>>228>>authors>>authors_1>>Md Asifur Rahman": 0.1263919472694397, "IJCAI2023>>program>>Main Track>>3002>>authors>>authors_1>>Francesco Belardinelli": 0.12700265645980835, "IJCAI2023>>program>>Main Track>>3934>>authors>>authors_4>>Luc De Raedt": 0.130470871925354, "IJCAI2023>>program>>Main Track>>534>>authors>>authors_1>>Cuong Tran": 0.13176590204238892, "IJCAI2023>>program>>Main Track>>396>>authors>>authors_4>>Chi-Man Pun": 0.13229995965957642, "IJCAI2023>>program>>Main Track>>3138>>authors>>authors_1>>Kezhen Chen": 0.1327139139175415, "IJCAI2023>>program>>Main Track>>892>>authors>>authors_1>>Zesen Cheng": 0.13272875547409058, "IJCAI2023>>program>>Main Track>>234>>keywords>>keywords_3>>Planning and Scheduling -> PS: Applications": 0.13299953937530518, "IJCAI2023>>program>>Main Track>>4271>>title>>Algorithmics of Egalitarian versus Equitable Sequences of Committees": 0.1330307126045227, "IJCAI2023>>program>>Main Track>>2929>>authors>>authors_1>>Haipeng Chen": 0.13326549530029297, "IJCAI2023>>program>>Main Track>>1798>>authors>>authors_1>>Xuhao Jiang": 0.13393133878707886, "IJCAI2023>>program>>Main Track>>3482>>authors>>authors_5>>Truyen Tran": 0.1340140700340271, "IJCAI2023>>program>>Main Track>>774>>authors>>authors_3>>Yabiao Wang": 0.1342141032218933}, "What are the keywords associated with the presentation under the Main Track program number 298?": {"IJCAI2023>>program>>Main Track>>234>>keywords>>keywords_3>>Planning and Scheduling -> PS: Applications": 0.10069966316223145, "IJCAI2023>>program>>Main Track>>4383>>keywords>>keywords_2>>Machine Learning -> ML: Clustering": 0.10476464033126831, "IJCAI2023>>program>>Main Track>>3195>>keywords>>keywords_2>>Computer Vision -> CV: Neural generative models, auto encoders, GANs": 0.10504764318466187, "IJCAI2023>>program>>Main Track>>4062>>keywords>>keywords_2>>Game Theory and Economic Paradigms -> GTEP: Computational social choice": 0.1067659854888916, "IJCAI2023>>program>>Main Track>>728>>keywords>>keywords_1>>Agent-based and Multi-agent Systems -> MAS: Multi-agent learning": 0.10821443796157837, "IJCAI2023>>program>>Main Track>>4799>>keywords>>keywords_3>>Knowledge Representation and Reasoning -> KRR: Causality": 0.10863548517227173, "IJCAI2023>>program>>Main Track>>2409>>keywords>>keywords_2>>Computer Vision -> CV: Representation learning": 0.10998624563217163, "IJCAI2023>>program>>Main Track>>924>>keywords>>keywords_2>>Computer Vision -> CV: 3D computer vision": 0.11004137992858887, "IJCAI2023>>program>>Main Track>>1384>>keywords>>keywords_2>>Computer Vision -> CV: Applications": 0.11005431413650513, "IJCAI2023>>program>>Main Track>>2077>>keywords>>keywords_2>>Computer Vision -> CV: Recognition (object detection, categorization)": 0.11059004068374634, "IJCAI2023>>program>>Main Track>>4228>>keywords>>keywords_1>>Game Theory and Economic Paradigms -> GTEP: Auctions and market-based systems": 0.11193108558654785, "IJCAI2023>>program>>Main Track>>298>>keywords>>keywords_2>>Computer Vision -> CV: 3D computer vision": 0.11217248439788818, "IJCAI2023>>program>>Main Track>>604>>keywords>>keywords_2>>Computer Vision -> CV: Video analysis and understanding": 0.11230683326721191, "IJCAI2023>>program>>Main Track>>3572>>keywords>>keywords_3>>Machine Learning -> ML: Learning graphical models": 0.1129104495048523, "IJCAI2023>>program>>Main Track>>4724>>keywords>>keywords_1>>Data Mining -> DM: Frequent pattern mining": 0.11308139562606812, "IJCAI2023>>program>>Main Track>>2178>>keywords>>keywords_2>>Planning and Scheduling -> PS: Scheduling": 0.11322861909866333}, "What is the approach proposed by the authors of the presentation under the Main Track program number 298 to enhance 3D surface based on 2D normal images?": {"IJCAI2023>>program>>Main Track>>298>>title>>3D Surface Super-resolution from Enhanced 2D Normal Images: A Multimodal-driven Variational AutoEncoder Approach": 0.11671555042266846, "IJCAI2023>>program>>Main Track>>298>>abstract>>3D surface super-resolution is an important technical tool in virtual reality, and it is also a research hotspot in computer vision. Due to the unstructured and irregular nature of 3D object data, it is usually difficult to obtain high-quality surface details and geometry textures via a low-cost hardware setup. In this paper, we establish a multimodal-driven variational autoencoder (mmVAE) framework to perform 3D surface enhancement based on 2D normal images. To fully leverage the multimodal learning, we investigate a multimodal Gaussian mixture model (mmGMM) to align and fuse the latent feature representations from different modalities, and further propose a cross-scale encoder-decoder structure to reconstruct high-resolution normal images. Experimental results on several benchmark datasets demonstrate that our method delivers promising surface geometry structures and details in comparison with competitive advances.": 0.11811572313308716}, "What is the title of the paper with the ID 4306 in the main track of IJCAI2023?": {"IJCAI2023>>program>>Main Track>>5195>>authors>>authors_3>>Thiago D. Simão": 0.09700298309326172, "IJCAI2023>>program>>Main Track>>4206>>authors>>authors_2>>Hendrik Molter": 0.09799432754516602, "IJCAI2023>>program>>Main Track>>2178>>authors>>authors_3>>Ruben Solozabal Ochoa de Retana": 0.0992555022239685, "IJCAI2023>>program>>Main Track>>1493>>authors>>authors_5>>Dong Yan": 0.09994834661483765, "IJCAI2023>>program>>Main Track>>4276>>authors>>authors_8>>Bettina Könighofer": 0.10070836544036865, "IJCAI2023>>program>>Journal Track>>J5939>>authors>>authors_1>>Elias Schede": 0.10208505392074585, "IJCAI2023>>program>>Main Track>>3863>>authors>>authors_7>>Hua Wu": 0.10303103923797607, "IJCAI2023>>program>>Main Track>>5012>>authors>>authors_4>>Yidong Chen": 0.10381817817687988, "IJCAI2023>>program>>Main Track>>1493>>authors>>authors_6>>Jun Zhu": 0.1040722131729126, "IJCAI2023>>program>>Main Track>>4586>>authors>>authors_5>>Shinnosuke Takamichi": 0.10409629344940186, "IJCAI2023>>program>>Main Track>>4376>>authors>>authors_1>>Zhaiming Shen": 0.10410869121551514, "IJCAI2023>>program>>Main Track>>3497>>authors>>authors_2>>Jun Yuan": 0.10441839694976807, "IJCAI2023>>program>>Main Track>>4276>>authors>>authors_4>>Katrine Bjørner": 0.10451149940490723, "IJCAI2023>>program>>Main Track>>4580>>authors>>authors_1>>Carlos Hernández": 0.10470741987228394, "IJCAI2023>>program>>Main Track>>4276>>authors>>authors_6>>Scott J. Shapiro": 0.10542905330657959, "IJCAI2023>>program>>Main Track>>863>>authors>>authors_5>>Kai Chen": 0.10643208026885986, "IJCAI2023>>program>>Main Track>>1493>>authors>>authors_3>>Xinning Zhou": 0.10656052827835083, "IJCAI2023>>program>>Main Track>>1045>>authors>>authors_3>>Elisheva S. Shamash": 0.1067575216293335, "IJCAI2023>>program>>Main Track>>4732>>authors>>authors_1>>Martin Svatoš": 0.10688066482543945, "IJCAI2023>>program>>Main Track>>3540>>authors>>authors_2>>Li Yu": 0.1074143648147583}, "Who are the authors of the paper titled 'Unifying Core-Guided and Implicit Hitting Set Based Optimization'?": {"IJCAI2023>>program>>Survey Track>>SV5660>>authors>>authors_2>>Kun Zhou": 0.16620856523513794, "IJCAI2023>>program>>Main Track>>4454>>authors>>authors_4>>Kurt Mehlhorn": 0.1700928807258606, "IJCAI2023>>program>>Main Track>>398>>authors>>authors_3>>Zhongang Qi": 0.17321085929870605, "IJCAI2023>>program>>Main Track>>1493>>authors>>authors_6>>Jun Zhu": 0.1736927032470703, "IJCAI2023>>program>>Main Track>>1626>>authors>>authors_1>>Peizheng Li": 0.1741366982460022, "IJCAI2023>>program>>Main Track>>1493>>authors>>authors_5>>Dong Yan": 0.17433780431747437, "IJCAI2023>>program>>Main Track>>5155>>authors>>authors_2>>Guodong Long": 0.1744672656059265, "IJCAI2023>>program>>Main Track>>924>>authors>>authors_1>>Xin Zhao": 0.17468661069869995, "IJCAI2023>>program>>Journal Track>>J5758>>authors>>authors_2>>Matthew E. Taylor": 0.17477506399154663, "IJCAI2023>>calls>>Journal Track>>Accepted Papers List>>J5950>>authors>>authors_5>>Subbarao Kambhampati": 0.17490935325622559, "IJCAI2023>>program>>Main Track>>1834>>authors>>authors_2>>Simon Jantsch": 0.17493754625320435, "IJCAI2023>>program>>Main Track>>1274>>authors>>authors_1>>Yiduo Li": 0.17561501264572144, "IJCAI2023>>program>>Main Track>>2587>>authors>>authors_5>>Gongping Yang": 0.17623931169509888, "IJCAI2023>>program>>Main Track>>2358>>authors>>authors_4>>Jianjun Xu": 0.1766912341117859, "IJCAI2023>>program>>Main Track>>3873>>authors>>authors_3>>Michael Wooldridge": 0.1767803430557251, "IJCAI2023>>program>>Main Track>>1384>>authors>>authors_1>>Xixuan Hao": 0.1768432855606079, "IJCAI2023>>program>>Survey Track>>SV5630>>authors>>authors_1>>Zhichun Guo": 0.1770704984664917, "IJCAI2023>>program>>Main Track>>863>>authors>>authors_8>>Jie Chen": 0.17878079414367676, "IJCAI2023>>program>>Main Track>>87>>authors>>authors_5>>Xinbing Wang": 0.17878156900405884, "IJCAI2023>>program>>Main Track>>1716>>authors>>authors_4>>Caihua Liu": 0.17936241626739502}, "What is the abstract of the paper with the ID 4306 in the main track of IJCAI2023?": {"IJCAI2023>>program>>Main Track>>2178>>authors>>authors_3>>Ruben Solozabal Ochoa de Retana": 0.09570068120956421, "IJCAI2023>>program>>Main Track>>5195>>authors>>authors_3>>Thiago D. Simão": 0.10242104530334473, "IJCAI2023>>program>>Main Track>>4580>>authors>>authors_1>>Carlos Hernández": 0.104189932346344, "IJCAI2023>>program>>Best Papers from Sister Conferences Track>>SC20>>abstract": 0.10425758361816406, "IJCAI2023>>program>>Main Track>>1493>>authors>>authors_5>>Dong Yan": 0.10464680194854736, "IJCAI2023>>program>>Main Track>>4206>>authors>>authors_2>>Hendrik Molter": 0.10486584901809692, "IJCAI2023>>program>>Main Track>>4376>>authors>>authors_1>>Zhaiming Shen": 0.10546135902404785, "IJCAI2023>>program>>Main Track>>194>>authors>>authors_4>>Xuming He": 0.10633796453475952, "IJCAI2023>>program>>Main Track>>3863>>authors>>authors_7>>Hua Wu": 0.10683327913284302, "IJCAI2023>>program>>Main Track>>648>>authors>>authors_8>>Yue Qi": 0.10710752010345459, "IJCAI2023>>program>>Main Track>>863>>authors>>authors_5>>Kai Chen": 0.10799634456634521, "IJCAI2023>>program>>Main Track>>4276>>authors>>authors_8>>Bettina Könighofer": 0.10800504684448242, "IJCAI2023>>program>>Main Track>>3497>>authors>>authors_2>>Jun Yuan": 0.10825109481811523, "IJCAI2023>>program>>Main Track>>2705>>authors>>authors_5>>Dangyang Chen": 0.10851162672042847, "IJCAI2023>>program>>Main Track>>774>>authors>>authors_3>>Yabiao Wang": 0.10881912708282471, "IJCAI2023>>program>>Main Track>>5012>>authors>>authors_4>>Yidong Chen": 0.10918313264846802, "IJCAI2023>>program>>Best Papers from Sister Conferences Track>>SC8>>abstract": 0.10926848649978638, "IJCAI2023>>program>>Main Track>>1045>>authors>>authors_3>>Elisheva S. Shamash": 0.11030256748199463, "IJCAI2023>>program>>Main Track>>1798>>authors>>authors_4>>Chenxi Ma": 0.11042702198028564, "IJCAI2023>>program>>Main Track>>4766>>authors>>authors_4>>Abdoulaye Banire Diallo": 0.1106826663017273}, "What are the keywords associated with the paper ID 4306 in IJCAI2023?": {"IJCAI2023>>program>>Best Papers from Sister Conferences Track>>SC3>>keywords>>keywords_2>>Sister Conferences Best Papers -> Data Mining": 0.09484171867370605, "IJCAI2023>>program>>Main Track>>4226>>keywords>>keywords_2>>Computer Vision -> CV: Applications": 0.09665876626968384, "IJCAI2023>>program>>Best Papers from Sister Conferences Track>>SC12>>keywords>>keywords_1>>Sister Conferences Best Papers -> Constraint Satisfaction and Optimization": 0.09755182266235352, "IJCAI2023>>program>>Best Papers from Sister Conferences Track>>SC3>>keywords>>keywords_3>>Sister Conferences Best Papers -> Humans and AI": 0.09901076555252075, "IJCAI2023>>program>>Best Papers from Sister Conferences Track>>SC25>>keywords>>keywords_1>>Sister Conferences Best Papers -> Machine Learning": 0.09962093830108643, "IJCAI2023>>program>>Best Papers from Sister Conferences Track>>SC3>>keywords>>keywords_1>>Sister Conferences Best Papers -> AI Ethics, Trust, Fairness": 0.10110658407211304, "IJCAI2023>>program>>Main Track>>1593>>keywords>>keywords_2>>Computer Vision -> CV: Applications": 0.10169589519500732, "IJCAI2023>>program>>Main Track>>3704>>keywords>>keywords_1>>Constraint Satisfaction and Optimization -> CSO: Distributed constraints": 0.1037188172340393, "IJCAI2023>>program>>Journal Track>>J5946>>keywords>>keywords_3>>Multidisciplinary Topics and Applications -> MDA: Security and privacy": 0.1046038269996643, "IJCAI2023>>program>>Main Track>>326>>keywords>>keywords_1>>Humans and AI -> HAI: Human computation and crowdsourcing": 0.1048160195350647, "IJCAI2023>>program>>Best Papers from Sister Conferences Track>>SC12>>keywords>>keywords_4>>Sister Conferences Best Papers -> Search": 0.10575932264328003, "IJCAI2023>>program>>Main Track>>4783>>keywords>>keywords_2>>Knowledge Representation and Reasoning -> KRR: Preference modelling and preference-based reasoning": 0.1078253984451294, "IJCAI2023>>program>>Main Track>>2693>>keywords>>keywords_1>>Multidisciplinary Topics and Applications -> MDA: Health and medicine": 0.10810667276382446, "IJCAI2023>>calls>>Journal Track>>Accepted Papers List>>J5950>>keywords>>keywords_3>>Mixed Planning": 0.10826849937438965, "IJCAI2023>>program>>Main Track>>1596>>keywords>>keywords_2>>Computer Vision -> CV: Image and video retrieval": 0.10834962129592896}, "Is the paper titled 'Unifying Core-Guided and Implicit Hitting Set Based Optimization' related to Constraint Satisfaction and Optimization?": {"IJCAI2023>>program>>Main Track>>4306>>title>>Unifying Core-Guided and Implicit Hitting Set Based Optimization": 0.1277034878730774, "IJCAI2023>>program>>Main Track>>4306>>abstract>>Two of the most central algorithmic paradigms implemented in practical solvers for maximum satisfiability (MaxSAT) and other related declarative paradigms for NP-hard combinatorial optimization are the core-guided (CG) and implicit hitting set (IHS) approaches. We develop a general unifying algorithmic framework, based on the recent notion of abstract cores, that captures both CG and IHS computations. The framework offers a unified way of establishing the correctness of variants of the approaches, and can be instantiated in novel ways giving rise to new algorithmic variants of the core-guided and IHS approaches. We illustrate the latter aspect by developing a prototype implementation of an algorithm variant for MaxSAT based on the framework.": 0.14194971323013306, "IJCAI2023>>program>>Journal Track>>J5948>>keywords>>keywords_1>>Constraint Satisfaction and Optimization -> CSO: Constraint programming": 0.15200746059417725, "IJCAI2023>>program>>Main Track>>2005>>keywords>>keywords_2>>Constraint Satisfaction and Optimization -> CSO: Constraint optimization": 0.15414047241210938, "IJCAI2023>>program>>Main Track>>1836>>keywords>>keywords_1>>Constraint Satisfaction and Optimization -> CSO: Constraint learning and acquisition": 0.1548362374305725, "IJCAI2023>>program>>Main Track>>3697>>keywords>>keywords_2>>Constraint Satisfaction and Optimization -> CSO: Applications": 0.15521538257598877, "IJCAI2023>>program>>Journal Track>>J5942>>keywords>>keywords_1>>Constraint Satisfaction and Optimization -> CSO: Satisfiabilty": 0.157110333442688, "IJCAI2023>>program>>Journal Track>>J5948>>keywords>>keywords_2>>Constraint Satisfaction and Optimization -> CSO: Modeling": 0.15774375200271606, "IJCAI2023>>program>>Journal Track>>J5934>>keywords>>keywords_1>>Constraint Satisfaction and Optimization -> CSO: Satisfiabilty": 0.15808475017547607, "IJCAI2023>>program>>Main Track>>2905>>keywords>>keywords_2>>Constraint Satisfaction and Optimization -> CSO: Constraint programming": 0.15847760438919067, "IJCAI2023>>program>>Best Papers from Sister Conferences Track>>SC18>>keywords>>keywords_2>>Sister Conferences Best Papers -> Constraint Satisfaction and Optimization": 0.15916168689727783}, "What is the title of the paper with id 1850 in the main track program of the IJCAI2023 conference?": {"IJCAI2023>>program>>Main Track>>863>>authors>>authors_5>>Kai Chen": 0.08190703392028809, "IJCAI2023>>program>>Main Track>>5195>>authors>>authors_3>>Thiago D. Simão": 0.08303612470626831, "IJCAI2023>>program>>Main Track>>1493>>authors>>authors_5>>Dong Yan": 0.08663839101791382, "IJCAI2023>>program>>Main Track>>2178>>authors>>authors_3>>Ruben Solozabal Ochoa de Retana": 0.08779692649841309, "IJCAI2023>>program>>Main Track>>1045>>authors>>authors_3>>Elisheva S. Shamash": 0.08850008249282837, "IJCAI2023>>program>>Main Track>>2754>>authors>>authors_5>>Bin Cui": 0.08866631984710693, "IJCAI2023>>program>>Main Track>>200>>authors>>authors_4>>Yiran Chen": 0.08937346935272217, "IJCAI2023>>program>>Main Track>>4376>>authors>>authors_1>>Zhaiming Shen": 0.09035968780517578, "IJCAI2023>>program>>Main Track>>5012>>authors>>authors_4>>Yidong Chen": 0.09041792154312134, "IJCAI2023>>program>>Main Track>>1756>>authors>>authors_4>>Conrad Schecker": 0.0914260745048523, "IJCAI2023>>program>>Main Track>>3832>>authors>>authors_6>>Felix Ulrich-Oltean": 0.0914769172668457, "IJCAI2023>>program>>Main Track>>2705>>authors>>authors_5>>Dangyang Chen": 0.09216773509979248, "IJCAI2023>>program>>Main Track>>1621>>authors>>authors_3>>Jörg Hoffmann": 0.09270679950714111, "IJCAI2023>>program>>Journal Track>>J5939>>authors>>authors_1>>Elias Schede": 0.09301835298538208, "IJCAI2023>>program>>Journal Track>>J5552>>authors>>authors_2>>Nardine Osman": 0.09326338768005371, "IJCAI2023>>program>>Main Track>>2230>>authors>>authors_3>>Sheila McIlraith": 0.09435588121414185, "IJCAI2023>>program>>Journal Track>>J5552>>authors>>authors_1>>Nieves Montes": 0.09439772367477417, "IJCAI2023>>program>>Main Track>>4580>>authors>>authors_1>>Carlos Hernández": 0.09456700086593628, "IJCAI2023>>program>>Main Track>>4586>>authors>>authors_5>>Shinnosuke Takamichi": 0.09470498561859131}, "Who are the authors of the paper 'MultiPar-T: Multiparty-Transformer for Capturing Contingent Behaviors in Group Conversations'?": {"IJCAI2023>>program>>Journal Track>>J5758>>authors>>authors_2>>Matthew E. Taylor": 0.1598859429359436, "IJCAI2023>>program>>Main Track>>1834>>authors>>authors_3>>Christel Baier": 0.16537022590637207, "IJCAI2023>>program>>Main Track>>1834>>authors>>authors_2>>Simon Jantsch": 0.16664814949035645, "IJCAI2023>>program>>Main Track>>4454>>authors>>authors_4>>Kurt Mehlhorn": 0.1669350266456604, "IJCAI2023>>program>>Main Track>>1379>>authors>>authors_1>>James Kotary": 0.16741079092025757, "IJCAI2023>>program>>Survey Track>>SV5648>>authors>>authors_5>>Pradeep K. Murukannaiah": 0.16828811168670654, "IJCAI2023>>program>>Demonstrations Track>>DM5742>>authors>>authors_3>>Biplav Srivastava": 0.16982543468475342, "IJCAI2023>>program>>Best Papers from Sister Conferences Track>>SC18>>authors>>authors_1>>Patricia Bouyer": 0.17104578018188477, "IJCAI2023>>program>>Journal Track>>J5924>>authors>>authors_6>>Daniele Magazzeni": 0.17107242345809937, "IJCAI2023>>calls>>Journal Track>>Accepted Papers List>>J5950>>authors>>authors_5>>Subbarao Kambhampati": 0.17122197151184082, "IJCAI2023>>program>>Best Papers from Sister Conferences Track>>SC20>>authors>>authors_1>>Annie Marsden": 0.1713581681251526, "IJCAI2023>>program>>Main Track>>1856>>authors>>authors_2>>Kate Larson": 0.17162978649139404, "IJCAI2023>>program>>Main Track>>1626>>authors>>authors_1>>Peizheng Li": 0.1718740463256836, "IJCAI2023>>program>>Survey Track>>SV5660>>authors>>authors_2>>Kun Zhou": 0.17263364791870117, "IJCAI2023>>program>>Best Papers from Sister Conferences Track>>SC23>>authors>>authors_4>>Zhitang Chen": 0.17273545265197754, "IJCAI2023>>program>>Demonstrations Track>>DM5742>>authors>>authors_7>>Andrea Loreggia": 0.17468923330307007, "IJCAI2023>>program>>Main Track>>2759>>authors>>authors_3>>Sagar Malhotra": 0.17483359575271606, "IJCAI2023>>program>>Demonstrations Track>>DM5731>>authors>>authors_4>>Prasenjit Mitra": 0.17585283517837524}, "What is the abstract of the paper with id 1850 in the main track program of IJCAI2023 conference?": {"IJCAI2023>>program>>Main Track>>2178>>authors>>authors_3>>Ruben Solozabal Ochoa de Retana": 0.08588558435440063, "IJCAI2023>>program>>Main Track>>863>>authors>>authors_5>>Kai Chen": 0.08688515424728394, "IJCAI2023>>program>>Main Track>>2705>>authors>>authors_5>>Dangyang Chen": 0.09204697608947754, "IJCAI2023>>program>>Main Track>>5195>>authors>>authors_3>>Thiago D. Simão": 0.09261173009872437, "IJCAI2023>>program>>Main Track>>1493>>authors>>authors_5>>Dong Yan": 0.09369015693664551, "IJCAI2023>>program>>Main Track>>4376>>authors>>authors_1>>Zhaiming Shen": 0.09390270709991455, "IJCAI2023>>program>>Main Track>>1756>>authors>>authors_4>>Conrad Schecker": 0.09480273723602295, "IJCAI2023>>program>>Main Track>>4580>>authors>>authors_1>>Carlos Hernández": 0.09505248069763184, "IJCAI2023>>program>>Main Track>>1045>>authors>>authors_3>>Elisheva S. Shamash": 0.09518206119537354, "IJCAI2023>>program>>Main Track>>1798>>authors>>authors_4>>Chenxi Ma": 0.0951998233795166, "IJCAI2023>>program>>Best Papers from Sister Conferences Track>>SC20>>abstract": 0.09531396627426147, "IJCAI2023>>program>>Main Track>>485>>authors>>authors_4>>Guojie Song": 0.09535914659500122, "IJCAI2023>>program>>Main Track>>194>>authors>>authors_4>>Xuming He": 0.0956229567527771, "IJCAI2023>>program>>Main Track>>5012>>authors>>authors_4>>Yidong Chen": 0.09582442045211792, "IJCAI2023>>program>>Main Track>>2754>>authors>>authors_5>>Bin Cui": 0.09584659337997437, "IJCAI2023>>program>>Main Track>>580>>authors>>authors_5>>Gerald Schaefer": 0.09634041786193848, "IJCAI2023>>program>>Main Track>>2230>>authors>>authors_3>>Sheila McIlraith": 0.09704762697219849, "IJCAI2023>>program>>Best Papers from Sister Conferences Track>>SC8>>abstract": 0.09712612628936768, "IJCAI2023>>program>>Main Track>>1621>>authors>>authors_3>>Jörg Hoffmann": 0.09722501039505005, "IJCAI2023>>program>>Main Track>>2929>>authors>>authors_1>>Haipeng Chen": 0.09731447696685791}, "In which categories does the paper 'MultiPar-T: Multiparty-Transformer for Capturing Contingent Behaviors in Group Conversations' fall under?": {"IJCAI2023>>program>>Main Track>>1850>>abstract>>As we move closer to real-world social AI systems, AI agents must be able to deal with multiparty (group) conversations. Recognizing and interpreting multiparty behaviors is challenging, as the system must recognize individual behavioral cues, deal with the complexity of multiple streams of data from multiple people, and recognize the subtle contingent social exchanges that take place amongst group members. To tackle this challenge, we propose the Multiparty-Transformer (Multipar- T), a transformer model for multiparty behavior modeling. The core component of our proposed approach is Crossperson Attention, which is specifically designed to detect contingent behavior between pairs of people. We verify the effectiveness of Multipar-T on a publicly available video-based group engagement detection benchmark, where it outperforms state-of-the-art approaches in average F-1 scores by 5.2% and individual class F-1 scores by up to 10.0%. Through qualitative analysis, we show that our Crossperson Attention module is able to discover contingent behaviors.": 0.10988026857376099, "IJCAI2023>>program>>Main Track>>1850>>title>>MultiPar-T: Multiparty-Transformer for Capturing Contingent Behaviors in Group Conversations": 0.11033308506011963, "IJCAI2023>>calls>>Journal Track>>Accepted Papers List>>J5950>>keywords>>keywords_3>>Mixed Planning": 0.17002171277999878, "IJCAI2023>>program>>Best Papers from Sister Conferences Track>>SC8>>title>>Harnessing Neighborhood Modeling and Asymmetry Preservation for Digraph Representation Learning": 0.17045605182647705, "IJCAI2023>>program>>Best Papers from Sister Conferences Track>>SC4>>title>>On the Versatile Uses of Partial Distance Correlation in Deep Learning": 0.17061644792556763}, "What is the main feature of the Multipar-T proposed in the 'MultiPar-T: Multiparty-Transformer for Capturing Contingent Behaviors in Group Conversations' paper?": {"IJCAI2023>>program>>Main Track>>1850>>abstract>>As we move closer to real-world social AI systems, AI agents must be able to deal with multiparty (group) conversations. Recognizing and interpreting multiparty behaviors is challenging, as the system must recognize individual behavioral cues, deal with the complexity of multiple streams of data from multiple people, and recognize the subtle contingent social exchanges that take place amongst group members. To tackle this challenge, we propose the Multiparty-Transformer (Multipar- T), a transformer model for multiparty behavior modeling. The core component of our proposed approach is Crossperson Attention, which is specifically designed to detect contingent behavior between pairs of people. We verify the effectiveness of Multipar-T on a publicly available video-based group engagement detection benchmark, where it outperforms state-of-the-art approaches in average F-1 scores by 5.2% and individual class F-1 scores by up to 10.0%. Through qualitative analysis, we show that our Crossperson Attention module is able to discover contingent behaviors.": 0.09269857406616211, "IJCAI2023>>program>>Main Track>>1850>>title>>MultiPar-T: Multiparty-Transformer for Capturing Contingent Behaviors in Group Conversations": 0.09555399417877197}, "What is the title of the paper with id 223 in the Main Track program for IJCAI2023?": {"IJCAI2023>>program>>Main Track>>2230>>authors>>authors_3>>Sheila McIlraith": 0.08491760492324829, "IJCAI2023>>program>>Main Track>>2230>>authors>>authors_1>>Pouya Shati": 0.08798348903656006, "IJCAI2023>>program>>Main Track>>2178>>authors>>authors_3>>Ruben Solozabal Ochoa de Retana": 0.08810156583786011, "IJCAI2023>>program>>Main Track>>863>>authors>>authors_5>>Kai Chen": 0.08944761753082275, "IJCAI2023>>program>>Main Track>>1493>>authors>>authors_5>>Dong Yan": 0.08980566263198853, "IJCAI2023>>program>>Main Track>>4376>>authors>>authors_1>>Zhaiming Shen": 0.0928199291229248, "IJCAI2023>>program>>Main Track>>5195>>authors>>authors_3>>Thiago D. Simão": 0.09342199563980103, "IJCAI2023>>program>>Main Track>>1621>>authors>>authors_3>>Jörg Hoffmann": 0.09381306171417236, "IJCAI2023>>program>>Main Track>>2276>>authors>>authors_2>>Di Jin": 0.09422427415847778, "IJCAI2023>>program>>Main Track>>1045>>authors>>authors_3>>Elisheva S. Shamash": 0.09436941146850586, "IJCAI2023>>program>>Main Track>>1798>>authors>>authors_4>>Chenxi Ma": 0.09441792964935303, "IJCAI2023>>program>>Main Track>>2038>>authors>>authors_1>>Zhongjing Du": 0.09450244903564453, "IJCAI2023>>program>>Main Track>>200>>authors>>authors_4>>Yiran Chen": 0.09489637613296509, "IJCAI2023>>program>>Main Track>>2321>>authors>>authors_1>>Jun Yu": 0.09522378444671631, "IJCAI2023>>program>>Main Track>>228>>authors>>authors_1>>Md Asifur Rahman": 0.09539484977722168, "IJCAI2023>>program>>Main Track>>4586>>authors>>authors_5>>Shinnosuke Takamichi": 0.09586739540100098, "IJCAI2023>>program>>Main Track>>4732>>authors>>authors_1>>Martin Svatoš": 0.09596258401870728, "IJCAI2023>>program>>Main Track>>2590>>authors>>authors_2>>Yanjie Ze": 0.09609133005142212, "IJCAI2023>>program>>Main Track>>5012>>authors>>authors_4>>Yidong Chen": 0.09636455774307251, "IJCAI2023>>program>>Main Track>>1231>>authors>>authors_3>>Seyed-Mohammad Seyed-Javadi": 0.09639960527420044}, "Who are the authors of the paper about Connectome Reconstruction in the IJCAI2023 conference?": {"IJCAI2023>>program>>Best Papers from Sister Conferences Track>>SC10>>authors>>authors_3>>Stefano Sferrazza": 0.1305866241455078, "IJCAI2023>>program>>Main Track>>2754>>authors>>authors_3>>Xupeng Miao": 0.1310526728630066, "IJCAI2023>>program>>Main Track>>1834>>authors>>authors_3>>Christel Baier": 0.1343156099319458, "IJCAI2023>>program>>Journal Track>>J5924>>authors>>authors_6>>Daniele Magazzeni": 0.13490867614746094, "IJCAI2023>>program>>Main Track>>4454>>authors>>authors_4>>Kurt Mehlhorn": 0.13560181856155396, "IJCAI2023>>program>>Survey Track>>SV5630>>authors>>authors_1>>Zhichun Guo": 0.13579392433166504, "IJCAI2023>>program>>Main Track>>5281>>authors>>authors_2>>Hao Chen": 0.13622313737869263, "IJCAI2023>>program>>Best Papers from Sister Conferences Track>>SC24>>authors>>authors_5>>Min Zhang": 0.1363086700439453, "IJCAI2023>>program>>Main Track>>3873>>authors>>authors_3>>Michael Wooldridge": 0.13684368133544922, "IJCAI2023>>program>>Main Track>>2793>>authors>>authors_4>>Chengnian Sun": 0.13688719272613525, "IJCAI2023>>program>>Main Track>>863>>authors>>authors_7>>Chang Liu": 0.13707447052001953, "IJCAI2023>>program>>Main Track>>644>>authors>>authors_5>>Geng Chen": 0.13725179433822632, "IJCAI2023>>calls>>Main Track>>Accepted Papers List>>141>>authors>>authors_4>>Qiang Fu": 0.13784438371658325, "IJCAI2023>>program>>Main Track>>1032>>authors>>authors_2>>Marie-Francine Moens": 0.1379263997077942, "IJCAI2023>>program>>Best Papers from Sister Conferences Track>>SC22>>authors>>authors_4>>Mohsen Imani": 0.13802391290664673, "IJCAI2023>>program>>Main Track>>1384>>authors>>authors_1>>Xixuan Hao": 0.13802987337112427, "IJCAI2023>>program>>Main Track>>1493>>authors>>authors_6>>Jun Zhu": 0.13833504915237427, "IJCAI2023>>program>>Survey Track>>SV5593>>authors>>authors_1>>Chengyi Liu": 0.13837456703186035, "IJCAI2023>>program>>Main Track>>398>>authors>>authors_3>>Zhongang Qi": 0.13842153549194336}, "What are the challenges faced in neural connectivity reconstruction as discussed in the paper?": {"IJCAI2023>>program>>Main Track>>223>>title>>Appearance Prompt Vision Transformer for Connectome Reconstruction": 0.16097360849380493, "IJCAI2023>>program>>Main Track>>1219>>abstract>>Federated graph learning collaboratively learns a global graph neural network with distributed graphs, where the non-independent and identically distributed property is one of the major challenge. Most relative arts focus on traditional distributed tasks like images and voices, incapable of the graph structures. This paper firstly reveals that local client distortion is brought by both node-level semantics and graph-level structure. First, for node-level semantic, we find that contrasting nodes from distinct classes is beneficial to provide a well-performing discrimination. We pull the local node towards the global node of the same class and push them away from the global node of different classes. Second, we postulate that a well-structural graph neural network possesses similarity for neighbors due to the inherent adjacency relationships. However, aligning each node with adjacent nodes hinders discrimination due to the potential class inconsistency. We transform the adjacency relationships into the similarity distribution and leverage the global model to distill the relation knowledge into the local model, which preserves the structural information and discriminability of the local model. Empirical results on three graph datasets manifest the superiority of the proposed method over counterparts.": 0.17179596424102783, "IJCAI2023>>program>>Doctoral Consortium Track>>DC5856>>abstract>>Deep neural networks (DNNs) have gained huge attention over the last several years due to their promising results in various tasks. However, due to their large model size and over-parameterization, they are recognized as being computationally demanding. Therefore, deep learning models are not well-suited to applications with limited computational resources and battery life. Current solutions to reduce computation costs mainly focus on inference efficiency while being resource-intensive during training. This Ph.D. research aims to address these challenges by developing cost-effective neural networks that can achieve decent performance on various complex tasks using minimum computational resources during training and inference of the network.": 0.17486679553985596}, "What is the proposed solution to the challenges faced in neural connectivity reconstruction in the research paper?": {"IJCAI2023>>program>>Main Track>>223>>title>>Appearance Prompt Vision Transformer for Connectome Reconstruction": 0.14867079257965088, "IJCAI2023>>program>>Doctoral Consortium Track>>DC5856>>abstract>>Deep neural networks (DNNs) have gained huge attention over the last several years due to their promising results in various tasks. However, due to their large model size and over-parameterization, they are recognized as being computationally demanding. Therefore, deep learning models are not well-suited to applications with limited computational resources and battery life. Current solutions to reduce computation costs mainly focus on inference efficiency while being resource-intensive during training. This Ph.D. research aims to address these challenges by developing cost-effective neural networks that can achieve decent performance on various complex tasks using minimum computational resources during training and inference of the network.": 0.1643681526184082, "IJCAI2023>>program>>Main Track>>3955>>abstract>>It has been found that many real networks, such as power grids and the Internet, are non-robust, i.e., attacking a small set of nodes would cause the paralysis of the entire network. Thus, the Network Enhancement Problem~(NEP), i.e., improving the robustness of a given network by modifying its structure, has attracted increasing attention. Heuristics have been proposed to address NEP. However, a hand-engineered heuristic often has significant performance limitations. A recently proposed model solving NEP by reinforcement learning has shown superior performance than heuristics on in-distribution datasets. However, their model shows considerably inferior out-of-distribution generalization ability when enhancing networks against the degree-based targeted attack. In this paper, we propose a more effective model with stronger generalization ability by incorporating domain knowledge including measurements of local network structures and decision criteria of heuristics. We further design a hierarchical attention model to utilize the network structure directly, where the query range changes from local to global. Finally, we propose neural confined local search~(NCLS) to realize the effective search of a large neighborhood, which exploits a learned model to confine the neighborhood to avoid exhaustive enumeration. We conduct extensive experiments on synthetic and real networks to verify the ability of our models.": 0.16474699974060059}, "What are the key advantages of the Appearance Prompt Vision Transformer (APViT) as discussed in the paper?": {"IJCAI2023>>program>>Main Track>>223>>abstract>>Neural connectivity reconstruction aims to understand the function of biological reconstruction and promote basic scientific research. The intricate morphology and densely intertwined branches make it an extremely challenging task. Most previous best-performing methods adopt affinity learning or metric learning. Nevertheless, they either neglect to model explicit voxel semantics caused by implicit optimization or are hysteresis to spatial information. Furthermore, the inherent locality of 3D CNNs limits modeling long-range dependencies, leading to sub-optimal results. In this work, we propose a coherent and unified Appearance Prompt Vision Transformer (APViT) to integrate affinity and metric learning to exploit the complementarity by learning long-range spatial dependencies. The proposed APViT enjoys several merits. First, the extension continuity-aware attention module aims at constructing hierarchical attention customized for neuron extensibility and slice continuity to learn instance voxel semantic context from a global perspective and utilize continuity priors to enhance voxel spatial awareness. Second, the appearance prompt modulator is responsible for leveraging voxel-adaptive appearance knowledge conditioned on affinity rich in spatial information to instruct instance voxel semantics, exploiting the potential of affinity learning to complement metric learning. Extensive experimental results on multiple challenging benchmarks demonstrate that our APViT achieves consistent improvements with huge flexibility under the same post-processing strategy.": 0.10311698913574219, "IJCAI2023>>program>>Main Track>>223>>title>>Appearance Prompt Vision Transformer for Connectome Reconstruction": 0.10734963417053223}, "What is the title of the Journal J5942 in IJCAI2023?": {"IJCAI2023>>program>>Journal Track>>J5939>>authors>>authors_1>>Elias Schede": 0.08682984113693237, "IJCAI2023>>program>>Journal Track>>J5941>>authors>>authors_2>>Andrew Searns": 0.09502500295639038, "IJCAI2023>>program>>Journal Track>>J5937>>authors>>authors_3>>Fabiano Dalpiaz": 0.10288816690444946, "IJCAI2023>>program>>Main Track>>3863>>authors>>authors_7>>Hua Wu": 0.10520273447036743, "IJCAI2023>>program>>Journal Track>>J5586>>authors>>authors_2>>Pavel Naumov": 0.10554194450378418, "IJCAI2023>>program>>Journal Track>>J5921>>authors>>authors_4>>Qianchuan Zhao": 0.10561728477478027, "IJCAI2023>>program>>Main Track>>2184>>authors>>authors_3>>Kun Wei": 0.10683935880661011, "IJCAI2023>>program>>Main Track>>5195>>authors>>authors_3>>Thiago D. Simão": 0.10845702886581421, "IJCAI2023>>program>>Main Track>>194>>authors>>authors_1>>Chuanyang Hu": 0.10870969295501709, "IJCAI2023>>program>>Journal Track>>J5933>>authors>>authors_5>>Gilles Pesant": 0.11020392179489136, "IJCAI2023>>program>>Main Track>>4206>>authors>>authors_2>>Hendrik Molter": 0.11060017347335815, "IJCAI2023>>program>>Journal Track>>J5930>>title>>Motion Planning Under Uncertainty with Complex Agents and Environments via Hybrid Search (Extended Abstract)": 0.11137509346008301, "IJCAI2023>>program>>Main Track>>200>>authors>>authors_4>>Yiran Chen": 0.11150294542312622, "IJCAI2023>>program>>Journal Track>>J5927>>authors>>authors_2>>Janardhan Rao Doppa": 0.11271548271179199, "IJCAI2023>>calls>>Journal Track>>Accepted Papers List>>J5950>>keywords>>keywords_3>>Mixed Planning": 0.11355823278427124, "IJCAI2023>>program>>Journal Track>>J5920>>authors>>authors_6>>Shivam Gupta": 0.11366850137710571, "IJCAI2023>>program>>Journal Track>>J5950>>authors>>authors_1>>Kebing Jin": 0.11407774686813354, "IJCAI2023>>program>>Journal Track>>J5933>>authors>>authors_6>>Josep Suy": 0.11433190107345581, "IJCAI2023>>program>>Survey Track>>SV5630>>authors>>authors_7>>Olaf Wiest": 0.11435365676879883}, "Who are the authors of the journal J5942 in the IJCAI2023 program?": {"IJCAI2023>>program>>Journal Track>>J5939>>authors>>authors_1>>Elias Schede": 0.0786283016204834, "IJCAI2023>>program>>Journal Track>>J5924>>authors>>authors_6>>Daniele Magazzeni": 0.08559638261795044, "IJCAI2023>>program>>Main Track>>2754>>authors>>authors_3>>Xupeng Miao": 0.0894474983215332, "IJCAI2023>>program>>Journal Track>>J5920>>authors>>authors_6>>Shivam Gupta": 0.09050339460372925, "IJCAI2023>>program>>Main Track>>345>>authors>>authors_2>>Zhiwei He": 0.09083151817321777, "IJCAI2023>>program>>Main Track>>604>>authors>>authors_8>>Jie Chen": 0.09436112642288208, "IJCAI2023>>program>>Journal Track>>J5758>>authors>>authors_1>>Sriram Ganapathi Subramanian": 0.09514784812927246, "IJCAI2023>>program>>Journal Track>>J5941>>authors>>authors_2>>Andrew Searns": 0.09565246105194092, "IJCAI2023>>program>>Journal Track>>J5921>>authors>>authors_2>>Shuai Ma": 0.09578078985214233, "IJCAI2023>>program>>Main Track>>2793>>authors>>authors_4>>Chengnian Sun": 0.096035897731781, "IJCAI2023>>program>>Journal Track>>J5931>>authors>>authors_3>>Felix Labelle": 0.09615564346313477, "IJCAI2023>>program>>Main Track>>5014>>authors>>authors_7>>Yanjie Fu": 0.0962897539138794, "IJCAI2023>>program>>Survey Track>>SV5644>>authors>>authors_4>>Wray Buntine": 0.09643524885177612, "IJCAI2023>>program>>Journal Track>>J5921>>authors>>authors_4>>Qianchuan Zhao": 0.09649968147277832, "IJCAI2023>>program>>Main Track>>1067>>authors>>authors_4>>Jun Zhou": 0.09669864177703857, "IJCAI2023>>program>>Journal Track>>J5933>>authors>>authors_5>>Gilles Pesant": 0.09671896696090698, "IJCAI2023>>program>>Journal Track>>J5933>>authors>>authors_6>>Josep Suy": 0.09675300121307373, "IJCAI2023>>program>>Journal Track>>J5937>>authors>>authors_3>>Fabiano Dalpiaz": 0.09675842523574829, "IJCAI2023>>program>>Main Track>>5281>>authors>>authors_2>>Hao Chen": 0.09686565399169922, "IJCAI2023>>program>>Main Track>>924>>authors>>authors_1>>Xin Zhao": 0.09710335731506348}, "What is the main subject of the journal J5942 in the IJCAI2023 program?": {"IJCAI2023>>program>>Journal Track>>J5939>>authors>>authors_1>>Elias Schede": 0.08904862403869629, "IJCAI2023>>program>>Main Track>>3863>>authors>>authors_7>>Hua Wu": 0.09067320823669434, "IJCAI2023>>program>>Journal Track>>J5941>>authors>>authors_2>>Andrew Searns": 0.09467709064483643, "IJCAI2023>>program>>Main Track>>4376>>authors>>authors_1>>Zhaiming Shen": 0.09608572721481323, "IJCAI2023>>program>>Main Track>>200>>authors>>authors_4>>Yiran Chen": 0.09609955549240112, "IJCAI2023>>program>>Main Track>>4629>>authors>>authors_3>>Victor Gutierrez-Basulto": 0.09630566835403442, "IJCAI2023>>program>>Main Track>>1621>>authors>>authors_3>>Jörg Hoffmann": 0.09695720672607422, "IJCAI2023>>program>>Main Track>>2178>>authors>>authors_3>>Ruben Solozabal Ochoa de Retana": 0.09807896614074707, "IJCAI2023>>program>>Main Track>>1067>>authors>>authors_4>>Jun Zhou": 0.09881609678268433, "IJCAI2023>>program>>Main Track>>2144>>authors>>authors_8>>Min-Ling Zhang": 0.09892398118972778, "IJCAI2023>>program>>Main Track>>3273>>authors>>authors_2>>Hanna Sumita": 0.09920525550842285, "IJCAI2023>>program>>Main Track>>4586>>authors>>authors_5>>Shinnosuke Takamichi": 0.09932327270507812, "IJCAI2023>>program>>Journal Track>>J5933>>authors>>authors_5>>Gilles Pesant": 0.09935534000396729, "IJCAI2023>>program>>Main Track>>580>>authors>>authors_5>>Gerald Schaefer": 0.09967809915542603, "IJCAI2023>>program>>Main Track>>1756>>authors>>authors_4>>Conrad Schecker": 0.10054099559783936, "IJCAI2023>>program>>Journal Track>>J5920>>authors>>authors_6>>Shivam Gupta": 0.10085713863372803, "IJCAI2023>>program>>Main Track>>1798>>authors>>authors_4>>Chenxi Ma": 0.10116535425186157, "IJCAI2023>>program>>Main Track>>2929>>authors>>authors_1>>Haipeng Chen": 0.10133421421051025, "IJCAI2023>>program>>Main Track>>5195>>authors>>authors_3>>Thiago D. Simão": 0.10152238607406616, "IJCAI2023>>program>>Main Track>>4636>>authors>>authors_9>>James T. Neal": 0.10185468196868896}, "What are the keywords associated with Journal J5942 in IJCAI2023?": {"IJCAI2023>>program>>Journal Track>>J5946>>keywords>>keywords_3>>Multidisciplinary Topics and Applications -> MDA: Security and privacy": 0.0826425552368164, "IJCAI2023>>program>>Journal Track>>J5942>>keywords>>keywords_3>>Constraint Satisfaction and Optimization -> CSO: Constraint satisfaction": 0.08692842721939087, "IJCAI2023>>program>>Journal Track>>J5935>>keywords>>keywords_7>>Search -> S: Heuristic search": 0.08693164587020874, "IJCAI2023>>program>>Journal Track>>J5943>>keywords>>keywords_4>>Constraint Satisfaction and Optimization -> CSO: Satisfiabilty": 0.09027647972106934, "IJCAI2023>>program>>Journal Track>>J5923>>keywords>>keywords_4>>AI Ethics, Trust, Fairness -> ETF: Ethical, legal and societal issues": 0.09032094478607178, "IJCAI2023>>program>>Journal Track>>J5935>>keywords>>keywords_6>>Search -> S: Combinatorial search and optimisation": 0.09101730585098267, "IJCAI2023>>program>>Journal Track>>J5935>>keywords>>keywords_5>>Search -> S: Applications": 0.09146547317504883, "IJCAI2023>>program>>Journal Track>>J5943>>keywords>>keywords_7>>Machine Learning -> ML: Symbolic methods": 0.09206128120422363, "IJCAI2023>>program>>Journal Track>>J5935>>keywords>>keywords_4>>Knowledge Representation and Reasoning -> KRR: Semantic Web": 0.09261572360992432, "IJCAI2023>>calls>>Journal Track>>Accepted Papers List>>J5950>>keywords>>keywords_3>>Mixed Planning": 0.09411686658859253, "IJCAI2023>>program>>Journal Track>>J5928>>keywords>>keywords_1>>Multidisciplinary Topics and Applications -> MDA: Security and privacy": 0.09577822685241699, "IJCAI2023>>program>>Journal Track>>J5926>>keywords>>keywords_4>>Uncertainty in AI -> UAI: Sequential decision making": 0.09660917520523071, "IJCAI2023>>program>>Journal Track>>J5939>>authors>>authors_1>>Elias Schede": 0.09725964069366455, "IJCAI2023>>program>>Journal Track>>J5949>>keywords>>keywords_1>>Knowledge Representation and Reasoning -> KRR: Non-monotonic reasoning": 0.09728759527206421, "IJCAI2023>>program>>Journal Track>>J5935>>keywords>>keywords_2>>Knowledge Representation and Reasoning -> KRR: Applications": 0.09764409065246582}, "What problem are the authors addressing in Journal J5942 in IJCAI2023?": {"IJCAI2023>>program>>Journal Track>>J5939>>authors>>authors_1>>Elias Schede": 0.1040959358215332, "IJCAI2023>>program>>Journal Track>>J5941>>authors>>authors_2>>Andrew Searns": 0.1094326376914978, "IJCAI2023>>program>>Journal Track>>J5937>>authors>>authors_3>>Fabiano Dalpiaz": 0.12049800157546997, "IJCAI2023>>program>>Journal Track>>J5927>>authors>>authors_2>>Janardhan Rao Doppa": 0.12180912494659424, "IJCAI2023>>program>>Main Track>>3863>>authors>>authors_7>>Hua Wu": 0.12386423349380493, "IJCAI2023>>program>>Journal Track>>J5920>>authors>>authors_6>>Shivam Gupta": 0.12444496154785156, "IJCAI2023>>program>>Journal Track>>J5924>>authors>>authors_6>>Daniele Magazzeni": 0.12570351362228394, "IJCAI2023>>program>>Journal Track>>J5924>>title>>A Logic-based Explanation Generation Framework for Classical and Hybrid Planning Problems (Extended Abstract)": 0.1257423758506775, "IJCAI2023>>program>>Journal Track>>J5921>>authors>>authors_4>>Qianchuan Zhao": 0.126511812210083, "IJCAI2023>>program>>Journal Track>>J5921>>authors>>authors_2>>Shuai Ma": 0.12658363580703735, "IJCAI2023>>program>>Journal Track>>J5949>>authors>>authors_2>>Iliana Petrova": 0.12668538093566895, "IJCAI2023>>program>>Journal Track>>J5933>>authors>>authors_5>>Gilles Pesant": 0.12749850749969482, "IJCAI2023>>program>>Main Track>>5195>>authors>>authors_3>>Thiago D. Simão": 0.12788134813308716, "IJCAI2023>>program>>Journal Track>>J5949>>authors>>authors_1>>Piero A. Bonatti": 0.1282469630241394, "IJCAI2023>>program>>Journal Track>>J5586>>authors>>authors_2>>Pavel Naumov": 0.1285915970802307, "IJCAI2023>>program>>Journal Track>>J5930>>authors>>authors_2>>Brian Williams": 0.12884914875030518, "IJCAI2023>>program>>Survey Track>>SV5630>>authors>>authors_5>>Roshni G. Iyer": 0.12893319129943848}, "What is the title of the paper with the ID 1009 in the main track of the IJCAI2023 program?": {"IJCAI2023>>program>>Main Track>>1493>>authors>>authors_5>>Dong Yan": 0.08848285675048828, "IJCAI2023>>program>>Main Track>>863>>authors>>authors_5>>Kai Chen": 0.08991926908493042, "IJCAI2023>>program>>Main Track>>5195>>authors>>authors_3>>Thiago D. Simão": 0.09024792909622192, "IJCAI2023>>program>>Main Track>>2178>>authors>>authors_3>>Ruben Solozabal Ochoa de Retana": 0.0911022424697876, "IJCAI2023>>program>>Journal Track>>J5939>>authors>>authors_1>>Elias Schede": 0.09266841411590576, "IJCAI2023>>program>>Main Track>>1045>>authors>>authors_3>>Elisheva S. Shamash": 0.09286010265350342, "IJCAI2023>>program>>Main Track>>1493>>authors>>authors_3>>Xinning Zhou": 0.0953560471534729, "IJCAI2023>>program>>Main Track>>5012>>authors>>authors_4>>Yidong Chen": 0.09637421369552612, "IJCAI2023>>program>>Main Track>>863>>authors>>authors_8>>Jie Chen": 0.09653478860855103, "IJCAI2023>>program>>Main Track>>902>>authors>>authors_1>>Tong Liu": 0.09682297706604004, "IJCAI2023>>program>>Main Track>>1798>>authors>>authors_4>>Chenxi Ma": 0.09703058004379272, "IJCAI2023>>program>>Main Track>>3832>>authors>>authors_6>>Felix Ulrich-Oltean": 0.09711354970932007, "IJCAI2023>>program>>Main Track>>200>>authors>>authors_4>>Yiran Chen": 0.09776145219802856, "IJCAI2023>>program>>Main Track>>1493>>authors>>authors_6>>Jun Zhu": 0.09842699766159058, "IJCAI2023>>program>>Main Track>>4586>>authors>>authors_5>>Shinnosuke Takamichi": 0.09863191843032837, "IJCAI2023>>program>>Main Track>>3497>>authors>>authors_2>>Jun Yuan": 0.09876370429992676, "IJCAI2023>>program>>Main Track>>4376>>authors>>authors_1>>Zhaiming Shen": 0.09898883104324341, "IJCAI2023>>program>>Main Track>>1009>>authors>>authors_3>>Tam Le": 0.09937816858291626, "IJCAI2023>>program>>Main Track>>2929>>authors>>authors_1>>Haipeng Chen": 0.09947067499160767, "IJCAI2023>>program>>Main Track>>648>>authors>>authors_8>>Yue Qi": 0.09974801540374756}, "Who are the authors of the paper titled 'Dynamic Flows on Curved Space Generated by Labeled Data'?": {"IJCAI2023>>program>>Main Track>>1384>>authors>>authors_1>>Xixuan Hao": 0.16584265232086182, "IJCAI2023>>program>>Survey Track>>SV5660>>authors>>authors_2>>Kun Zhou": 0.16737759113311768, "IJCAI2023>>program>>Main Track>>536>>authors>>authors_1>>Rao Fu": 0.1675877571105957, "IJCAI2023>>program>>Survey Track>>SV5593>>authors>>authors_8>>Qing Li": 0.16879302263259888, "IJCAI2023>>program>>Main Track>>1834>>authors>>authors_3>>Christel Baier": 0.16915935277938843, "IJCAI2023>>program>>Demonstrations Track>>DM5742>>authors>>authors_7>>Andrea Loreggia": 0.17002743482589722, "IJCAI2023>>program>>Main Track>>536>>authors>>authors_3>>Qian Li": 0.17095822095870972, "IJCAI2023>>program>>Main Track>>1856>>authors>>authors_2>>Kate Larson": 0.1710984706878662, "IJCAI2023>>program>>Main Track>>729>>authors>>authors_1>>Luca Marzari": 0.1714634895324707, "IJCAI2023>>calls>>Journal Track>>Accepted Papers List>>J5950>>authors>>authors_5>>Subbarao Kambhampati": 0.1719874143600464, "IJCAI2023>>program>>Main Track>>1626>>authors>>authors_1>>Peizheng Li": 0.17203623056411743, "IJCAI2023>>program>>Main Track>>4454>>authors>>authors_4>>Kurt Mehlhorn": 0.17224925756454468, "IJCAI2023>>program>>Survey Track>>SV5593>>authors>>authors_1>>Chengyi Liu": 0.17281925678253174, "IJCAI2023>>program>>Demonstrations Track>>DM5705>>authors>>authors_5>>Kavitha Srinivas": 0.17304444313049316, "IJCAI2023>>program>>Main Track>>4526>>authors>>authors_4>>Lirong Xia": 0.1730847954750061, "IJCAI2023>>program>>Main Track>>398>>authors>>authors_3>>Zhongang Qi": 0.17366719245910645, "IJCAI2023>>program>>Main Track>>1856>>authors>>authors_1>>David Radke": 0.1737111210823059, "IJCAI2023>>program>>Journal Track>>J5924>>authors>>authors_6>>Daniele Magazzeni": 0.1746196150779724, "IJCAI2023>>program>>Main Track>>3395>>authors>>authors_8>>Xiaolong Xu": 0.17462056875228882, "IJCAI2023>>program>>Main Track>>3127>>authors>>authors_7>>Lei Jiao": 0.17548584938049316}, "What is the abstract of the paper with the ID 1009?": {"IJCAI2023>>program>>Main Track>>1009>>abstract>>The scarcity of labeled data is a long-standing challenge for many machine learning tasks. We propose our gradient flow method to leverage the existing dataset (i.e., source) to generate new samples that are close to the dataset of interest (i.e., target). We lift both datasets to the space of probability distributions on the feature-Gaussian manifold, and then develop a gradient flow method that minimizes the maximum mean discrepancy loss. To perform the gradient flow of distributions on the curved feature-Gaussian space, we unravel the Riemannian structure of the space and compute explicitly the  Riemannian gradient of the loss function induced by the optimal transport metric. For practical applications, we also propose a discretized flow, and provide conditional results guaranteeing the global convergence of the flow to the optimum. We illustrate the results of our proposed gradient flow method on several real-world datasets and show our method can improve the accuracy of classification models in transfer learning settings.": 0.15276432037353516, "IJCAI2023>>program>>Best Papers from Sister Conferences Track>>SC5>>abstract": 0.1616930365562439, "IJCAI2023>>program>>Best Papers from Sister Conferences Track>>SC8>>abstract": 0.16227984428405762, "IJCAI2023>>program>>Journal Track>>J5944>>abstract>>The growing literature on confidentiality in knowledge representation and reasoning sometimes may cause a false sense of security, due to lack of details about\nthe attacker, and some misconceptions about security-related concepts. This paper\nanalyzes the vulnerabilities of some recent knowledge protection methods to increase the awareness about their actual effectiveness and their mutual differences.": 0.16391873359680176, "IJCAI2023>>program>>Best Papers from Sister Conferences Track>>SC6>>abstract": 0.16590815782546997, "IJCAI2023>>program>>Best Papers from Sister Conferences Track>>SC20>>abstract": 0.16645276546478271, "IJCAI2023>>program>>Main Track>>1493>>authors>>authors_3>>Xinning Zhou": 0.16723030805587769, "IJCAI2023>>program>>Main Track>>1626>>authors>>authors_1>>Peizheng Li": 0.16723090410232544, "IJCAI2023>>program>>Best Papers from Sister Conferences Track>>SC4>>abstract": 0.16809040307998657, "IJCAI2023>>calls>>Journal Track>>Accepted Papers List>>J5950>>authors>>authors_5>>Subbarao Kambhampati": 0.168975830078125, "IJCAI2023>>program>>Main Track>>194>>authors>>authors_4>>Xuming He": 0.16959190368652344}, "What are the keywords associated with the paper titled 'Dynamic Flows on Curved Space Generated by Labeled Data'?": {"IJCAI2023>>program>>Main Track>>1009>>title>>Dynamic Flows on Curved Space Generated by Labeled Data": 0.09715968370437622, "IJCAI2023>>program>>Main Track>>1809>>keywords>>keywords_2>>Machine Learning -> ML: Geometric learning": 0.16002154350280762, "IJCAI2023>>calls>>Main Track>>Accepted Papers List>>141>>keywords>>keywords_1>>Data Mining -> DM: Mining graphs": 0.16127777099609375, "IJCAI2023>>program>>Main Track>>1009>>abstract>>The scarcity of labeled data is a long-standing challenge for many machine learning tasks. We propose our gradient flow method to leverage the existing dataset (i.e., source) to generate new samples that are close to the dataset of interest (i.e., target). We lift both datasets to the space of probability distributions on the feature-Gaussian manifold, and then develop a gradient flow method that minimizes the maximum mean discrepancy loss. To perform the gradient flow of distributions on the curved feature-Gaussian space, we unravel the Riemannian structure of the space and compute explicitly the  Riemannian gradient of the loss function induced by the optimal transport metric. For practical applications, we also propose a discretized flow, and provide conditional results guaranteeing the global convergence of the flow to the optimum. We illustrate the results of our proposed gradient flow method on several real-world datasets and show our method can improve the accuracy of classification models in transfer learning settings.": 0.1616295576095581, "IJCAI2023>>program>>Survey Track>>SV5488>>keywords>>keywords_2>>Survey -> Machine Learning": 0.1638549566268921, "IJCAI2023>>program>>Doctoral Consortium Track>>DC5856>>keywords>>keywords_2>>Machine Learning -> ML: Feature extraction, selection and dimensionality reduction": 0.1644492745399475, "IJCAI2023>>program>>Main Track>>5155>>keywords>>keywords_2>>Machine Learning -> ML: Time series and data streams": 0.16487497091293335, "IJCAI2023>>program>>Main Track>>1493>>keywords>>keywords_1>>Machine Learning -> ML: Reinforcement learning": 0.16506707668304443, "IJCAI2023>>program>>Doctoral Consortium Track>>DC5856>>keywords>>keywords_3>>Machine Learning -> ML: Classification": 0.16551321744918823, "IJCAI2023>>program>>Main Track>>375>>keywords>>keywords_1>>Machine Learning -> ML: Classification": 0.16572421789169312, "IJCAI2023>>program>>Main Track>>2877>>keywords>>keywords_4>>Machine Learning -> ML: Semi-supervised learning": 0.1664130687713623}, "Does the paper with ID 1009 have a solution to the scarcity of labeled data?": {"IJCAI2023>>program>>Main Track>>1100>>abstract>>This paper studies multiparty learning, aiming to learn a model using the private data of different participants. Model reuse is a promising solution for multiparty learning, assuming that a local model has been trained for each party. Considering the potential sample selection bias among different parties, some heterogeneous model reuse approaches have been developed. However, although pre-trained local classifiers are utilized in these approaches, the characteristics of the local data are not well exploited. This motivates us to estimate the density of local data and design an auxiliary model together with the local classifiers for reuse. To address the scenarios where some local models are not well pre-trained, we further design a multiparty cross-entropy loss for calibration. Upon existing works, we address a challenging problem of heterogeneous model reuse from a decision theory perspective and take advantage of recent advances in density estimation. Experimental results on both synthetic and benchmark data demonstrate the superiority of the proposed method.": 0.16844773292541504, "IJCAI2023>>program>>Main Track>>194>>abstract>>Despite deep learning has achieved great success, it often relies on a large amount of training data with accurate labels, which are expensive and time-consuming to collect. A prominent direction to reduce the cost is to learn with noisy labels, which are ubiquitous in the real-world applications. A critical challenge for such a learning task is to reduce the effect of network memorization on the falsely-labeled data.  In this work, we propose an iterative selection approach based on the Weibull mixture model, which identifies clean data by considering the overall learning dynamics of each data instance. In contrast to the previous small-loss heuristics, we leverage the observation that deep network is easy to memorize and hard to forget clean data. In particular, we measure the difficulty of memorization and forgetting for each instance via the transition times between being misclassified and being memorized in training, and integrate them into a novel metric for selection. Based on the proposed metric, we retain a subset of identified clean data and repeat the selection procedure to iteratively refine the clean subset, which is finally used for model training. To validate our method, we perform extensive experiments on synthetic noisy datasets and real-world web data, and our strategy outperforms existing noisy-label learning methods.": 0.17337673902511597}, "What is the title of the paper AI4SG5788 in the special track on AI for Good at IJCAI 2023?": {"IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5784>>authors>>authors_4>>Stéphane de Tourdonnet": 0.061347126960754395, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5683>>authors>>authors_1>>Dixin Luo": 0.06355279684066772, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5784>>authors>>authors_2>>Philippe Vismara": 0.06463360786437988, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5788>>authors>>authors_4>>Julio Nogima": 0.06587207317352295, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5683>>authors>>authors_2>>Haoran Cheng": 0.06633800268173218, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5431>>keywords>>keywords_2>>AI for Good -> Multidisciplinary Topics and Applications": 0.06833118200302124, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5800>>keywords>>keywords_3>>AI for Good -> Search": 0.07023769617080688, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5757>>authors>>authors_1>>Sujan Dutta": 0.07108622789382935, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5821>>authors>>authors_1>>Nicolò Tamagnone": 0.07119137048721313, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5888>>authors>>authors_4>>Ganesh Ramakrishnan": 0.07139688730239868, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5788>>authors>>authors_3>>Marisa Vasconcelos": 0.07346236705780029, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5755>>authors>>authors_3>>Changjun Jiang": 0.07347595691680908, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5795>>authors>>authors_2>>Yunhe Feng": 0.07352489233016968, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5817>>authors>>authors_4>>Morgane Hoffmann": 0.07504302263259888, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5814>>authors>>authors_3>>Nalini Saligram": 0.07504904270172119}, "Who are the authors of the paper AI4SG5788 presented at IJCAI 2023?": {"IJCAI2023>>program>>Special Track on AI for Good>>AI4SG4388>>authors>>authors_1>>Markus B. Pettersson": 0.09263259172439575, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5611>>authors>>authors_3>>Jing Wu": 0.09496527910232544, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5778>>authors>>authors_3>>Ming Zhong": 0.09636127948760986, "IJCAI2023>>program>>Main Track>>194>>authors>>authors_1>>Chuanyang Hu": 0.0967566967010498, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5683>>authors>>authors_4>>Hongteng Xu": 0.09718018770217896, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5788>>authors>>authors_3>>Marisa Vasconcelos": 0.09770709276199341, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5611>>authors>>authors_4>>Nicolas Martin": 0.09828305244445801, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5431>>authors>>authors_1>>Qinqing Liu": 0.09859573841094971, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5817>>authors>>authors_8>>Michèle Sebag": 0.09861677885055542, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5750>>authors>>authors_6>>Desheng Zhang": 0.09941881895065308, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5777>>authors>>authors_3>>Xinyu Xiong": 0.10079175233840942, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5611>>authors>>authors_7>>Zahra Kalantari": 0.10083365440368652, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5800>>authors>>authors_9>>Guannan Zhang": 0.10108250379562378, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5694>>authors>>authors_4>>Dennis Shasha": 0.10250860452651978, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5800>>authors>>authors_6>>Ma Wenqi": 0.10260128974914551, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5840>>authors>>authors_6>>Jiageng Wu": 0.1028565764427185}, "What is the abstract content of the paper coded as AI4SG5788 under the Special Track on AI for Good at IJCAI 2023?": {"IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5431>>keywords>>keywords_2>>AI for Good -> Multidisciplinary Topics and Applications": 0.07560014724731445, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5814>>authors>>authors_3>>Nalini Saligram": 0.0758734941482544, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5784>>authors>>authors_4>>Stéphane de Tourdonnet": 0.07589006423950195, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5683>>authors>>authors_2>>Haoran Cheng": 0.07694804668426514, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5784>>authors>>authors_2>>Philippe Vismara": 0.07733339071273804, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5800>>keywords>>keywords_3>>AI for Good -> Search": 0.0777362585067749, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5683>>authors>>authors_1>>Dixin Luo": 0.07831346988677979, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5800>>authors>>authors_4>>Hongxuan Zhang": 0.07951205968856812, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5755>>authors>>authors_3>>Changjun Jiang": 0.08017796277999878, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG3664>>authors>>authors_5>>Xiang Zhang": 0.08022379875183105, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5788>>authors>>authors_4>>Julio Nogima": 0.08047735691070557, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5888>>authors>>authors_4>>Ganesh Ramakrishnan": 0.08072465658187866, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5458>>authors>>authors_10>>Changjun Jiang": 0.08078545331954956, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5750>>authors>>authors_3>>Yiwei Song": 0.08150273561477661, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5821>>authors>>authors_1>>Nicolò Tamagnone": 0.08164322376251221}, "What are the keywords associated with the paper AI4SG5788 at IJCAI 2023?": {"IJCAI2023>>program>>Best Papers from Sister Conferences Track>>SC3>>keywords>>keywords_3>>Sister Conferences Best Papers -> Humans and AI": 0.08796459436416626, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5836>>keywords>>keywords_1>>AI for Good -> Planning and Scheduling": 0.08808010816574097, "IJCAI2023>>program>>Best Papers from Sister Conferences Track>>SC3>>keywords>>keywords_1>>Sister Conferences Best Papers -> AI Ethics, Trust, Fairness": 0.09018748998641968, "IJCAI2023>>program>>Best Papers from Sister Conferences Track>>SC25>>keywords>>keywords_1>>Sister Conferences Best Papers -> Machine Learning": 0.09176468849182129, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5788>>keywords>>keywords_1>>AI for Good -> Humans and AI": 0.09220266342163086, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5836>>keywords>>keywords_3>>AI for Good -> Humans and AI": 0.09259247779846191, "IJCAI2023>>program>>Best Papers from Sister Conferences Track>>SC3>>keywords>>keywords_2>>Sister Conferences Best Papers -> Data Mining": 0.09266376495361328, "IJCAI2023>>program>>Best Papers from Sister Conferences Track>>SC12>>keywords>>keywords_1>>Sister Conferences Best Papers -> Constraint Satisfaction and Optimization": 0.09281915426254272, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG1368>>keywords>>keywords_2>>AI for Good -> Machine Learning": 0.09359478950500488, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5881>>keywords>>keywords_5>>AI for Good -> Multidisciplinary Topics and Applications": 0.09374362230300903, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5788>>keywords>>keywords_2>>AI for Good -> AI Ethics, Trust, Fairness": 0.0945594310760498, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5801>>keywords>>keywords_2>>AI for Good -> Natural Language Processing": 0.09512078762054443, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5431>>keywords>>keywords_2>>AI for Good -> Multidisciplinary Topics and Applications": 0.09526979923248291, "IJCAI2023>>program>>Survey Track>>SV5647>>keywords>>keywords_1>>Survey -> Machine Learning": 0.09607630968093872}, "Which indigenous language is discussed in the case study in the paper AI4SG5788 at IJCAI 2023?": {"IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5788>>abstract>>In this paper we discuss how AI can contribute to support the documentation and vitalization of Indigenous languages and how that involves a delicate balancing of ensuring social impact, exploring technical opportunities, and dealing with ethical constraints. We start by surveying previous work on using AI and NLP to support critical activities of strengthening Indigenous and endangered languages and discussing key limitations of current technologies. After presenting basic ethical constraints of working with Indigenous languages and communities, we propose that creating and deploying language technology ethically with and for Indigenous communities forces AI researchers and engineers to address some of the main shortcomings and criticisms of current technologies. Those ideas are also explored in the discussion of a real case of development of large language models for Brazilian Indigenous languages.": 0.10474282503128052, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5788>>title>>Balancing Social Impact, Opportunities, and Ethical Constraints of Using AI in the Documentation and Vitalization of Indigenous Languages": 0.12633687257766724, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5840>>abstract>>Large pre-trained models have revolutionized natural language processing (NLP) research and applications, but high training costs and limited data resources have prevented their benefits from being shared equally amongst speakers of all the world’s languages. To address issues of cross-linguistic access to such models and reduce energy consumption for sustainability during large-scale model training, this study proposes an effective and energy-efficient framework called GreenPLM that uses bilingual lexicons to directly “translate” pre-trained language models of one language into another at almost no additional cost. We validate this approach in 18 languages’ BERT models and show that this framework is comparable to, if not better than, other heuristics with high training costs. In addition, given lightweight continued pre-training on limited data where available, this framework outperforms the original monolingual language models in six out of seven tested languages with up to 200x less pre-training. Aiming at the Leave No One Behind Principle (LNOB), our approach manages to reduce inequalities between languages and energy consumption greatly. We make our code and models publicly available.": 0.14691764116287231, "IJCAI2023>>program>>Main Track>>3863>>authors>>authors_7>>Hua Wu": 0.154333233833313}, "What is the title of the paper with ID 547 in the Main Track program at IJCAI 2023?": {"IJCAI2023>>program>>Main Track>>5195>>authors>>authors_3>>Thiago D. Simão": 0.08846122026443481, "IJCAI2023>>program>>Main Track>>1493>>authors>>authors_5>>Dong Yan": 0.09204977750778198, "IJCAI2023>>program>>Main Track>>2178>>authors>>authors_3>>Ruben Solozabal Ochoa de Retana": 0.09259676933288574, "IJCAI2023>>program>>Main Track>>863>>authors>>authors_5>>Kai Chen": 0.09426134824752808, "IJCAI2023>>program>>Main Track>>200>>authors>>authors_4>>Yiran Chen": 0.09483093023300171, "IJCAI2023>>program>>Main Track>>1045>>authors>>authors_3>>Elisheva S. Shamash": 0.09505343437194824, "IJCAI2023>>program>>Main Track>>4376>>authors>>authors_1>>Zhaiming Shen": 0.09583324193954468, "IJCAI2023>>program>>Main Track>>5012>>authors>>authors_4>>Yidong Chen": 0.09819763898849487, "IJCAI2023>>program>>Main Track>>902>>authors>>authors_1>>Tong Liu": 0.0982198715209961, "IJCAI2023>>program>>Main Track>>1798>>authors>>authors_4>>Chenxi Ma": 0.09884607791900635, "IJCAI2023>>program>>Main Track>>1621>>authors>>authors_3>>Jörg Hoffmann": 0.0991051197052002, "IJCAI2023>>program>>Main Track>>2230>>authors>>authors_3>>Sheila McIlraith": 0.09927546977996826, "IJCAI2023>>program>>Main Track>>4586>>authors>>authors_5>>Shinnosuke Takamichi": 0.09934782981872559, "IJCAI2023>>program>>Main Track>>3832>>authors>>authors_6>>Felix Ulrich-Oltean": 0.09937453269958496, "IJCAI2023>>program>>Main Track>>1756>>authors>>authors_4>>Conrad Schecker": 0.09942591190338135, "IJCAI2023>>program>>Main Track>>2705>>authors>>authors_5>>Dangyang Chen": 0.09976470470428467, "IJCAI2023>>program>>Main Track>>4276>>authors>>authors_8>>Bettina Könighofer": 0.09991806745529175, "IJCAI2023>>program>>Main Track>>5145>>authors>>authors_1>>Chenghao Liu": 0.09992516040802002, "IJCAI2023>>program>>Main Track>>2754>>authors>>authors_5>>Bin Cui": 0.10009467601776123}, "Who are the authors of the paper with ID 547 in the Main Track program at IJCAI 2023?": {"IJCAI2023>>program>>Main Track>>863>>authors>>authors_5>>Kai Chen": 0.08645409345626831, "IJCAI2023>>program>>Main Track>>863>>authors>>authors_8>>Jie Chen": 0.0870976448059082, "IJCAI2023>>program>>Main Track>>1493>>authors>>authors_5>>Dong Yan": 0.08759105205535889, "IJCAI2023>>program>>Main Track>>2038>>authors>>authors_5>>Xi Wu": 0.08887261152267456, "IJCAI2023>>program>>Main Track>>3873>>authors>>authors_3>>Michael Wooldridge": 0.08943420648574829, "IJCAI2023>>program>>Main Track>>2969>>authors>>authors_4>>Devarajan Sridharan": 0.09054017066955566, "IJCAI2023>>program>>Main Track>>4276>>authors>>authors_8>>Bettina Könighofer": 0.09207391738891602, "IJCAI2023>>program>>Main Track>>2759>>authors>>authors_1>>Alessandro Daniele": 0.09227722883224487, "IJCAI2023>>program>>Main Track>>5164>>authors>>authors_4>>Jianyong Wang": 0.09262281656265259, "IJCAI2023>>program>>Main Track>>604>>authors>>authors_8>>Jie Chen": 0.09317690134048462, "IJCAI2023>>program>>Main Track>>1045>>authors>>authors_3>>Elisheva S. Shamash": 0.09329468011856079, "IJCAI2023>>program>>Main Track>>1493>>authors>>authors_6>>Jun Zhu": 0.09336650371551514, "IJCAI2023>>program>>Main Track>>3675>>authors>>authors_2>>Hongbin Xu": 0.09342855215072632, "IJCAI2023>>program>>Main Track>>4062>>authors>>authors_3>>Pascal Lenzner": 0.09365606307983398, "IJCAI2023>>program>>Main Track>>4853>>authors>>authors_1>>Dmitrii Avdiukhin": 0.09381633996963501, "IJCAI2023>>program>>Main Track>>902>>authors>>authors_1>>Tong Liu": 0.09412539005279541, "IJCAI2023>>program>>Main Track>>1813>>authors>>authors_5>>Naijia Wang": 0.09438985586166382, "IJCAI2023>>program>>Main Track>>4454>>authors>>authors_4>>Kurt Mehlhorn": 0.0948743224143982, "IJCAI2023>>program>>Main Track>>5281>>authors>>authors_2>>Hao Chen": 0.0949602723121643, "IJCAI2023>>program>>Main Track>>2230>>authors>>authors_3>>Sheila McIlraith": 0.09506869316101074}, "What is the abstract of the paper with ID 547 in the Main Track program at IJCAI 2023?": {"IJCAI2023>>program>>Main Track>>2178>>authors>>authors_3>>Ruben Solozabal Ochoa de Retana": 0.08917522430419922, "IJCAI2023>>program>>Main Track>>863>>authors>>authors_5>>Kai Chen": 0.09433937072753906, "IJCAI2023>>program>>Main Track>>1493>>authors>>authors_5>>Dong Yan": 0.09472042322158813, "IJCAI2023>>program>>Main Track>>5195>>authors>>authors_3>>Thiago D. Simão": 0.09534788131713867, "IJCAI2023>>program>>Main Track>>2705>>authors>>authors_5>>Dangyang Chen": 0.09735572338104248, "IJCAI2023>>program>>Main Track>>1045>>authors>>authors_3>>Elisheva S. Shamash": 0.09743094444274902, "IJCAI2023>>program>>Main Track>>4376>>authors>>authors_1>>Zhaiming Shen": 0.09853523969650269, "IJCAI2023>>program>>Main Track>>1798>>authors>>authors_4>>Chenxi Ma": 0.09889870882034302, "IJCAI2023>>program>>Main Track>>2754>>abstract>>Large-scale deep learning models contribute to significant performance improvements on varieties of downstream tasks. Current data and model parallelism approaches utilize model replication and partition techniques to support the distributed training of ultra-large models. However, directly deploying these systems often leads to sub-optimal training efficiency due to the complex model architectures and the strict device memory constraints. In this paper, we propose Optimal Sharded Data Parallel (OSDP), an automated parallel training system that combines the advantages from both data and model parallelism. Given the model description and the device information, OSDP makes trade-offs between the memory consumption and the hardware utilization, thus automatically generates the distributed computation graph and maximizes the overall system throughput. In addition, OSDP introduces operator splitting to further alleviate peak memory footprints during training with negligible overheads, which enables the trainability of larger models as well as the higher throughput. Extensive experimental results of OSDP on multiple different kinds of large-scale models demonstrate that the proposed strategy outperforms the state-of-the-art in multiple regards.": 0.09978967905044556, "IJCAI2023>>program>>Main Track>>902>>authors>>authors_1>>Tong Liu": 0.09999090433120728, "IJCAI2023>>program>>Main Track>>1756>>authors>>authors_4>>Conrad Schecker": 0.10010641813278198, "IJCAI2023>>program>>Main Track>>5012>>authors>>authors_4>>Yidong Chen": 0.10046851634979248}, "What are the keywords related to the paper with ID 547 in the Main Track program at IJCAI 2023?": {"IJCAI2023>>program>>Main Track>>3540>>keywords>>keywords_3>>Machine Learning -> ML: Robustness": 0.08199596405029297, "IJCAI2023>>program>>Main Track>>4766>>keywords>>keywords_3>>Machine Learning -> ML: Unsupervised learning": 0.0861465334892273, "IJCAI2023>>program>>Main Track>>451>>keywords>>keywords_3>>Machine Learning -> ML: Probabilistic machine learning": 0.08616852760314941, "IJCAI2023>>program>>Main Track>>3378>>keywords>>keywords_1>>Humans and AI -> HAI: Human-AI collaboration": 0.08792048692703247, "IJCAI2023>>program>>Main Track>>4226>>keywords>>keywords_2>>Computer Vision -> CV: Applications": 0.08799707889556885, "IJCAI2023>>program>>Main Track>>4516>>keywords>>keywords_2>>Knowledge Representation and Reasoning -> KRR: Preference modelling and preference-based reasoning": 0.0880202054977417, "IJCAI2023>>program>>Main Track>>1009>>keywords>>keywords_1>>Machine Learning -> ML: Optimization": 0.08820569515228271, "IJCAI2023>>program>>Main Track>>11>>keywords>>keywords_2>>Machine Learning -> ML: Deep reinforcement learning": 0.08821094036102295, "IJCAI2023>>program>>Main Track>>2178>>authors>>authors_3>>Ruben Solozabal Ochoa de Retana": 0.0882464051246643, "IJCAI2023>>program>>Main Track>>2911>>keywords>>keywords_1>>Natural Language Processing -> NLP: Text classification": 0.08860969543457031, "IJCAI2023>>program>>Main Track>>2577>>keywords>>keywords_1>>Knowledge Representation and Reasoning -> KRR: Automated reasoning and theorem proving": 0.08924275636672974, "IJCAI2023>>program>>Main Track>>1593>>keywords>>keywords_2>>Computer Vision -> CV: Applications": 0.08952587842941284, "IJCAI2023>>program>>Main Track>>3117>>keywords>>keywords_1>>Uncertainty in AI -> UAI: Bayesian networks": 0.08986914157867432, "IJCAI2023>>program>>Main Track>>5155>>keywords>>keywords_1>>Machine Learning -> ML: Federated learning": 0.08989155292510986, "IJCAI2023>>program>>Main Track>>451>>keywords>>keywords_1>>Computer Vision -> CV: 3D computer vision": 0.09020382165908813, "IJCAI2023>>program>>Best Papers from Sister Conferences Track>>SC25>>keywords>>keywords_1>>Sister Conferences Best Papers -> Machine Learning": 0.09060162305831909, "IJCAI2023>>program>>Main Track>>3955>>keywords>>keywords_1>>Data Mining -> DM: Mining graphs": 0.0908157229423523}, "What is the main conclusion of the paper with ID 547 in the Main Track program at IJCAI 2023?": {"IJCAI2023>>program>>Main Track>>5195>>authors>>authors_3>>Thiago D. Simão": 0.09965395927429199, "IJCAI2023>>program>>Main Track>>1621>>authors>>authors_3>>Jörg Hoffmann": 0.10411584377288818, "IJCAI2023>>program>>Main Track>>1493>>authors>>authors_5>>Dong Yan": 0.10466533899307251, "IJCAI2023>>program>>Main Track>>863>>authors>>authors_5>>Kai Chen": 0.10517716407775879, "IJCAI2023>>program>>Main Track>>1798>>authors>>authors_4>>Chenxi Ma": 0.10659575462341309, "IJCAI2023>>program>>Main Track>>1045>>authors>>authors_3>>Elisheva S. Shamash": 0.10775262117385864, "IJCAI2023>>program>>Main Track>>2178>>authors>>authors_3>>Ruben Solozabal Ochoa de Retana": 0.10809016227722168, "IJCAI2023>>program>>Main Track>>200>>authors>>authors_4>>Yiran Chen": 0.10829108953475952, "IJCAI2023>>program>>Main Track>>2230>>authors>>authors_3>>Sheila McIlraith": 0.1083601713180542, "IJCAI2023>>program>>Main Track>>3832>>authors>>authors_6>>Felix Ulrich-Oltean": 0.10856491327285767, "IJCAI2023>>program>>Main Track>>4586>>authors>>authors_5>>Shinnosuke Takamichi": 0.11028403043746948, "IJCAI2023>>program>>Main Track>>4276>>authors>>authors_4>>Katrine Bjørner": 0.11053216457366943}, "What is the title of the study conducted in track 4482 at the IJCAI 2023 conference?": {"IJCAI2023>>program>>Main Track>>3863>>authors>>authors_7>>Hua Wu": 0.09586334228515625, "IJCAI2023>>program>>Main Track>>4206>>authors>>authors_2>>Hendrik Molter": 0.10046428442001343, "IJCAI2023>>program>>Journal Track>>J5939>>authors>>authors_1>>Elias Schede": 0.10134971141815186, "IJCAI2023>>program>>Main Track>>200>>authors>>authors_4>>Yiran Chen": 0.10171586275100708, "IJCAI2023>>program>>Main Track>>2184>>authors>>authors_3>>Kun Wei": 0.10249531269073486, "IJCAI2023>>program>>Survey Track>>SV5563>>authors>>authors_4>>Xindong Wu": 0.10414952039718628, "IJCAI2023>>program>>Main Track>>4376>>authors>>authors_1>>Zhaiming Shen": 0.10439598560333252, "IJCAI2023>>program>>Journal Track>>J5941>>authors>>authors_2>>Andrew Searns": 0.10484302043914795, "IJCAI2023>>program>>Survey Track>>SV5487>>authors>>authors_7>>Zhiming Zhao": 0.10504913330078125, "IJCAI2023>>program>>Main Track>>5195>>authors>>authors_3>>Thiago D. Simão": 0.10578006505966187, "IJCAI2023>>program>>Main Track>>1778>>authors>>authors_6>>Zhiwen Yu": 0.10612326860427856, "IJCAI2023>>program>>Main Track>>5281>>authors>>authors_2>>Hao Chen": 0.10628682374954224, "IJCAI2023>>program>>Survey Track>>SV5557>>authors>>authors_1>>Jun Xia": 0.10653364658355713, "IJCAI2023>>program>>Main Track>>2178>>authors>>authors_3>>Ruben Solozabal Ochoa de Retana": 0.10653448104858398, "IJCAI2023>>program>>Main Track>>194>>authors>>authors_1>>Chuanyang Hu": 0.10719001293182373, "IJCAI2023>>program>>Main Track>>4484>>authors>>authors_4>>Francesco Pasquale": 0.10805714130401611, "IJCAI2023>>program>>Main Track>>4766>>authors>>authors_4>>Abdoulaye Banire Diallo": 0.10815542936325073, "IJCAI2023>>program>>Survey Track>>SV5630>>authors>>authors_7>>Olaf Wiest": 0.10821020603179932, "IJCAI2023>>program>>Main Track>>4580>>authors>>authors_1>>Carlos Hernández": 0.10875922441482544, "IJCAI2023>>program>>Survey Track>>SV5509>>authors>>authors_5>>Yingxue Zhang": 0.10877281427383423}, "Who is the author of the study titled 'Fair Division of a Graph into Compact Bundles' at the IJCAI 2023 conference?": {"IJCAI2023>>program>>Main Track>>1834>>authors>>authors_2>>Simon Jantsch": 0.1392536163330078, "IJCAI2023>>program>>Main Track>>1920>>authors>>authors_1>>Cristian-Paul Bara": 0.1402301788330078, "IJCAI2023>>program>>Main Track>>1834>>authors>>authors_3>>Christel Baier": 0.14172965288162231, "IJCAI2023>>program>>Main Track>>3873>>authors>>authors_3>>Michael Wooldridge": 0.14385932683944702, "IJCAI2023>>program>>Main Track>>2754>>authors>>authors_3>>Xupeng Miao": 0.14497649669647217, "IJCAI2023>>program>>Journal Track>>J5758>>authors>>authors_1>>Sriram Ganapathi Subramanian": 0.14509576559066772, "IJCAI2023>>program>>Journal Track>>J5939>>authors>>authors_5>>Viktor Bengs": 0.14527153968811035, "IJCAI2023>>program>>Survey Track>>SV5660>>authors>>authors_1>>Yifan Li": 0.1454763412475586, "IJCAI2023>>program>>Main Track>>1379>>authors>>authors_1>>James Kotary": 0.14597511291503906, "IJCAI2023>>program>>Main Track>>1493>>authors>>authors_6>>Jun Zhu": 0.14673352241516113, "IJCAI2023>>program>>Demonstrations Track>>DM5742>>authors>>authors_3>>Biplav Srivastava": 0.1470913290977478, "IJCAI2023>>program>>Main Track>>2759>>authors>>authors_1>>Alessandro Daniele": 0.14734041690826416, "IJCAI2023>>program>>Main Track>>1045>>authors>>authors_1>>Takayuki Osogami": 0.14862072467803955, "IJCAI2023>>program>>Main Track>>2759>>authors>>authors_3>>Sagar Malhotra": 0.14886188507080078, "IJCAI2023>>program>>Journal Track>>J5939>>authors>>authors_1>>Elias Schede": 0.1489095687866211, "IJCAI2023>>program>>Journal Track>>J5924>>authors>>authors_6>>Daniele Magazzeni": 0.14936721324920654, "IJCAI2023>>program>>Survey Track>>SV5644>>authors>>authors_4>>Wray Buntine": 0.14944016933441162, "IJCAI2023>>program>>Main Track>>1793>>authors>>authors_1>>Siddhartha Banerjee": 0.15009093284606934, "IJCAI2023>>program>>Best Papers from Sister Conferences Track>>SC24>>authors>>authors_5>>Min Zhang": 0.15060681104660034}, "What is the abstract of the study conducted by Jayakrishnan Madathil at the IJCAI 2023 conference?": {"IJCAI2023>>program>>Best Papers from Sister Conferences Track>>SC20>>abstract": 0.11269271373748779, "IJCAI2023>>program>>Best Papers from Sister Conferences Track>>SC1>>abstract": 0.11877059936523438, "IJCAI2023>>program>>Best Papers from Sister Conferences Track>>SC1>>title>>Causal Conceptions of Fairness and their Consequences.": 0.12039709091186523, "IJCAI2023>>program>>Best Papers from Sister Conferences Track>>SC24>>abstract": 0.1210101842880249, "IJCAI2023>>program>>Best Papers from Sister Conferences Track>>SC21>>authors>>authors_3>>Qi Zhu": 0.12131655216217041, "IJCAI2023>>program>>Survey Track>>SV5563>>authors>>authors_1>>Yi He": 0.12146228551864624, "IJCAI2023>>program>>Main Track>>2184>>authors>>authors_3>>Kun Wei": 0.12172943353652954, "IJCAI2023>>program>>Best Papers from Sister Conferences Track>>SC4>>abstract": 0.12189221382141113, "IJCAI2023>>program>>Best Papers from Sister Conferences Track>>SC8>>abstract": 0.12219405174255371, "IJCAI2023>>program>>Best Papers from Sister Conferences Track>>SC20>>authors>>authors_3>>Aaron Sidford": 0.12249648571014404, "IJCAI2023>>program>>Main Track>>3151>>authors>>authors_3>>Bhaskar Ramasubramanian": 0.12279218435287476, "IJCAI2023>>program>>Best Papers from Sister Conferences Track>>SC26>>abstract": 0.12284117937088013, "IJCAI2023>>program>>Main Track>>200>>authors>>authors_4>>Yiran Chen": 0.12292373180389404, "IJCAI2023>>program>>Main Track>>5195>>authors>>authors_3>>Thiago D. Simão": 0.12338721752166748, "IJCAI2023>>program>>Main Track>>3863>>authors>>authors_7>>Hua Wu": 0.12405669689178467, "IJCAI2023>>program>>Journal Track>>J5941>>authors>>authors_2>>Andrew Searns": 0.12420207262039185, "IJCAI2023>>program>>Journal Track>>J5920>>authors>>authors_6>>Shivam Gupta": 0.12455803155899048, "IJCAI2023>>program>>Best Papers from Sister Conferences Track>>SC5>>abstract": 0.1249130368232727, "IJCAI2023>>program>>Main Track>>194>>authors>>authors_1>>Chuanyang Hu": 0.12492537498474121, "IJCAI2023>>program>>Best Papers from Sister Conferences Track>>SC5>>title>>Learning Causal Effects on Hypergraphs": 0.12513816356658936}, "What are the keywords related to the study titled 'Fair Division of a Graph into Compact Bundles' at the IJCAI 2023 conference?": {"IJCAI2023>>program>>Main Track>>4482>>abstract>>We study the computational complexity of fair division of indivisible items in an enriched model: there is an underlying graph on the set of items. And we have to allocate the items (i.e., the vertices of the graph) to a set of agents in such a way that (a) the allocation is fair (for appropriate notions of fairness) and (b) each agent receives a bundle of items (i.e., a subset of vertices) that induces a subgraph with a specific “nice structure.” This model has previously been studied in the literature with the nice structure being a connected subgraph. In this paper, we propose an alternative for connectivity in fair division. We introduce compact graphs, and look for fair allocations in which each agent receives a compact bundle of items. Through compactness, we attempt to capture the idea that every agent must receive a bundle of “closely related” items. We prove a host of hardness and tractability results with respect to fairness concepts such as proportionality, envy-freeness and maximin share guarantee.": 0.08574223518371582, "IJCAI2023>>program>>Main Track>>2774>>title>>Approximating Fair Division on D-Claw-Free Graphs": 0.11388468742370605, "IJCAI2023>>program>>Main Track>>2774>>abstract>>We study the problem of fair allocation of indivisible goods that form a graph and the bundles that are distributed to agents are connected subgraphs of this graph. We focus on the maximin share and the proportional fairness criteria. It is well-known that allocations satisfying these criteria may not exist for many graphs including complete graphs and cycles. Therefore, it is natural to look for approximate allocations, i.e., allocations guaranteeing each agent a certain portion of the value that is satisfactory to her. In this paper we consider the class of graphs of goods which do not contain a star with d+1 edges (where d > 1) as an induced subgraph. For this class of graphs we prove that there is an allocation assigning each agent a connected bundle of value at least 1/d of her maximin share. Moreover, for the same class of graphs of goods, we show a theorem which specifies what fraction of the proportional share can be guaranteed to each agent if the values of single goods for the agents are bounded by a given fraction of this share.": 0.11867213249206543}, "What is the primary field of study for the research conducted by Jayakrishnan Madathil at the IJCAI 2023 conference?": {"IJCAI2023>>program>>Early Career Track>>Early Career Track_6>>Research field>>AI and Multi-agent Systems for Real World Decision Making": 0.112232506275177, "IJCAI2023>>program>>Main Track>>200>>authors>>authors_4>>Yiran Chen": 0.11558771133422852, "IJCAI2023>>program>>Survey Track>>SV5647>>keywords>>keywords_1>>Survey -> Machine Learning": 0.11729949712753296, "IJCAI2023>>program>>Main Track>>2184>>authors>>authors_3>>Kun Wei": 0.11775946617126465, "IJCAI2023>>program>>Main Track>>3863>>authors>>authors_7>>Hua Wu": 0.1186876893043518, "IJCAI2023>>program>>Main Track>>11>>keywords>>keywords_2>>Machine Learning -> ML: Deep reinforcement learning": 0.11912935972213745, "IJCAI2023>>program>>Best Papers from Sister Conferences Track>>SC20>>authors>>authors_3>>Aaron Sidford": 0.11930423974990845, "IJCAI2023>>program>>Journal Track>>J5941>>authors>>authors_2>>Andrew Searns": 0.12055540084838867, "IJCAI2023>>calls>>Call For Demos>>Call For Demos_5>>Topics>>Topics_1>>As the premier general conference on AI, IJCAI 2023 welcomes submissions across all areas of AI; the Demonstrations Track of IJCAI 2023 follows this principle, though with an emphasis on the applicability of AI to practice. Thus, the track scope includes (but is not limited to) the following subfields of AI:>>As the premier general conference on AI, IJCAI 2023 welcomes submissions across all areas of AI; the Demonstrations Track of IJCAI 2023 follows this principle, though with an emphasis on the applicability of AI to practice. Thus, the track scope includes (but is not limited to) the following subfields of AI:_12>>Planning and Scheduling": 0.12078404426574707}, "What is the title of the paper J5946 in the Journal Track?": {"IJCAI2023>>program>>Journal Track>>J5939>>authors>>authors_1>>Elias Schede": 0.11537498235702515, "IJCAI2023>>program>>Journal Track>>J5925>>authors>>authors_2>>Mouaz Al-Mallah": 0.1334286332130432, "IJCAI2023>>program>>Journal Track>>J5927>>authors>>authors_2>>Janardhan Rao Doppa": 0.14082318544387817, "IJCAI2023>>program>>Journal Track>>J5937>>authors>>authors_3>>Fabiano Dalpiaz": 0.14125847816467285, "IJCAI2023>>program>>Journal Track>>J5930>>title>>Motion Planning Under Uncertainty with Complex Agents and Environments via Hybrid Search (Extended Abstract)": 0.1416112780570984, "IJCAI2023>>program>>Journal Track>>J5943>>title>>On Tackling Explanation Redundancy in Decision Trees (Extended Abstract)": 0.14277338981628418, "IJCAI2023>>calls>>Journal Track>>Accepted Papers List>>J5950>>keywords>>keywords_3>>Mixed Planning": 0.14435243606567383, "IJCAI2023>>program>>Journal Track>>J5941>>authors>>authors_2>>Andrew Searns": 0.14480584859848022, "IJCAI2023>>program>>Journal Track>>J5938>>title>>Survey and Evaluation of Causal Discovery Methods for Time Series (Extended Abstract)": 0.14488816261291504, "IJCAI2023>>program>>Journal Track>>J5758>>authors>>authors_1>>Sriram Ganapathi Subramanian": 0.1455727219581604, "IJCAI2023>>program>>Journal Track>>J5950>>authors>>authors_2>>Hankz Hankui Zhuo": 0.14575707912445068, "IJCAI2023>>calls>>Journal Track>>Accepted Papers List>>J5950>>authors>>authors_5>>Subbarao Kambhampati": 0.14610213041305542, "IJCAI2023>>program>>Journal Track>>J5933>>authors>>authors_6>>Josep Suy": 0.14669477939605713, "IJCAI2023>>program>>Journal Track>>J5924>>title>>A Logic-based Explanation Generation Framework for Classical and Hybrid Planning Problems (Extended Abstract)": 0.14782506227493286, "IJCAI2023>>program>>Journal Track>>J5946>>authors>>authors_1>>Farhad Mohsin": 0.14926153421401978, "IJCAI2023>>program>>Journal Track>>J5935>>authors>>authors_1>>Patrick Rodler": 0.14946353435516357, "IJCAI2023>>program>>Journal Track>>J5552>>authors>>authors_1>>Nieves Montes": 0.14971321821212769}, "Who are the authors of the paper J5946 in the Journal Track?": {"IJCAI2023>>program>>Journal Track>>J5939>>authors>>authors_1>>Elias Schede": 0.1083635687828064, "IJCAI2023>>program>>Journal Track>>J5758>>authors>>authors_2>>Matthew E. Taylor": 0.12024348974227905, "IJCAI2023>>program>>Journal Track>>J5758>>authors>>authors_1>>Sriram Ganapathi Subramanian": 0.12274277210235596, "IJCAI2023>>program>>Journal Track>>J5924>>authors>>authors_6>>Daniele Magazzeni": 0.12298750877380371, "IJCAI2023>>program>>Journal Track>>J5927>>authors>>authors_2>>Janardhan Rao Doppa": 0.12740248441696167, "IJCAI2023>>program>>Main Track>>1856>>authors>>authors_2>>Kate Larson": 0.13089114427566528, "IJCAI2023>>program>>Journal Track>>J5931>>authors>>authors_3>>Felix Labelle": 0.13130146265029907, "IJCAI2023>>program>>Journal Track>>J5923>>authors>>authors_1>>Loïs Vanhée": 0.13185983896255493, "IJCAI2023>>program>>Journal Track>>J5950>>authors>>authors_2>>Hankz Hankui Zhuo": 0.13209271430969238, "IJCAI2023>>calls>>Journal Track>>Accepted Papers List>>J5950>>authors>>authors_5>>Subbarao Kambhampati": 0.13223320245742798, "IJCAI2023>>program>>Journal Track>>J5948>>authors>>authors_1>>Özgür Akgün": 0.13334596157073975, "IJCAI2023>>program>>Journal Track>>J5925>>authors>>authors_2>>Mouaz Al-Mallah": 0.13343214988708496, "IJCAI2023>>program>>Journal Track>>J5933>>authors>>authors_6>>Josep Suy": 0.13349145650863647, "IJCAI2023>>program>>Main Track>>4454>>authors>>authors_4>>Kurt Mehlhorn": 0.13432955741882324, "IJCAI2023>>program>>Main Track>>1834>>authors>>authors_2>>Simon Jantsch": 0.13433003425598145, "IJCAI2023>>program>>Journal Track>>J5950>>authors>>authors_3>>Zhanhao Xiao": 0.13441163301467896, "IJCAI2023>>program>>Main Track>>863>>authors>>authors_8>>Jie Chen": 0.13454151153564453, "IJCAI2023>>program>>Journal Track>>J5946>>authors>>authors_1>>Farhad Mohsin": 0.13457822799682617}, "What is the central theme of the paper J5946?": {"IJCAI2023>>program>>Journal Track>>J5946>>title>>Learning to Design Fair and Private Voting Rules (Extended Abstract)": 0.17877209186553955, "IJCAI2023>>program>>Journal Track>>J5944>>abstract>>The growing literature on confidentiality in knowledge representation and reasoning sometimes may cause a false sense of security, due to lack of details about\nthe attacker, and some misconceptions about security-related concepts. This paper\nanalyzes the vulnerabilities of some recent knowledge protection methods to increase the awareness about their actual effectiveness and their mutual differences.": 0.1794944405555725, "IJCAI2023>>program>>Journal Track>>J5927>>authors>>authors_2>>Janardhan Rao Doppa": 0.18015682697296143, "IJCAI2023>>program>>Journal Track>>J5946>>keywords>>keywords_3>>Multidisciplinary Topics and Applications -> MDA: Security and privacy": 0.18066471815109253, "IJCAI2023>>program>>Journal Track>>J5923>>keywords>>keywords_6>>AI Ethics, Trust, Fairness -> ETF: Societal Impact of AI": 0.18113374710083008, "IJCAI2023>>program>>Journal Track>>J5944>>keywords>>keywords_2>>Knowledge Representation and Reasoning -> General": 0.18114835023880005, "IJCAI2023>>program>>Journal Track>>J5586>>abstract>>The article proposes a new approach to reasoning about knowledge and strategies in multiagent systems. It emphasizes data, not agents, as the source of strategic knowledge. The approach brings together Armstrong’s functional dependency expression from database theory, a data-informed knowledge modality based on a recent work by Baltag and van Benthem, and a newly proposed data-informed strategy modality. The main technical result is a sound and complete logical system that describes the interplay between these three logical operators.": 0.18433648347854614, "IJCAI2023>>program>>Journal Track>>J5923>>keywords>>keywords_2>>AI Ethics, Trust, Fairness -> General": 0.18467265367507935, "IJCAI2023>>program>>Journal Track>>J5924>>title>>A Logic-based Explanation Generation Framework for Classical and Hybrid Planning Problems (Extended Abstract)": 0.18748044967651367, "IJCAI2023>>program>>Journal Track>>J5939>>authors>>authors_1>>Elias Schede": 0.18845134973526, "IJCAI2023>>program>>Journal Track>>J5922>>keywords>>keywords_6>>Robotics -> ROB: Cognitive robotics": 0.18954890966415405, "IJCAI2023>>program>>Journal Track>>J5944>>title>>A False Sense of Security (Extended Abstract)": 0.1902393102645874}, "What is the new concept introduced in this paper J5946?": {"IJCAI2023>>program>>Journal Track>>J5946>>title>>Learning to Design Fair and Private Voting Rules (Extended Abstract)": 0.1668304204940796, "IJCAI2023>>program>>Journal Track>>J5927>>authors>>authors_2>>Janardhan Rao Doppa": 0.1737757921218872, "IJCAI2023>>program>>Journal Track>>J5939>>authors>>authors_1>>Elias Schede": 0.17427849769592285, "IJCAI2023>>program>>Journal Track>>J5944>>abstract>>The growing literature on confidentiality in knowledge representation and reasoning sometimes may cause a false sense of security, due to lack of details about\nthe attacker, and some misconceptions about security-related concepts. This paper\nanalyzes the vulnerabilities of some recent knowledge protection methods to increase the awareness about their actual effectiveness and their mutual differences.": 0.17452818155288696, "IJCAI2023>>program>>Journal Track>>J5946>>keywords>>keywords_3>>Multidisciplinary Topics and Applications -> MDA: Security and privacy": 0.17474812269210815, "IJCAI2023>>program>>Journal Track>>J5924>>title>>A Logic-based Explanation Generation Framework for Classical and Hybrid Planning Problems (Extended Abstract)": 0.17590254545211792, "IJCAI2023>>calls>>Journal Track>>Accepted Papers List>>J5950>>authors>>authors_5>>Subbarao Kambhampati": 0.17632275819778442, "IJCAI2023>>program>>Journal Track>>J5948>>title>>Conjure: Automatic Generation of Constraint Models from Problem Specifications (Extended Abstract)": 0.1772192120552063, "IJCAI2023>>program>>Journal Track>>J5925>>title>>Interpretable Local Concept-based Explanation with Human Feedback to Predict All-cause Mortality (Extended Abstract)": 0.17925482988357544}, "What are the keywords related to paper J5946?": {"IJCAI2023>>program>>Journal Track>>J5946>>keywords>>keywords_3>>Multidisciplinary Topics and Applications -> MDA: Security and privacy": 0.149255633354187, "IJCAI2023>>program>>Journal Track>>J5935>>keywords>>keywords_6>>Search -> S: Combinatorial search and optimisation": 0.1572972536087036, "IJCAI2023>>program>>Journal Track>>J5935>>keywords>>keywords_5>>Search -> S: Applications": 0.16134518384933472, "IJCAI2023>>program>>Journal Track>>J5930>>keywords>>keywords_3>>Robotics -> ROB: Motion and path planning": 0.1620524525642395, "IJCAI2023>>program>>Journal Track>>J5944>>keywords>>keywords_2>>Knowledge Representation and Reasoning -> General": 0.1622081995010376, "IJCAI2023>>program>>Journal Track>>J5935>>keywords>>keywords_7>>Search -> S: Heuristic search": 0.1626814603805542, "IJCAI2023>>program>>Journal Track>>J5936>>keywords>>keywords_1>>Natural Language Processing -> NLP: Tagging, chunking, and parsing": 0.16487658023834229, "IJCAI2023>>program>>Journal Track>>J5936>>keywords>>keywords_2>>Natural Language Processing -> General": 0.16490256786346436, "IJCAI2023>>program>>Journal Track>>J5922>>keywords>>keywords_5>>Planning and Scheduling -> PS: Robot planning": 0.1651419997215271, "IJCAI2023>>program>>Journal Track>>J5943>>keywords>>keywords_4>>Constraint Satisfaction and Optimization -> CSO: Satisfiabilty": 0.16569066047668457, "IJCAI2023>>calls>>Journal Track>>Accepted Papers List>>J5950>>keywords>>keywords_1>>Planning": 0.1671532392501831, "IJCAI2023>>program>>Journal Track>>J5940>>keywords>>keywords_5>>Knowledge Representation and Reasoning -> KRR: Reasoning about knowledge and belief": 0.1672002077102661, "IJCAI2023>>calls>>Journal Track>>Accepted Papers List>>J5950>>keywords>>keywords_3>>Mixed Planning": 0.16720592975616455, "IJCAI2023>>program>>Journal Track>>J5946>>keywords>>keywords_2>>AI Ethics, Trust, Fairness -> ETF: Fairness and diversity": 0.16728121042251587, "IJCAI2023>>program>>Journal Track>>J5920>>keywords>>keywords_1>>AI Ethics, Trust, Fairness -> General": 0.16759824752807617, "IJCAI2023>>program>>Journal Track>>J5688>>keywords>>keywords_2>>Agent-based and Multi-agent Systems -> MAS: Agent theories and models": 0.16778308153152466}, "What is the paper J5946 proposing?": {"IJCAI2023>>calls>>Journal Track>>Accepted Papers List>>J5950>>authors>>authors_5>>Subbarao Kambhampati": 0.17153626680374146, "IJCAI2023>>program>>Journal Track>>J5939>>authors>>authors_1>>Elias Schede": 0.17194020748138428, "IJCAI2023>>program>>Journal Track>>J5946>>title>>Learning to Design Fair and Private Voting Rules (Extended Abstract)": 0.17234456539154053, "IJCAI2023>>program>>Journal Track>>J5944>>abstract>>The growing literature on confidentiality in knowledge representation and reasoning sometimes may cause a false sense of security, due to lack of details about\nthe attacker, and some misconceptions about security-related concepts. This paper\nanalyzes the vulnerabilities of some recent knowledge protection methods to increase the awareness about their actual effectiveness and their mutual differences.": 0.17837166786193848, "IJCAI2023>>calls>>Journal Track>>Accepted Papers List>>J5950>>keywords>>keywords_3>>Mixed Planning": 0.182567298412323, "IJCAI2023>>program>>Journal Track>>J5927>>authors>>authors_2>>Janardhan Rao Doppa": 0.18303263187408447, "IJCAI2023>>program>>Journal Track>>J5924>>title>>A Logic-based Explanation Generation Framework for Classical and Hybrid Planning Problems (Extended Abstract)": 0.18380850553512573, "IJCAI2023>>program>>Journal Track>>J5924>>authors>>authors_6>>Daniele Magazzeni": 0.184262216091156, "IJCAI2023>>program>>Journal Track>>J5937>>authors>>authors_3>>Fabiano Dalpiaz": 0.18519484996795654, "IJCAI2023>>program>>Journal Track>>J5921>>authors>>authors_4>>Qianchuan Zhao": 0.18677520751953125, "IJCAI2023>>program>>Main Track>>1626>>authors>>authors_1>>Peizheng Li": 0.18693792819976807}, "According to the paper J5946, is it possible to always obtain maximal economic efficiency with high fairness?": {"IJCAI2023>>program>>Journal Track>>J5946>>abstract>>Voting is used widely to aggregate preferences to make a collective decision. In this paper, we focus on evaluating and designing voting rules that support both the privacy of the voting agents and a notion of fairness over such agents. First, we introduce a novel notion of group fairness and adopt the existing notion of local differential privacy. We then evaluate the level of group fairness in several existing voting rules, as well as the trade-offs between fairness and privacy, showing that it is not possible to always obtain maximal economic efficiency with high fairness. \nThen, we present both a machine learning and a constrained optimization approach to design new voting rules that are fair while maintaining a high level of economic efficiency. Finally, we empirically examine the effect of adding noise to create local differentially private voting rules and discuss the three-way trade-off between economic efficiency, fairness, and privacy.": 0.13583284616470337, "IJCAI2023>>program>>Survey Track>>SV5550>>abstract>>Fair allocation of indivisible goods is a central topic in many AI applications. Unfortunately, the corresponding problems are known to be NP-hard for many fairness concepts, so unless P = NP, exact polynomial-time algorithms cannot exist for them. In practical applications, however, it would be highly desirable to find exact solutions as quickly as possible. This motivates the study of algorithms that—even though they only run in exponential time—are as fast as possible and exactly solve such problems. We present known complexity results for them and give a survey of important techniques for designing such algorithms, mainly focusing on four common fairness notions: max-min fairness, maximin share, maximizing Nash social welfare, and envy-freeness. We also highlight the most challenging open problems for future work.": 0.1405341625213623, "IJCAI2023>>program>>Main Track>>1003>>title>>New Fairness Concepts for Allocating Indivisible Items": 0.15535807609558105, "IJCAI2023>>program>>Journal Track>>J5941>>title>>Ordinal Maximin Share Approximation for Goods (Extended Abstract)": 0.15635746717453003}, "Does the paper J5946 discuss the effect of adding noise to create local differentially private voting rules?": {"IJCAI2023>>program>>Journal Track>>J5946>>abstract>>Voting is used widely to aggregate preferences to make a collective decision. In this paper, we focus on evaluating and designing voting rules that support both the privacy of the voting agents and a notion of fairness over such agents. First, we introduce a novel notion of group fairness and adopt the existing notion of local differential privacy. We then evaluate the level of group fairness in several existing voting rules, as well as the trade-offs between fairness and privacy, showing that it is not possible to always obtain maximal economic efficiency with high fairness. \nThen, we present both a machine learning and a constrained optimization approach to design new voting rules that are fair while maintaining a high level of economic efficiency. Finally, we empirically examine the effect of adding noise to create local differentially private voting rules and discuss the three-way trade-off between economic efficiency, fairness, and privacy.": 0.11988574266433716, "IJCAI2023>>program>>Journal Track>>J5946>>title>>Learning to Design Fair and Private Voting Rules (Extended Abstract)": 0.12507390975952148, "IJCAI2023>>program>>Main Track>>4526>>abstract>>We study strategic behavior in iterative plurality voting for multiple issues under uncertainty. We introduce a model synthesizing simultaneous multi-issue voting with Meir et al. [2014]’s local dominance theory, in which agents repeatedly update their votes based on sets of vote profiles they deem possible, and determine its convergence properties. After demonstrating that local dominance improvement dynamics may fail to converge, we present two sufficient model refinements that guarantee convergence from any initial vote profile for binary issues: constraining agents to have O-legal preferences, where issues are ordered by importance, and endowing agents with less uncertainty about issues they are modifying than others. Our empirical studies demonstrate that while cycles are common for agents without uncertainty, introducing uncertainty makes convergence almost guaranteed in practice.": 0.17969530820846558}, "What is the title of program 1560 in the Main Track of IJCAI2023?": {"IJCAI2023>>program>>Main Track>>1756>>authors>>authors_4>>Conrad Schecker": 0.08539319038391113, "IJCAI2023>>program>>Main Track>>1798>>authors>>authors_4>>Chenxi Ma": 0.08959174156188965, "IJCAI2023>>program>>Main Track>>3832>>authors>>authors_6>>Felix Ulrich-Oltean": 0.09013617038726807, "IJCAI2023>>program>>Main Track>>1621>>authors>>authors_3>>Jörg Hoffmann": 0.09018939733505249, "IJCAI2023>>program>>Main Track>>2716>>authors>>authors_5>>Siya Qiu": 0.09039968252182007, "IJCAI2023>>program>>Main Track>>1045>>authors>>authors_3>>Elisheva S. Shamash": 0.09120213985443115, "IJCAI2023>>program>>Main Track>>2178>>authors>>authors_3>>Ruben Solozabal Ochoa de Retana": 0.09138435125350952, "IJCAI2023>>program>>Main Track>>4376>>authors>>authors_1>>Zhaiming Shen": 0.09234839677810669, "IJCAI2023>>program>>Main Track>>621>>authors>>authors_1>>Xiang Li": 0.09261047840118408, "IJCAI2023>>program>>Main Track>>1540>>authors>>authors_7>>Enhong Chen": 0.09269070625305176, "IJCAI2023>>program>>Main Track>>2038>>authors>>authors_1>>Zhongjing Du": 0.0928158164024353, "IJCAI2023>>program>>Main Track>>4586>>authors>>authors_5>>Shinnosuke Takamichi": 0.09343862533569336, "IJCAI2023>>program>>Main Track>>1542>>authors>>authors_4>>Dongfang Liu": 0.09346580505371094, "IJCAI2023>>program>>Main Track>>2590>>authors>>authors_2>>Yanjie Ze": 0.09413129091262817, "IJCAI2023>>program>>Main Track>>4090>>authors>>authors_1>>Yifan Wu": 0.09428048133850098, "IJCAI2023>>program>>Main Track>>1679>>authors>>authors_4>>Przemysław A. Wałęga": 0.09434205293655396, "IJCAI2023>>program>>Main Track>>1685>>authors>>authors_3>>Chenhao Lin": 0.09487950801849365, "IJCAI2023>>program>>Main Track>>1542>>authors>>authors_1>>Liqi Yan": 0.09488242864608765, "IJCAI2023>>program>>Main Track>>2230>>authors>>authors_3>>Sheila McIlraith": 0.09508109092712402}, "Who are the authors of the program 1560 in the Main Track of IJCAI2023?": {"IJCAI2023>>program>>Main Track>>1716>>authors>>authors_4>>Caihua Liu": 0.08151596784591675, "IJCAI2023>>program>>Main Track>>604>>authors>>authors_8>>Jie Chen": 0.0816422700881958, "IJCAI2023>>program>>Main Track>>1067>>authors>>authors_4>>Jun Zhou": 0.08286464214324951, "IJCAI2023>>program>>Main Track>>1621>>authors>>authors_3>>Jörg Hoffmann": 0.08315932750701904, "IJCAI2023>>program>>Main Track>>1685>>authors>>authors_6>>Junyi Zhao": 0.08320575952529907, "IJCAI2023>>program>>Main Track>>1698>>authors>>authors_2>>Miao Xu": 0.08324670791625977, "IJCAI2023>>program>>Main Track>>2759>>authors>>authors_1>>Alessandro Daniele": 0.08401525020599365, "IJCAI2023>>program>>Main Track>>660>>authors>>authors_1>>Xinyang Huang": 0.08453363180160522, "IJCAI2023>>program>>Main Track>>408>>authors>>authors_3>>Zejian Li": 0.08485233783721924, "IJCAI2023>>program>>Main Track>>1540>>authors>>authors_7>>Enhong Chen": 0.08497041463851929, "IJCAI2023>>program>>Main Track>>2038>>authors>>authors_6>>Jiliu Zhou": 0.08503484725952148, "IJCAI2023>>program>>Main Track>>1813>>authors>>authors_5>>Naijia Wang": 0.08533453941345215, "IJCAI2023>>program>>Main Track>>863>>authors>>authors_5>>Kai Chen": 0.08550673723220825, "IJCAI2023>>program>>Main Track>>1685>>authors>>authors_3>>Chenhao Lin": 0.08561205863952637, "IJCAI2023>>program>>Main Track>>2038>>authors>>authors_5>>Xi Wu": 0.08562755584716797, "IJCAI2023>>program>>Main Track>>3832>>authors>>authors_6>>Felix Ulrich-Oltean": 0.08596360683441162, "IJCAI2023>>program>>Main Track>>1340>>authors>>authors_2>>Ming Yan": 0.08602941036224365, "IJCAI2023>>program>>Main Track>>4586>>authors>>authors_5>>Shinnosuke Takamichi": 0.08638948202133179, "IJCAI2023>>program>>Main Track>>1540>>authors>>authors_4>>Hao Wang": 0.08639639616012573, "IJCAI2023>>program>>Main Track>>863>>authors>>authors_8>>Jie Chen": 0.08658909797668457, "IJCAI2023>>program>>Main Track>>1361>>authors>>authors_1>>Hanyuan Chen": 0.08667033910751343}, "What is the main area of research or the keywords of the program 1560 in the Main Track of IJCAI2023?": {"IJCAI2023>>program>>Main Track>>3540>>keywords>>keywords_3>>Machine Learning -> ML: Robustness": 0.07950246334075928, "IJCAI2023>>program>>Main Track>>11>>keywords>>keywords_2>>Machine Learning -> ML: Deep reinforcement learning": 0.07960885763168335, "IJCAI2023>>program>>Main Track>>4516>>keywords>>keywords_2>>Knowledge Representation and Reasoning -> KRR: Preference modelling and preference-based reasoning": 0.0825355052947998, "IJCAI2023>>program>>Main Track>>256>>keywords>>keywords_2>>Multidisciplinary Topics and Applications -> MDA: Health and medicine": 0.08367347717285156, "IJCAI2023>>program>>Main Track>>1268>>keywords>>keywords_2>>Natural Language Processing -> NLP: Interpretability and analysis of models for NLP": 0.08414173126220703, "IJCAI2023>>program>>Main Track>>2911>>keywords>>keywords_1>>Natural Language Processing -> NLP: Text classification": 0.08448565006256104, "IJCAI2023>>program>>Main Track>>2178>>authors>>authors_3>>Ruben Solozabal Ochoa de Retana": 0.08575266599655151, "IJCAI2023>>program>>Main Track>>1685>>keywords>>keywords_2>>Humans and AI -> HAI: Personalization and user modeling": 0.08598685264587402, "IJCAI2023>>program>>Main Track>>3195>>keywords>>keywords_2>>Computer Vision -> CV: Neural generative models, auto encoders, GANs": 0.08679568767547607, "IJCAI2023>>program>>Main Track>>4765>>keywords>>keywords_1>>Natural Language Processing -> NLP: Question answering": 0.08731281757354736, "IJCAI2023>>program>>Main Track>>827>>keywords>>keywords_1>>Computer Vision -> CV: 3D computer vision": 0.08747076988220215, "IJCAI2023>>program>>Main Track>>451>>keywords>>keywords_3>>Machine Learning -> ML: Probabilistic machine learning": 0.08779984712600708, "IJCAI2023>>program>>Main Track>>3959>>keywords>>keywords_1>>Computer Vision -> CV: Adversarial learning, adversarial attack and defense methods": 0.08866119384765625, "IJCAI2023>>program>>Main Track>>4799>>keywords>>keywords_3>>Knowledge Representation and Reasoning -> KRR: Causality": 0.08869975805282593, "IJCAI2023>>program>>Main Track>>5155>>keywords>>keywords_1>>Machine Learning -> ML: Federated learning": 0.0889320969581604, "IJCAI2023>>program>>Main Track>>663>>keywords>>keywords_2>>Machine Learning -> ML: Clustering": 0.08894115686416626}, "What is the abstract of the program 1560 in the Main Track of IJCAI2023?": {"IJCAI2023>>program>>Main Track>>2178>>authors>>authors_3>>Ruben Solozabal Ochoa de Retana": 0.083854079246521, "IJCAI2023>>program>>Main Track>>1756>>authors>>authors_4>>Conrad Schecker": 0.0876082181930542, "IJCAI2023>>program>>Main Track>>2716>>authors>>authors_5>>Siya Qiu": 0.0896335244178772, "IJCAI2023>>program>>Main Track>>1798>>authors>>authors_4>>Chenxi Ma": 0.09014040231704712, "IJCAI2023>>program>>Main Track>>774>>authors>>authors_3>>Yabiao Wang": 0.09114640951156616, "IJCAI2023>>program>>Main Track>>1542>>authors>>authors_4>>Dongfang Liu": 0.09253084659576416, "IJCAI2023>>program>>Main Track>>580>>authors>>authors_5>>Gerald Schaefer": 0.09264779090881348, "IJCAI2023>>program>>Main Track>>4629>>authors>>authors_3>>Victor Gutierrez-Basulto": 0.09363466501235962, "IJCAI2023>>program>>Main Track>>1621>>authors>>authors_3>>Jörg Hoffmann": 0.09436309337615967, "IJCAI2023>>program>>Main Track>>4580>>authors>>authors_1>>Carlos Hernández": 0.09486174583435059, "IJCAI2023>>program>>Main Track>>2705>>authors>>authors_5>>Dangyang Chen": 0.09529781341552734, "IJCAI2023>>program>>Main Track>>2276>>authors>>authors_2>>Di Jin": 0.09536826610565186, "IJCAI2023>>program>>Main Track>>4376>>authors>>authors_1>>Zhaiming Shen": 0.09542912244796753, "IJCAI2023>>program>>Main Track>>1045>>authors>>authors_3>>Elisheva S. Shamash": 0.09556621313095093, "IJCAI2023>>program>>Main Track>>863>>authors>>authors_5>>Kai Chen": 0.09564673900604248, "IJCAI2023>>program>>Main Track>>2038>>authors>>authors_1>>Zhongjing Du": 0.09614694118499756, "IJCAI2023>>program>>Main Track>>648>>authors>>authors_8>>Yue Qi": 0.09657835960388184, "IJCAI2023>>program>>Main Track>>2230>>authors>>authors_3>>Sheila McIlraith": 0.09667277336120605, "IJCAI2023>>program>>Main Track>>396>>authors>>authors_4>>Chi-Man Pun": 0.09697335958480835, "IJCAI2023>>program>>Main Track>>1540>>authors>>authors_4>>Hao Wang": 0.0973515510559082}, "Can the agent always execute the nondeterministic abstract actions to completion at the concrete level in the program 1560 in the Main Track of IJCAI2023?": {"IJCAI2023>>program>>Main Track>>1560>>abstract>>We develop a general framework for abstracting the behavior of an agent that operates in a nondeterministic domain, i.e., where the agent does not control\nthe outcome of the nondeterministic actions, based on the nondeterministic situation calculus and the ConGolog programming language. We assume that\nwe have both an abstract and a concrete nondeterministic basic action theory, and a refinement mapping which  specifies how abstract actions, decomposed into agent actions and environment reactions, are implemented by concrete ConGolog programs. This new setting supports strategic reasoning and strategy synthesis, by allowing us to quantify separately on agent actions and environment reactions. We show that if the agent has a (strong FOND) plan/strategy to achieve a goal/complete a task at the abstract level, and it can always execute the nondeterministic abstract actions to completion at the concrete level, then there exist a refinement of it that is a (strong FOND) plan/strategy to achieve the refinement of the goal/task at the concrete level.": 0.09145909547805786, "IJCAI2023>>program>>Main Track>>1560>>title>>Abstraction of Nondeterministic Situation Calculus Action Theories": 0.11613190174102783, "IJCAI2023>>program>>Main Track>>4276>>abstract>>Principled accountability for autonomous decision-making in uncertain environments requires distinguishing intentional outcomes from negligent designs from actual accidents. We propose analyzing the behavior of autonomous agents through a quantitative measure of the evidence of intentional behavior. We model an uncertain environment as a Markov Decision Process (MDP). For a given scenario, we rely on probabilistic model checking to compute the ability of the agent to influence reaching a certain event. We call this the scope of agency. We say that there is evidence of intentional behavior if the scope of agency is high and the decisions of the agent are close to being optimal for reaching the event.  Our method applies counterfactual reasoning to automatically generate relevant scenarios that can be analyzed to increase the confidence of our assessment. In a case study, we show how our method can distinguish between ‘intentional’ and ‘accidental’ traffic collisions.": 0.13138049840927124, "IJCAI2023>>program>>Main Track>>4276>>title>>Analyzing Intentional Behavior in Autonomous Agents under Uncertainty": 0.13721948862075806}, "What is the title of the paper with id 3863?": {"IJCAI2023>>program>>Main Track>>1626>>authors>>authors_1>>Peizheng Li": 0.15405374765396118, "IJCAI2023>>program>>Main Track>>1626>>authors>>authors_2>>Shuxiao Ding": 0.15877974033355713, "IJCAI2023>>program>>Main Track>>1493>>authors>>authors_3>>Xinning Zhou": 0.1603095531463623, "IJCAI2023>>program>>Main Track>>1493>>authors>>authors_5>>Dong Yan": 0.16198158264160156, "IJCAI2023>>program>>Main Track>>3863>>authors>>authors_7>>Hua Wu": 0.1628354787826538, "IJCAI2023>>calls>>Journal Track>>Accepted Papers List>>J5950>>authors>>authors_5>>Subbarao Kambhampati": 0.1654343605041504, "IJCAI2023>>program>>Main Track>>398>>authors>>authors_3>>Zhongang Qi": 0.16613483428955078, "IJCAI2023>>program>>Main Track>>2836>>authors>>authors_2>>Markus Bläser": 0.16662544012069702, "IJCAI2023>>program>>Main Track>>4969>>authors>>authors_1>>Dabin Zhang": 0.16848701238632202, "IJCAI2023>>program>>Main Track>>1856>>authors>>authors_2>>Kate Larson": 0.16863209009170532, "IJCAI2023>>program>>Journal Track>>J5939>>authors>>authors_1>>Elias Schede": 0.169175386428833, "IJCAI2023>>program>>Main Track>>3873>>authors>>authors_3>>Michael Wooldridge": 0.17119407653808594, "IJCAI2023>>program>>Main Track>>4480>>title>>Adversarial Contention Resolution Games": 0.17181092500686646, "IJCAI2023>>program>>Main Track>>1493>>authors>>authors_6>>Jun Zhu": 0.17297297716140747, "IJCAI2023>>program>>Main Track>>5012>>authors>>authors_4>>Yidong Chen": 0.17313069105148315, "IJCAI2023>>program>>Doctoral Consortium Track>>DC5898>>authors>>authors_1>>Wiebke Hutiri": 0.17340821027755737, "IJCAI2023>>program>>Main Track>>5195>>authors>>authors_3>>Thiago D. Simão": 0.1739281415939331, "IJCAI2023>>program>>Main Track>>4351>>authors>>authors_2>>Konstantinos Thomas": 0.17395365238189697, "IJCAI2023>>program>>Best Papers from Sister Conferences Track>>SC7>>authors>>authors_1>>Dario Simionato": 0.17447710037231445, "IJCAI2023>>program>>Main Track>>4438>>authors>>authors_4>>Keerthiram Murugesan": 0.17470252513885498}, "Who are the authors of the paper with id 3863?": {"IJCAI2023>>program>>Main Track>>1856>>authors>>authors_2>>Kate Larson": 0.1383400559425354, "IJCAI2023>>program>>Main Track>>1626>>authors>>authors_1>>Peizheng Li": 0.14051097631454468, "IJCAI2023>>program>>Main Track>>398>>authors>>authors_3>>Zhongang Qi": 0.14530134201049805, "IJCAI2023>>program>>Main Track>>3873>>authors>>authors_3>>Michael Wooldridge": 0.14626610279083252, "IJCAI2023>>program>>Main Track>>4760>>authors>>authors_2>>Andrzej Kaczmarczyk": 0.14869314432144165, "IJCAI2023>>program>>Main Track>>4078>>authors>>authors_3>>Stefan Szeider": 0.14912551641464233, "IJCAI2023>>program>>Journal Track>>J5919>>authors>>authors_3>>Pascal Van Hentenryck": 0.15016746520996094, "IJCAI2023>>program>>Main Track>>1384>>authors>>authors_1>>Xixuan Hao": 0.15071314573287964, "IJCAI2023>>program>>Main Track>>4969>>authors>>authors_1>>Dabin Zhang": 0.150750994682312, "IJCAI2023>>program>>Main Track>>2836>>authors>>authors_2>>Markus Bläser": 0.15080541372299194, "IJCAI2023>>program>>Main Track>>3848>>authors>>authors_4>>Enda Howley": 0.15120983123779297, "IJCAI2023>>program>>Journal Track>>J5924>>authors>>authors_6>>Daniele Magazzeni": 0.15188515186309814, "IJCAI2023>>program>>Main Track>>1856>>authors>>authors_1>>David Radke": 0.15270930528640747, "IJCAI2023>>program>>Main Track>>1493>>authors>>authors_5>>Dong Yan": 0.15298902988433838, "IJCAI2023>>program>>Main Track>>3526>>authors>>authors_1>>Xiaolin Zheng": 0.15323495864868164, "IJCAI2023>>program>>Main Track>>1493>>authors>>authors_6>>Jun Zhu": 0.15324139595031738, "IJCAI2023>>calls>>Journal Track>>Accepted Papers List>>J5950>>authors>>authors_5>>Subbarao Kambhampati": 0.15453076362609863, "IJCAI2023>>program>>Main Track>>4454>>authors>>authors_4>>Kurt Mehlhorn": 0.15463626384735107, "IJCAI2023>>program>>Main Track>>3832>>authors>>authors_4>>Ian Miguel": 0.1553206443786621, "IJCAI2023>>program>>Main Track>>1834>>authors>>authors_3>>Christel Baier": 0.1554875373840332}, "What is the abstract of the paper titled 'Less Learn Shortcut: Analyzing and Mitigating Learning of Spurious Feature-Label Correlation'?": {"IJCAI2023>>program>>Main Track>>3863>>title>>Less Learn Shortcut: Analyzing and Mitigating Learning of Spurious Feature-Label Correlation": 0.10823041200637817, "IJCAI2023>>program>>Main Track>>3863>>abstract>>Recent research has revealed that deep neural networks often take dataset biases as a shortcut to make decisions rather than understand tasks, leading to failures in real-world applications. In this study, we focus on the spurious correlation between word features and labels that models learn from the biased data distribution of training data. In particular, we define the word highly co-occurring with a specific label as biased word, and the example containing biased word as biased example. Our analysis shows that biased examples are easier for models to learn, while at the time of prediction, biased words make a significantly higher contribution to the models’ predictions, and models tend to assign predicted labels over-relying on the spurious correlation between words and labels. To mitigate models’ over-reliance on the shortcut (i.e. spurious correlation), we propose a training strategy Less-Learn-Shortcut (LLS): our strategy quantifies the biased degree of the biased examples and down-weights them accordingly. Experimental results on Question Matching, Natural Language Inference and Sentiment Analysis tasks show that LLS is a task-agnostic strategy and can improve the model performance on adversarial data while maintaining good performance on in-domain data.": 0.1405462622642517, "IJCAI2023>>program>>Main Track>>382>>title>>Learning to Learn from Corrupted Data for Few-Shot Learning": 0.1478360891342163}, "What are the keywords associated with the paper id 3863?": {"IJCAI2023>>program>>Main Track>>1593>>keywords>>keywords_2>>Computer Vision -> CV: Applications": 0.14160621166229248, "IJCAI2023>>program>>Best Papers from Sister Conferences Track>>SC3>>keywords>>keywords_1>>Sister Conferences Best Papers -> AI Ethics, Trust, Fairness": 0.14382266998291016, "IJCAI2023>>program>>Best Papers from Sister Conferences Track>>SC12>>keywords>>keywords_4>>Sister Conferences Best Papers -> Search": 0.14465326070785522, "IJCAI2023>>program>>Journal Track>>J5946>>keywords>>keywords_3>>Multidisciplinary Topics and Applications -> MDA: Security and privacy": 0.14488071203231812, "IJCAI2023>>calls>>Main Track>>Accepted Papers List>>141>>keywords>>keywords_1>>Data Mining -> DM: Mining graphs": 0.1471858024597168, "IJCAI2023>>program>>Best Papers from Sister Conferences Track>>SC3>>keywords>>keywords_2>>Sister Conferences Best Papers -> Data Mining": 0.14727872610092163, "IJCAI2023>>program>>Best Papers from Sister Conferences Track>>SC12>>keywords>>keywords_1>>Sister Conferences Best Papers -> Constraint Satisfaction and Optimization": 0.1494351625442505, "IJCAI2023>>program>>Main Track>>4068>>keywords>>keywords_1>>Game Theory and Economic Paradigms -> GTEP: Computational social choice": 0.1500387191772461, "IJCAI2023>>program>>Main Track>>4480>>keywords>>keywords_1>>Game Theory and Economic Paradigms -> GTEP: Noncooperative games": 0.1502901315689087, "IJCAI2023>>program>>Main Track>>3863>>keywords>>keywords_1>>Natural Language Processing -> NLP: Question answering": 0.150987446308136, "IJCAI2023>>program>>Main Track>>4226>>keywords>>keywords_2>>Computer Vision -> CV: Applications": 0.15108972787857056, "IJCAI2023>>program>>Main Track>>2877>>keywords>>keywords_1>>Multidisciplinary Topics and Applications -> MDA: Bioinformatics": 0.15207111835479736, "IJCAI2023>>program>>Doctoral Consortium Track>>DC5901>>keywords>>keywords_1>>General -> General": 0.15216946601867676, "IJCAI2023>>program>>Main Track>>1621>>keywords>>keywords_1>>Multidisciplinary Topics and Applications -> MDA: Software engineering": 0.15218466520309448, "IJCAI2023>>program>>Journal Track>>J5935>>keywords>>keywords_6>>Search -> S: Combinatorial search and optimisation": 0.15262138843536377}, "What is the proposed strategy in the study 'Less Learn Shortcut: Analyzing and Mitigating Learning of Spurious Feature-Label Correlation'?": {"IJCAI2023>>program>>Main Track>>3863>>title>>Less Learn Shortcut: Analyzing and Mitigating Learning of Spurious Feature-Label Correlation": 0.1153181791305542, "IJCAI2023>>program>>Main Track>>3863>>abstract>>Recent research has revealed that deep neural networks often take dataset biases as a shortcut to make decisions rather than understand tasks, leading to failures in real-world applications. In this study, we focus on the spurious correlation between word features and labels that models learn from the biased data distribution of training data. In particular, we define the word highly co-occurring with a specific label as biased word, and the example containing biased word as biased example. Our analysis shows that biased examples are easier for models to learn, while at the time of prediction, biased words make a significantly higher contribution to the models’ predictions, and models tend to assign predicted labels over-relying on the spurious correlation between words and labels. To mitigate models’ over-reliance on the shortcut (i.e. spurious correlation), we propose a training strategy Less-Learn-Shortcut (LLS): our strategy quantifies the biased degree of the biased examples and down-weights them accordingly. Experimental results on Question Matching, Natural Language Inference and Sentiment Analysis tasks show that LLS is a task-agnostic strategy and can improve the model performance on adversarial data while maintaining good performance on in-domain data.": 0.12805724143981934}, "What is the title of the research presented at the IJCAI2023 conference Main Track with the ID 5014?": {"IJCAI2023>>program>>Main Track>>5012>>authors>>authors_4>>Yidong Chen": 0.0878216028213501, "IJCAI2023>>program>>Main Track>>5195>>authors>>authors_3>>Thiago D. Simão": 0.08931988477706909, "IJCAI2023>>program>>Main Track>>4206>>authors>>authors_2>>Hendrik Molter": 0.09315246343612671, "IJCAI2023>>program>>Main Track>>4586>>authors>>authors_5>>Shinnosuke Takamichi": 0.09573674201965332, "IJCAI2023>>program>>Main Track>>4376>>authors>>authors_1>>Zhaiming Shen": 0.09637558460235596, "IJCAI2023>>program>>Main Track>>2178>>authors>>authors_3>>Ruben Solozabal Ochoa de Retana": 0.09647369384765625, "IJCAI2023>>program>>Main Track>>3863>>authors>>authors_7>>Hua Wu": 0.097048819065094, "IJCAI2023>>program>>Main Track>>5145>>authors>>authors_1>>Chenghao Liu": 0.09775930643081665, "IJCAI2023>>program>>Main Track>>200>>authors>>authors_4>>Yiran Chen": 0.09854364395141602, "IJCAI2023>>program>>Main Track>>2705>>authors>>authors_5>>Dangyang Chen": 0.09879910945892334, "IJCAI2023>>program>>Main Track>>4766>>authors>>authors_4>>Abdoulaye Banire Diallo": 0.09884011745452881, "IJCAI2023>>program>>Main Track>>5281>>authors>>authors_2>>Hao Chen": 0.09965187311172485, "IJCAI2023>>program>>Main Track>>1045>>authors>>authors_3>>Elisheva S. Shamash": 0.10058128833770752, "IJCAI2023>>program>>Main Track>>1621>>authors>>authors_3>>Jörg Hoffmann": 0.1005902886390686, "IJCAI2023>>program>>Main Track>>5164>>authors>>authors_4>>Jianyong Wang": 0.10129231214523315, "IJCAI2023>>program>>Main Track>>1798>>authors>>authors_4>>Chenxi Ma": 0.10162496566772461, "IJCAI2023>>program>>Journal Track>>J5941>>authors>>authors_2>>Andrew Searns": 0.1017422080039978, "IJCAI2023>>program>>Main Track>>2184>>authors>>authors_3>>Kun Wei": 0.10181158781051636, "IJCAI2023>>program>>Main Track>>3832>>authors>>authors_6>>Felix Ulrich-Oltean": 0.10196417570114136}, "Who are the authors of the research 'Adaptive Path-Memory Network for Temporal Knowledge Graph Reasoning' presented at IJCAI2023?": {"IJCAI2023>>program>>Main Track>>1045>>authors>>authors_1>>Takayuki Osogami": 0.13969582319259644, "IJCAI2023>>program>>Main Track>>5281>>authors>>authors_2>>Hao Chen": 0.14363551139831543, "IJCAI2023>>program>>Main Track>>4062>>authors>>authors_3>>Pascal Lenzner": 0.14400476217269897, "IJCAI2023>>program>>Survey Track>>SV5501>>authors>>authors_4>>Shreyasi Pathak": 0.1451229453086853, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5813>>authors>>authors_2>>Kathryn Mazaitis": 0.14645063877105713, "IJCAI2023>>program>>Main Track>>4454>>authors>>authors_4>>Kurt Mehlhorn": 0.14708733558654785, "IJCAI2023>>program>>Main Track>>1032>>authors>>authors_2>>Marie-Francine Moens": 0.14740592241287231, "IJCAI2023>>program>>Main Track>>3545>>authors>>authors_2>>Qiyu Kang": 0.14793717861175537, "IJCAI2023>>program>>Survey Track>>SV5593>>authors>>authors_1>>Chengyi Liu": 0.14800411462783813, "IJCAI2023>>program>>Main Track>>1476>>authors>>authors_1>>Ren-Jian Wang": 0.14802980422973633, "IJCAI2023>>program>>Main Track>>1379>>authors>>authors_1>>James Kotary": 0.14834856986999512, "IJCAI2023>>program>>Main Track>>87>>authors>>authors_5>>Xinbing Wang": 0.1484866738319397, "IJCAI2023>>calls>>Journal Track>>Accepted Papers List>>J5950>>authors>>authors_5>>Subbarao Kambhampati": 0.1487247347831726, "IJCAI2023>>program>>Survey Track>>SV5660>>authors>>authors_3>>Wayne Xin Zhao": 0.1487773060798645, "IJCAI2023>>program>>Main Track>>2759>>authors>>authors_1>>Alessandro Daniele": 0.14887946844100952, "IJCAI2023>>program>>Main Track>>1274>>authors>>authors_4>>Zhongwen Rao": 0.1492348313331604, "IJCAI2023>>program>>Main Track>>256>>authors>>authors_3>>Zhonghuang Wang": 0.14927035570144653, "IJCAI2023>>program>>Journal Track>>J5924>>authors>>authors_6>>Daniele Magazzeni": 0.1494770050048828, "IJCAI2023>>program>>Journal Track>>J5758>>authors>>authors_2>>Matthew E. Taylor": 0.14978480339050293}, "What is the abstract of the research 'Adaptive Path-Memory Network for Temporal Knowledge Graph Reasoning' presented at IJCAI2023?": {"IJCAI2023>>program>>Main Track>>5014>>abstract>>Temporal knowledge graph (TKG) reasoning aims to predict the future missing facts based on historical information and has gained increasing research interest recently. Lots of works have been made to model the historical structural and temporal characteristics for the reasoning task. Most existing works model the graph structure mainly depending on entity representation. However, the magnitude of TKG entities in real-world scenarios is considerable, and an increasing number of new entities will arise as time goes on. Therefore, we propose a novel architecture modeling with relation feature of TKG, namely aDAptivE path-MemOry Network (DaeMon), which adaptively models the temporal path information between query subject and each object candidate across history time. It models the historical information without depending on entity representation. Specifically, DaeMon uses path memory to record the temporal path information derived from path aggregation unit across timeline considering the memory passing strategy between adjacent timestamps. Extensive experiments conducted on four real-world TKG datasets demonstrate that our proposed model obtains substantial performance improvement and outperforms the state-of-the-art up to 4.8% absolute in MRR.": 0.07750111818313599, "IJCAI2023>>program>>Main Track>>5014>>title>>Adaptive Path-Memory Network for Temporal Knowledge Graph Reasoning": 0.09504848718643188, "IJCAI2023>>program>>Survey Track>>SV5569>>abstract>>Knowledge graph completion (KGC) predicts missing links and is crucial for real-life knowledge graphs, which widely suffer from incompleteness. \nKGC methods assume a knowledge graph is static, but that may lead to inaccurate prediction results because many facts in the knowledge graphs change over time. \nEmerging methods have recently shown improved prediction results by further incorporating the temporal validity of facts; namely, temporal knowledge graph completion (TKGC). \nWith this temporal information, TKGC methods explicitly learn the dynamic evolution of the knowledge graph that KGC methods fail to capture.\nIn this paper, for the first time, we comprehensively summarize the recent advances in TKGC research. \nFirst, we detail the background of TKGC, including the preliminary knowledge, benchmark datasets, and evaluation metrics. \nThen, we summarize existing TKGC methods based on how the temporal validity of facts is used to capture the temporal dynamics. \nFinally, we conclude the paper and present future research directions of TKGC.": 0.12805700302124023}, "What are the keywords associated with the research 'Adaptive Path-Memory Network for Temporal Knowledge Graph Reasoning' at IJCAI2023?": {"IJCAI2023>>program>>Main Track>>5014>>abstract>>Temporal knowledge graph (TKG) reasoning aims to predict the future missing facts based on historical information and has gained increasing research interest recently. Lots of works have been made to model the historical structural and temporal characteristics for the reasoning task. Most existing works model the graph structure mainly depending on entity representation. However, the magnitude of TKG entities in real-world scenarios is considerable, and an increasing number of new entities will arise as time goes on. Therefore, we propose a novel architecture modeling with relation feature of TKG, namely aDAptivE path-MemOry Network (DaeMon), which adaptively models the temporal path information between query subject and each object candidate across history time. It models the historical information without depending on entity representation. Specifically, DaeMon uses path memory to record the temporal path information derived from path aggregation unit across timeline considering the memory passing strategy between adjacent timestamps. Extensive experiments conducted on four real-world TKG datasets demonstrate that our proposed model obtains substantial performance improvement and outperforms the state-of-the-art up to 4.8% absolute in MRR.": 0.08458709716796875, "IJCAI2023>>program>>Main Track>>5014>>title>>Adaptive Path-Memory Network for Temporal Knowledge Graph Reasoning": 0.09245115518569946, "IJCAI2023>>program>>Main Track>>4783>>keywords>>keywords_2>>Knowledge Representation and Reasoning -> KRR: Preference modelling and preference-based reasoning": 0.12376385927200317, "IJCAI2023>>program>>Main Track>>880>>keywords>>keywords_1>>Knowledge Representation and Reasoning -> KRR: Qualitative, geometric, spatial, and temporal reasoning": 0.12406307458877563, "IJCAI2023>>program>>Doctoral Consortium Track>>DC5895>>keywords>>keywords_1>>Knowledge Representation and Reasoning -> KRR: Causality": 0.12517976760864258}, "What is the novel architecture that the paper 'Adaptive Path-Memory Network for Temporal Knowledge Graph Reasoning' is proposing?": {"IJCAI2023>>program>>Main Track>>5014>>abstract>>Temporal knowledge graph (TKG) reasoning aims to predict the future missing facts based on historical information and has gained increasing research interest recently. Lots of works have been made to model the historical structural and temporal characteristics for the reasoning task. Most existing works model the graph structure mainly depending on entity representation. However, the magnitude of TKG entities in real-world scenarios is considerable, and an increasing number of new entities will arise as time goes on. Therefore, we propose a novel architecture modeling with relation feature of TKG, namely aDAptivE path-MemOry Network (DaeMon), which adaptively models the temporal path information between query subject and each object candidate across history time. It models the historical information without depending on entity representation. Specifically, DaeMon uses path memory to record the temporal path information derived from path aggregation unit across timeline considering the memory passing strategy between adjacent timestamps. Extensive experiments conducted on four real-world TKG datasets demonstrate that our proposed model obtains substantial performance improvement and outperforms the state-of-the-art up to 4.8% absolute in MRR.": 0.10370242595672607, "IJCAI2023>>program>>Main Track>>5014>>title>>Adaptive Path-Memory Network for Temporal Knowledge Graph Reasoning": 0.11260825395584106, "IJCAI2023>>program>>Main Track>>1813>>title>>A Dual Semantic-Aware Recurrent Global-Adaptive Network for Vision-and-Language Navigation": 0.15509170293807983, "IJCAI2023>>program>>Demonstrations Track>>DM5729>>abstract>>Markov Logic Networks (MLN) are used for reasoning on uncertain and inconsistent temporal data. We proposed the TMLN (Temporal Markov Logic Network) which extends them with sorts/types, weights on rules and facts, and various temporal consistencies. The NeoMaPy framework integrates it as a knowledge graph based on conflict graphs which offers flexibility for reasoning with parametric Maximum A Posteriori (MAP) inferences, efficiency with an optimistic heuristic and interactive graph visualization for results explanation.": 0.15856510400772095}, "What is the title of the paper with ID 71 in the Main Track program?": {"IJCAI2023>>program>>Main Track>>3002>>authors>>authors_1>>Francesco Belardinelli": 0.1285097599029541, "IJCAI2023>>program>>Main Track>>3510>>authors>>authors_1>>Yongjuan Che": 0.13222330808639526, "IJCAI2023>>program>>Main Track>>4276>>authors>>authors_4>>Katrine Bjørner": 0.13261252641677856, "IJCAI2023>>program>>Main Track>>3497>>authors>>authors_2>>Jun Yuan": 0.13455522060394287, "IJCAI2023>>program>>Main Track>>2471>>authors>>authors_3>>Vladislav Ryzhikov": 0.13547319173812866, "IJCAI2023>>program>>Main Track>>4276>>authors>>authors_8>>Bettina Könighofer": 0.13582605123519897, "IJCAI2023>>program>>Main Track>>5012>>authors>>authors_4>>Yidong Chen": 0.13594281673431396, "IJCAI2023>>program>>Main Track>>1493>>authors>>authors_5>>Dong Yan": 0.1367080807685852, "IJCAI2023>>program>>Main Track>>3934>>authors>>authors_4>>Luc De Raedt": 0.13687032461166382, "IJCAI2023>>program>>Main Track>>4504>>authors>>authors_2>>Tobias Geibinger": 0.13714593648910522, "IJCAI2023>>program>>Main Track>>4853>>abstract>>We give polynomial time algorithms for escaping from high-dimensional saddle points under a moderate number of constraints. Given gradient access to a smooth function $f \\colon \\mathbb R^d \\to \\mathbb R$ we show that (noisy) gradient descent methods can escape from saddle points under a logarithmic number of inequality constraints. This constitutes progress (without reliance on NP-oracles or altering the definitions to only account for certain constraints) on the main open question of the breakthrough work of [Ge et al.`15] who showed an analogous result for unconstrained and equality-constrained problems. Our results hold for both regular and stochastic gradient descent.": 0.13756012916564941, "IJCAI2023>>program>>Main Track>>5323>>authors>>authors_1>>Sruthi Gorantla": 0.13875216245651245, "IJCAI2023>>program>>Main Track>>4401>>authors>>authors_4>>Johannes Oetsch": 0.13885706663131714, "IJCAI2023>>program>>Main Track>>863>>authors>>authors_8>>Jie Chen": 0.1393871307373047, "IJCAI2023>>program>>Main Track>>2230>>authors>>authors_3>>Sheila McIlraith": 0.13958263397216797}, "Who are the authors of the paper with ID 71?": {"IJCAI2023>>program>>Main Track>>4454>>authors>>authors_4>>Kurt Mehlhorn": 0.15495848655700684, "IJCAI2023>>program>>Main Track>>1626>>authors>>authors_1>>Peizheng Li": 0.15871042013168335, "IJCAI2023>>calls>>Main Track>>Accepted Papers List>>141>>authors>>authors_3>>Lun Du": 0.16182327270507812, "IJCAI2023>>program>>Main Track>>1856>>authors>>authors_2>>Kate Larson": 0.16306734085083008, "IJCAI2023>>program>>Main Track>>1834>>authors>>authors_3>>Christel Baier": 0.16533255577087402, "IJCAI2023>>program>>Main Track>>398>>authors>>authors_3>>Zhongang Qi": 0.1654711365699768, "IJCAI2023>>program>>Journal Track>>J5924>>authors>>authors_6>>Daniele Magazzeni": 0.165766179561615, "IJCAI2023>>program>>Journal Track>>J5919>>authors>>authors_3>>Pascal Van Hentenryck": 0.16609066724777222, "IJCAI2023>>program>>Main Track>>3832>>authors>>authors_4>>Ian Miguel": 0.1664752960205078, "IJCAI2023>>program>>Best Papers from Sister Conferences Track>>SC23>>authors>>authors_4>>Zhitang Chen": 0.1665666699409485, "IJCAI2023>>program>>Survey Track>>SV5648>>authors>>authors_5>>Pradeep K. Murukannaiah": 0.16715675592422485, "IJCAI2023>>program>>Main Track>>1384>>authors>>authors_1>>Xixuan Hao": 0.1684076189994812, "IJCAI2023>>program>>Main Track>>1379>>authors>>authors_1>>James Kotary": 0.16851717233657837, "IJCAI2023>>program>>Journal Track>>J5758>>authors>>authors_2>>Matthew E. Taylor": 0.1685241460800171, "IJCAI2023>>program>>Main Track>>4078>>authors>>authors_3>>Stefan Szeider": 0.16855835914611816, "IJCAI2023>>program>>Main Track>>1834>>authors>>authors_2>>Simon Jantsch": 0.16876786947250366, "IJCAI2023>>program>>Main Track>>1274>>authors>>authors_1>>Yiduo Li": 0.1688966155052185, "IJCAI2023>>program>>Survey Track>>SV5660>>authors>>authors_2>>Kun Zhou": 0.1692931056022644, "IJCAI2023>>program>>Main Track>>920>>authors>>authors_1>>Wenbo Li": 0.16972112655639648}, "What is the main idea of the paper titled 'Teaching What You Should Teach: A Data-Based Distillation Method'?": {"IJCAI2023>>program>>Main Track>>71>>abstract>>In real teaching scenarios, an excellent teacher always teaches what he (or she) is good at but the student is not. This gives the student the best assistance in making up for his (or her) weaknesses and becoming a good one overall. Enlightened by this, we introduce the \"Teaching what you Should Teach\" strategy into a knowledge distillation framework, and propose a data-based distillation method named \"TST\" that searches for desirable augmented samples to assist in distilling more efficiently and rationally. To be specific, we design a neural network-based data augmentation module with priori bias to find out what meets the teacher’s strengths but the student’s weaknesses, by learning magnitudes and probabilities to generate suitable data samples. By training the data augmentation module and the generalized distillation paradigm alternately, a student model is learned with excellent generalization ability. To verify the effectiveness of our method, we conducted extensive comparative experiments on object recognition, detection, and segmentation tasks. The results on the CIFAR-100, ImageNet-1k, MS-COCO, and Cityscapes datasets demonstrate that our method achieves state-of-the-art performance on almost all teacher-student pairs. Furthermore, we conduct visualization studies to explore what magnitudes and probabilities are needed for the distillation process.": 0.0950474739074707, "IJCAI2023>>program>>Main Track>>71>>title>>Teaching What You Should Teach: A Data-Based Distillation Method": 0.1396307349205017, "IJCAI2023>>program>>Survey Track>>SV5487>>abstract>>Dataset distillation is attracting more attention in machine learning as training sets continue to grow and the cost of training state-of-the-art models becomes increasingly high. By synthesizing datasets with high information density, dataset distillation offers a range of potential applications, including support for continual learning, neural architecture search, and privacy protection. Despite recent advances, we lack a holistic understanding of the approaches and applications. Our survey aims to bridge this gap by first proposing a taxonomy of dataset distillation, characterizing existing approaches, and then systematically reviewing the data modalities, and related applications. In addition, we summarize the challenges and discuss future directions for this field of research.": 0.14591342210769653, "IJCAI2023>>program>>Main Track>>299>>title>>Model Conversion via Differentially Private Data-Free Distillation": 0.16420680284500122}, "What are the main application areas of the method proposed in the paper 'Teaching What You Should Teach: A Data-Based Distillation Method'?": {"IJCAI2023>>program>>Main Track>>71>>abstract>>In real teaching scenarios, an excellent teacher always teaches what he (or she) is good at but the student is not. This gives the student the best assistance in making up for his (or her) weaknesses and becoming a good one overall. Enlightened by this, we introduce the \"Teaching what you Should Teach\" strategy into a knowledge distillation framework, and propose a data-based distillation method named \"TST\" that searches for desirable augmented samples to assist in distilling more efficiently and rationally. To be specific, we design a neural network-based data augmentation module with priori bias to find out what meets the teacher’s strengths but the student’s weaknesses, by learning magnitudes and probabilities to generate suitable data samples. By training the data augmentation module and the generalized distillation paradigm alternately, a student model is learned with excellent generalization ability. To verify the effectiveness of our method, we conducted extensive comparative experiments on object recognition, detection, and segmentation tasks. The results on the CIFAR-100, ImageNet-1k, MS-COCO, and Cityscapes datasets demonstrate that our method achieves state-of-the-art performance on almost all teacher-student pairs. Furthermore, we conduct visualization studies to explore what magnitudes and probabilities are needed for the distillation process.": 0.09388679265975952, "IJCAI2023>>program>>Survey Track>>SV5487>>abstract>>Dataset distillation is attracting more attention in machine learning as training sets continue to grow and the cost of training state-of-the-art models becomes increasingly high. By synthesizing datasets with high information density, dataset distillation offers a range of potential applications, including support for continual learning, neural architecture search, and privacy protection. Despite recent advances, we lack a holistic understanding of the approaches and applications. Our survey aims to bridge this gap by first proposing a taxonomy of dataset distillation, characterizing existing approaches, and then systematically reviewing the data modalities, and related applications. In addition, we summarize the challenges and discuss future directions for this field of research.": 0.1328030228614807, "IJCAI2023>>program>>Main Track>>71>>title>>Teaching What You Should Teach: A Data-Based Distillation Method": 0.14611148834228516, "IJCAI2023>>program>>Survey Track>>SV5487>>title>>A Survey on Dataset Distillation: Approaches, Applications and Future Directions": 0.14665651321411133}, "What kind of evaluation was conducted to verify the effectiveness of the method proposed in the paper 'Teaching What You Should Teach: A Data-Based Distillation Method'?": {"IJCAI2023>>program>>Main Track>>71>>abstract>>In real teaching scenarios, an excellent teacher always teaches what he (or she) is good at but the student is not. This gives the student the best assistance in making up for his (or her) weaknesses and becoming a good one overall. Enlightened by this, we introduce the \"Teaching what you Should Teach\" strategy into a knowledge distillation framework, and propose a data-based distillation method named \"TST\" that searches for desirable augmented samples to assist in distilling more efficiently and rationally. To be specific, we design a neural network-based data augmentation module with priori bias to find out what meets the teacher’s strengths but the student’s weaknesses, by learning magnitudes and probabilities to generate suitable data samples. By training the data augmentation module and the generalized distillation paradigm alternately, a student model is learned with excellent generalization ability. To verify the effectiveness of our method, we conducted extensive comparative experiments on object recognition, detection, and segmentation tasks. The results on the CIFAR-100, ImageNet-1k, MS-COCO, and Cityscapes datasets demonstrate that our method achieves state-of-the-art performance on almost all teacher-student pairs. Furthermore, we conduct visualization studies to explore what magnitudes and probabilities are needed for the distillation process.": 0.11281532049179077, "IJCAI2023>>program>>Survey Track>>SV5487>>abstract>>Dataset distillation is attracting more attention in machine learning as training sets continue to grow and the cost of training state-of-the-art models becomes increasingly high. By synthesizing datasets with high information density, dataset distillation offers a range of potential applications, including support for continual learning, neural architecture search, and privacy protection. Despite recent advances, we lack a holistic understanding of the approaches and applications. Our survey aims to bridge this gap by first proposing a taxonomy of dataset distillation, characterizing existing approaches, and then systematically reviewing the data modalities, and related applications. In addition, we summarize the challenges and discuss future directions for this field of research.": 0.16189217567443848, "IJCAI2023>>program>>Main Track>>71>>title>>Teaching What You Should Teach: A Data-Based Distillation Method": 0.1707172393798828, "IJCAI2023>>program>>Survey Track>>SV5487>>title>>A Survey on Dataset Distillation: Approaches, Applications and Future Directions": 0.1729986071586609}, "Has the method proposed in the paper 'Teaching What You Should Teach: A Data-Based Distillation Method' been tested on any real datasets?": {"IJCAI2023>>program>>Main Track>>71>>abstract>>In real teaching scenarios, an excellent teacher always teaches what he (or she) is good at but the student is not. This gives the student the best assistance in making up for his (or her) weaknesses and becoming a good one overall. Enlightened by this, we introduce the \"Teaching what you Should Teach\" strategy into a knowledge distillation framework, and propose a data-based distillation method named \"TST\" that searches for desirable augmented samples to assist in distilling more efficiently and rationally. To be specific, we design a neural network-based data augmentation module with priori bias to find out what meets the teacher’s strengths but the student’s weaknesses, by learning magnitudes and probabilities to generate suitable data samples. By training the data augmentation module and the generalized distillation paradigm alternately, a student model is learned with excellent generalization ability. To verify the effectiveness of our method, we conducted extensive comparative experiments on object recognition, detection, and segmentation tasks. The results on the CIFAR-100, ImageNet-1k, MS-COCO, and Cityscapes datasets demonstrate that our method achieves state-of-the-art performance on almost all teacher-student pairs. Furthermore, we conduct visualization studies to explore what magnitudes and probabilities are needed for the distillation process.": 0.10385417938232422, "IJCAI2023>>program>>Survey Track>>SV5487>>abstract>>Dataset distillation is attracting more attention in machine learning as training sets continue to grow and the cost of training state-of-the-art models becomes increasingly high. By synthesizing datasets with high information density, dataset distillation offers a range of potential applications, including support for continual learning, neural architecture search, and privacy protection. Despite recent advances, we lack a holistic understanding of the approaches and applications. Our survey aims to bridge this gap by first proposing a taxonomy of dataset distillation, characterizing existing approaches, and then systematically reviewing the data modalities, and related applications. In addition, we summarize the challenges and discuss future directions for this field of research.": 0.14171457290649414, "IJCAI2023>>program>>Survey Track>>SV5487>>title>>A Survey on Dataset Distillation: Approaches, Applications and Future Directions": 0.15966272354125977, "IJCAI2023>>program>>Main Track>>71>>title>>Teaching What You Should Teach: A Data-Based Distillation Method": 0.1675550937652588}, "What does the TST stand for in the proposed method in the paper?": {"IJCAI2023>>program>>Main Track>>71>>abstract>>In real teaching scenarios, an excellent teacher always teaches what he (or she) is good at but the student is not. This gives the student the best assistance in making up for his (or her) weaknesses and becoming a good one overall. Enlightened by this, we introduce the \"Teaching what you Should Teach\" strategy into a knowledge distillation framework, and propose a data-based distillation method named \"TST\" that searches for desirable augmented samples to assist in distilling more efficiently and rationally. To be specific, we design a neural network-based data augmentation module with priori bias to find out what meets the teacher’s strengths but the student’s weaknesses, by learning magnitudes and probabilities to generate suitable data samples. By training the data augmentation module and the generalized distillation paradigm alternately, a student model is learned with excellent generalization ability. To verify the effectiveness of our method, we conducted extensive comparative experiments on object recognition, detection, and segmentation tasks. The results on the CIFAR-100, ImageNet-1k, MS-COCO, and Cityscapes datasets demonstrate that our method achieves state-of-the-art performance on almost all teacher-student pairs. Furthermore, we conduct visualization studies to explore what magnitudes and probabilities are needed for the distillation process.": 0.18147480487823486}, "What kind of tasks does the method 'Teaching What You Should Teach: A Data-Based Distillation Method' apply to?": {"IJCAI2023>>program>>Main Track>>71>>abstract>>In real teaching scenarios, an excellent teacher always teaches what he (or she) is good at but the student is not. This gives the student the best assistance in making up for his (or her) weaknesses and becoming a good one overall. Enlightened by this, we introduce the \"Teaching what you Should Teach\" strategy into a knowledge distillation framework, and propose a data-based distillation method named \"TST\" that searches for desirable augmented samples to assist in distilling more efficiently and rationally. To be specific, we design a neural network-based data augmentation module with priori bias to find out what meets the teacher’s strengths but the student’s weaknesses, by learning magnitudes and probabilities to generate suitable data samples. By training the data augmentation module and the generalized distillation paradigm alternately, a student model is learned with excellent generalization ability. To verify the effectiveness of our method, we conducted extensive comparative experiments on object recognition, detection, and segmentation tasks. The results on the CIFAR-100, ImageNet-1k, MS-COCO, and Cityscapes datasets demonstrate that our method achieves state-of-the-art performance on almost all teacher-student pairs. Furthermore, we conduct visualization studies to explore what magnitudes and probabilities are needed for the distillation process.": 0.10033953189849854, "IJCAI2023>>program>>Survey Track>>SV5487>>abstract>>Dataset distillation is attracting more attention in machine learning as training sets continue to grow and the cost of training state-of-the-art models becomes increasingly high. By synthesizing datasets with high information density, dataset distillation offers a range of potential applications, including support for continual learning, neural architecture search, and privacy protection. Despite recent advances, we lack a holistic understanding of the approaches and applications. Our survey aims to bridge this gap by first proposing a taxonomy of dataset distillation, characterizing existing approaches, and then systematically reviewing the data modalities, and related applications. In addition, we summarize the challenges and discuss future directions for this field of research.": 0.1515311598777771, "IJCAI2023>>program>>Main Track>>71>>title>>Teaching What You Should Teach: A Data-Based Distillation Method": 0.15810251235961914, "IJCAI2023>>program>>Survey Track>>SV5487>>title>>A Survey on Dataset Distillation: Approaches, Applications and Future Directions": 0.16714459657669067}, "What are the primary concerns of the paper with ID 71 in the Main Track program?": {"IJCAI2023>>program>>Main Track>>4504>>authors>>authors_2>>Tobias Geibinger": 0.13846051692962646, "IJCAI2023>>program>>Main Track>>4276>>authors>>authors_4>>Katrine Bjørner": 0.14201313257217407, "IJCAI2023>>program>>Main Track>>4401>>authors>>authors_4>>Johannes Oetsch": 0.142561137676239, "IJCAI2023>>program>>Main Track>>3002>>authors>>authors_1>>Francesco Belardinelli": 0.1462690830230713, "IJCAI2023>>program>>Main Track>>3934>>authors>>authors_4>>Luc De Raedt": 0.14641600847244263, "IJCAI2023>>program>>Main Track>>3497>>authors>>authors_2>>Jun Yuan": 0.14650535583496094, "IJCAI2023>>program>>Main Track>>2230>>authors>>authors_3>>Sheila McIlraith": 0.14799225330352783, "IJCAI2023>>program>>Main Track>>2276>>authors>>authors_2>>Di Jin": 0.148626446723938, "IJCAI2023>>program>>Main Track>>1219>>authors>>authors_4>>Bo Du": 0.14900803565979004, "IJCAI2023>>program>>Main Track>>2077>>keywords>>keywords_1>>Computer Vision -> CV: Segmentation": 0.1492016315460205, "IJCAI2023>>program>>Main Track>>863>>authors>>authors_8>>Jie Chen": 0.14922255277633667, "IJCAI2023>>program>>Main Track>>4438>>authors>>authors_1>>Debarun Bhattacharjya": 0.1494826078414917, "IJCAI2023>>program>>Main Track>>2471>>authors>>authors_3>>Vladislav Ryzhikov": 0.14966720342636108, "IJCAI2023>>program>>Main Track>>4255>>keywords>>keywords_1>>Machine Learning -> ML: Robustness": 0.15056800842285156, "IJCAI2023>>program>>Main Track>>3040>>authors>>authors_3>>Zijie Song": 0.1506977677345276, "IJCAI2023>>program>>Main Track>>774>>authors>>authors_3>>Yabiao Wang": 0.1507803201675415, "IJCAI2023>>program>>Main Track>>3510>>authors>>authors_1>>Yongjuan Che": 0.1507907509803772, "IJCAI2023>>program>>Main Track>>1698>>authors>>authors_1>>Yi Gao": 0.15151703357696533, "IJCAI2023>>program>>Main Track>>4276>>authors>>authors_8>>Bettina Könighofer": 0.15158742666244507}, "Where does the 'Teaching What You Should Teach: A Data-Based Distillation Method' demonstrate state-of-the-art performance?": {"IJCAI2023>>program>>Main Track>>71>>abstract>>In real teaching scenarios, an excellent teacher always teaches what he (or she) is good at but the student is not. This gives the student the best assistance in making up for his (or her) weaknesses and becoming a good one overall. Enlightened by this, we introduce the \"Teaching what you Should Teach\" strategy into a knowledge distillation framework, and propose a data-based distillation method named \"TST\" that searches for desirable augmented samples to assist in distilling more efficiently and rationally. To be specific, we design a neural network-based data augmentation module with priori bias to find out what meets the teacher’s strengths but the student’s weaknesses, by learning magnitudes and probabilities to generate suitable data samples. By training the data augmentation module and the generalized distillation paradigm alternately, a student model is learned with excellent generalization ability. To verify the effectiveness of our method, we conducted extensive comparative experiments on object recognition, detection, and segmentation tasks. The results on the CIFAR-100, ImageNet-1k, MS-COCO, and Cityscapes datasets demonstrate that our method achieves state-of-the-art performance on almost all teacher-student pairs. Furthermore, we conduct visualization studies to explore what magnitudes and probabilities are needed for the distillation process.": 0.10703867673873901, "IJCAI2023>>program>>Main Track>>71>>title>>Teaching What You Should Teach: A Data-Based Distillation Method": 0.1514829397201538, "IJCAI2023>>program>>Survey Track>>SV5487>>abstract>>Dataset distillation is attracting more attention in machine learning as training sets continue to grow and the cost of training state-of-the-art models becomes increasingly high. By synthesizing datasets with high information density, dataset distillation offers a range of potential applications, including support for continual learning, neural architecture search, and privacy protection. Despite recent advances, we lack a holistic understanding of the approaches and applications. Our survey aims to bridge this gap by first proposing a taxonomy of dataset distillation, characterizing existing approaches, and then systematically reviewing the data modalities, and related applications. In addition, we summarize the challenges and discuss future directions for this field of research.": 0.15459805727005005, "IJCAI2023>>program>>Survey Track>>SV5487>>title>>A Survey on Dataset Distillation: Approaches, Applications and Future Directions": 0.1708216667175293}, "What is the title of the paper in the Special Track on AI for Good in the IJCAI2023 program?": {"IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5683>>authors>>authors_1>>Dixin Luo": 0.06787127256393433, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5683>>authors>>authors_2>>Haoran Cheng": 0.06840682029724121, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5784>>authors>>authors_2>>Philippe Vismara": 0.06982380151748657, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5757>>authors>>authors_1>>Sujan Dutta": 0.07562124729156494, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5788>>authors>>authors_3>>Marisa Vasconcelos": 0.07600593566894531, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5784>>authors>>authors_4>>Stéphane de Tourdonnet": 0.07666945457458496, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG3664>>authors>>authors_6>>Guosheng Yin": 0.07906574010848999, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5782>>authors>>authors_3>>Amit Zac": 0.08026576042175293, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5755>>authors>>authors_3>>Changjun Jiang": 0.08044695854187012, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5431>>keywords>>keywords_2>>AI for Good -> Multidisciplinary Topics and Applications": 0.08083951473236084, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5814>>authors>>authors_3>>Nalini Saligram": 0.08093273639678955, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5458>>authors>>authors_2>>Fan Li": 0.08105933666229248, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5422>>authors>>authors_1>>Junyoung Byun": 0.08129024505615234, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG3664>>authors>>authors_5>>Xiang Zhang": 0.08132022619247437, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5888>>authors>>authors_4>>Ganesh Ramakrishnan": 0.08163249492645264}, "Who are the authors of the paper 'Optimizing Crop Management with Reinforcement Learning and Imitation Learning'?": {"IJCAI2023>>program>>Main Track>>4454>>authors>>authors_4>>Kurt Mehlhorn": 0.17842155694961548, "IJCAI2023>>program>>Survey Track>>SV5660>>authors>>authors_2>>Kun Zhou": 0.17878305912017822, "IJCAI2023>>program>>Demonstrations Track>>DM5703>>authors>>authors_5>>Manuel Gil Pérez": 0.17957079410552979, "IJCAI2023>>program>>Special Track on AI, the Arts and Creativity>>ARTS1743>>authors>>authors_4>>Qifeng Liu": 0.18093860149383545, "IJCAI2023>>program>>Main Track>>2929>>authors>>authors_5>>Eric Rice": 0.1811985969543457, "IJCAI2023>>program>>Main Track>>1834>>authors>>authors_3>>Christel Baier": 0.18145865201950073, "IJCAI2023>>program>>Main Track>>2759>>authors>>authors_3>>Sagar Malhotra": 0.1817682385444641, "IJCAI2023>>program>>Main Track>>1856>>authors>>authors_1>>David Radke": 0.182009756565094, "IJCAI2023>>program>>Main Track>>4062>>authors>>authors_3>>Pascal Lenzner": 0.1821514368057251, "IJCAI2023>>program>>Demonstrations Track>>DM5731>>authors>>authors_4>>Prasenjit Mitra": 0.1823946237564087, "IJCAI2023>>program>>Main Track>>1379>>authors>>authors_1>>James Kotary": 0.18315017223358154, "IJCAI2023>>program>>Main Track>>1626>>authors>>authors_6>>Juergen Gall": 0.18339377641677856, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5431>>authors>>authors_1>>Qinqing Liu": 0.18343746662139893, "IJCAI2023>>program>>Main Track>>3073>>authors>>authors_3>>Eklavya Sharma": 0.18384385108947754, "IJCAI2023>>program>>Main Track>>1384>>authors>>authors_1>>Xixuan Hao": 0.18412643671035767, "IJCAI2023>>program>>Demonstrations Track>>DM5742>>authors>>authors_3>>Biplav Srivastava": 0.1842091679573059, "IJCAI2023>>program>>Main Track>>1796>>authors>>authors_1>>Yalin Yu": 0.1842285394668579, "IJCAI2023>>program>>Main Track>>1626>>authors>>authors_1>>Peizheng Li": 0.18434464931488037, "IJCAI2023>>program>>Journal Track>>J5758>>authors>>authors_2>>Matthew E. Taylor": 0.18443477153778076}, "What is the main focus of the paper presented in the Special Track on AI for Good?": {"IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5784>>authors>>authors_2>>Philippe Vismara": 0.09288167953491211, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5683>>authors>>authors_2>>Haoran Cheng": 0.10530674457550049, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5795>>authors>>authors_2>>Yunhe Feng": 0.10726481676101685, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5431>>keywords>>keywords_2>>AI for Good -> Multidisciplinary Topics and Applications": 0.10837113857269287, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5840>>keywords>>keywords_2>>AI for Good -> AI Ethics, Trust, Fairness": 0.10842627286911011, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5808>>authors>>authors_5>>Mennatullah Siam": 0.10946506261825562, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5757>>authors>>authors_1>>Sujan Dutta": 0.11043506860733032, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5458>>authors>>authors_2>>Fan Li": 0.11049818992614746, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5814>>authors>>authors_7>>Sandhya Ramalingam": 0.11060762405395508, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5800>>keywords>>keywords_3>>AI for Good -> Search": 0.11091816425323486, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5755>>authors>>authors_3>>Changjun Jiang": 0.11171919107437134, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5828>>title>>Limited Resource Allocation in a Non-Markovian World: The Case of Maternal and Child Healthcare": 0.11187601089477539, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5683>>authors>>authors_1>>Dixin Luo": 0.11275875568389893, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5813>>authors>>authors_1>>Ananya Joshi": 0.1131790280342102, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5808>>keywords>>keywords_1>>AI for Good -> Computer Vision": 0.11325860023498535}, "What simulation tool does the paper 'Optimizing Crop Management with Reinforcement Learning and Imitation Learning' use?": {"IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5611>>abstract>>Crop management has a significant impact on crop yield, economic profit, and the environment. Although management guidelines exist, finding the optimal management practices is challenging. Previous work used reinforcement learning (RL) and crop simulators to solve the problem, but the trained policies either have limited performance or are not deployable in the real world. In this paper, we present an intelligent crop management system that optimizes nitrogen fertilization and irrigation simultaneously via RL, imitation learning (IL), and crop simulations using the Decision Support System for Agrotechnology Transfer (DSSAT). We first use deep RL, in particular, deep Q-network, to train management policies that require a large number of state variables from the simulator as observations (denoted as full observation). We then invoke IL to train management policies that only need a few state variables that can be easily obtained or measured in the real world (denoted as partial observation) by mimicking the actions of the RL policies trained under full observation. Simulation experiments using the maize crop in Florida (US) and Zaragoza (Spain) demonstrate that the trained policies from both RL and IL techniques achieved more than 45\\% improvement in economic profit while causing less environmental impact compared with a baseline method. Most importantly, the IL-trained management policies are directly deployable in the real world as they use readily available information.": 0.11508715152740479, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5611>>title>>Optimizing Crop Management with Reinforcement Learning and Imitation Learning": 0.12519186735153198}, "What are the key techniques used in the paper 'Optimizing Crop Management with Reinforcement Learning and Imitation Learning'?": {"IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5611>>title>>Optimizing Crop Management with Reinforcement Learning and Imitation Learning": 0.1162264347076416, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5611>>abstract>>Crop management has a significant impact on crop yield, economic profit, and the environment. Although management guidelines exist, finding the optimal management practices is challenging. Previous work used reinforcement learning (RL) and crop simulators to solve the problem, but the trained policies either have limited performance or are not deployable in the real world. In this paper, we present an intelligent crop management system that optimizes nitrogen fertilization and irrigation simultaneously via RL, imitation learning (IL), and crop simulations using the Decision Support System for Agrotechnology Transfer (DSSAT). We first use deep RL, in particular, deep Q-network, to train management policies that require a large number of state variables from the simulator as observations (denoted as full observation). We then invoke IL to train management policies that only need a few state variables that can be easily obtained or measured in the real world (denoted as partial observation) by mimicking the actions of the RL policies trained under full observation. Simulation experiments using the maize crop in Florida (US) and Zaragoza (Spain) demonstrate that the trained policies from both RL and IL techniques achieved more than 45\\% improvement in economic profit while causing less environmental impact compared with a baseline method. Most importantly, the IL-trained management policies are directly deployable in the real world as they use readily available information.": 0.12525826692581177}, "What is the primary aim of the system presented in the 'Optimizing Crop Management with Reinforcement Learning and Imitation Learning' paper?": {"IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5611>>title>>Optimizing Crop Management with Reinforcement Learning and Imitation Learning": 0.11955249309539795, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5611>>abstract>>Crop management has a significant impact on crop yield, economic profit, and the environment. Although management guidelines exist, finding the optimal management practices is challenging. Previous work used reinforcement learning (RL) and crop simulators to solve the problem, but the trained policies either have limited performance or are not deployable in the real world. In this paper, we present an intelligent crop management system that optimizes nitrogen fertilization and irrigation simultaneously via RL, imitation learning (IL), and crop simulations using the Decision Support System for Agrotechnology Transfer (DSSAT). We first use deep RL, in particular, deep Q-network, to train management policies that require a large number of state variables from the simulator as observations (denoted as full observation). We then invoke IL to train management policies that only need a few state variables that can be easily obtained or measured in the real world (denoted as partial observation) by mimicking the actions of the RL policies trained under full observation. Simulation experiments using the maize crop in Florida (US) and Zaragoza (Spain) demonstrate that the trained policies from both RL and IL techniques achieved more than 45\\% improvement in economic profit while causing less environmental impact compared with a baseline method. Most importantly, the IL-trained management policies are directly deployable in the real world as they use readily available information.": 0.11968624591827393, "IJCAI2023>>program>>Main Track>>2873>>title>>CROP: Towards Distributional-Shift Robust Reinforcement Learning Using Compact Reshaped Observation Processing": 0.14195150136947632}, "What are the main benefits of the AI system presented in 'Optimizing Crop Management with Reinforcement Learning and Imitation Learning'?": {"IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5611>>abstract>>Crop management has a significant impact on crop yield, economic profit, and the environment. Although management guidelines exist, finding the optimal management practices is challenging. Previous work used reinforcement learning (RL) and crop simulators to solve the problem, but the trained policies either have limited performance or are not deployable in the real world. In this paper, we present an intelligent crop management system that optimizes nitrogen fertilization and irrigation simultaneously via RL, imitation learning (IL), and crop simulations using the Decision Support System for Agrotechnology Transfer (DSSAT). We first use deep RL, in particular, deep Q-network, to train management policies that require a large number of state variables from the simulator as observations (denoted as full observation). We then invoke IL to train management policies that only need a few state variables that can be easily obtained or measured in the real world (denoted as partial observation) by mimicking the actions of the RL policies trained under full observation. Simulation experiments using the maize crop in Florida (US) and Zaragoza (Spain) demonstrate that the trained policies from both RL and IL techniques achieved more than 45\\% improvement in economic profit while causing less environmental impact compared with a baseline method. Most importantly, the IL-trained management policies are directly deployable in the real world as they use readily available information.": 0.09667420387268066, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5611>>title>>Optimizing Crop Management with Reinforcement Learning and Imitation Learning": 0.10592347383499146, "IJCAI2023>>program>>Main Track>>2873>>title>>CROP: Towards Distributional-Shift Robust Reinforcement Learning Using Compact Reshaped Observation Processing": 0.14791101217269897}, "What advantage does the system developed in the paper 'Optimizing Crop Management with Reinforcement Learning and Imitation Learning' have for real-world applications?": {"IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5611>>abstract>>Crop management has a significant impact on crop yield, economic profit, and the environment. Although management guidelines exist, finding the optimal management practices is challenging. Previous work used reinforcement learning (RL) and crop simulators to solve the problem, but the trained policies either have limited performance or are not deployable in the real world. In this paper, we present an intelligent crop management system that optimizes nitrogen fertilization and irrigation simultaneously via RL, imitation learning (IL), and crop simulations using the Decision Support System for Agrotechnology Transfer (DSSAT). We first use deep RL, in particular, deep Q-network, to train management policies that require a large number of state variables from the simulator as observations (denoted as full observation). We then invoke IL to train management policies that only need a few state variables that can be easily obtained or measured in the real world (denoted as partial observation) by mimicking the actions of the RL policies trained under full observation. Simulation experiments using the maize crop in Florida (US) and Zaragoza (Spain) demonstrate that the trained policies from both RL and IL techniques achieved more than 45\\% improvement in economic profit while causing less environmental impact compared with a baseline method. Most importantly, the IL-trained management policies are directly deployable in the real world as they use readily available information.": 0.10533308982849121, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5611>>title>>Optimizing Crop Management with Reinforcement Learning and Imitation Learning": 0.12024319171905518}, "Which crops and regions were used for the experiments in 'Optimizing Crop Management with Reinforcement Learning and Imitation Learning'?": {"IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5611>>abstract>>Crop management has a significant impact on crop yield, economic profit, and the environment. Although management guidelines exist, finding the optimal management practices is challenging. Previous work used reinforcement learning (RL) and crop simulators to solve the problem, but the trained policies either have limited performance or are not deployable in the real world. In this paper, we present an intelligent crop management system that optimizes nitrogen fertilization and irrigation simultaneously via RL, imitation learning (IL), and crop simulations using the Decision Support System for Agrotechnology Transfer (DSSAT). We first use deep RL, in particular, deep Q-network, to train management policies that require a large number of state variables from the simulator as observations (denoted as full observation). We then invoke IL to train management policies that only need a few state variables that can be easily obtained or measured in the real world (denoted as partial observation) by mimicking the actions of the RL policies trained under full observation. Simulation experiments using the maize crop in Florida (US) and Zaragoza (Spain) demonstrate that the trained policies from both RL and IL techniques achieved more than 45\\% improvement in economic profit while causing less environmental impact compared with a baseline method. Most importantly, the IL-trained management policies are directly deployable in the real world as they use readily available information.": 0.11163300275802612, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5611>>title>>Optimizing Crop Management with Reinforcement Learning and Imitation Learning": 0.11243855953216553, "IJCAI2023>>program>>Main Track>>2873>>title>>CROP: Towards Distributional-Shift Robust Reinforcement Learning Using Compact Reshaped Observation Processing": 0.1383422613143921}, "What are the keywords for the paper 'Optimizing Crop Management with Reinforcement Learning and Imitation Learning'?": {"IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5611>>title>>Optimizing Crop Management with Reinforcement Learning and Imitation Learning": 0.10816842317581177, "IJCAI2023>>program>>Special Track on AI for Good>>AI4SG5611>>abstract>>Crop management has a significant impact on crop yield, economic profit, and the environment. Although management guidelines exist, finding the optimal management practices is challenging. Previous work used reinforcement learning (RL) and crop simulators to solve the problem, but the trained policies either have limited performance or are not deployable in the real world. In this paper, we present an intelligent crop management system that optimizes nitrogen fertilization and irrigation simultaneously via RL, imitation learning (IL), and crop simulations using the Decision Support System for Agrotechnology Transfer (DSSAT). We first use deep RL, in particular, deep Q-network, to train management policies that require a large number of state variables from the simulator as observations (denoted as full observation). We then invoke IL to train management policies that only need a few state variables that can be easily obtained or measured in the real world (denoted as partial observation) by mimicking the actions of the RL policies trained under full observation. Simulation experiments using the maize crop in Florida (US) and Zaragoza (Spain) demonstrate that the trained policies from both RL and IL techniques achieved more than 45\\% improvement in economic profit while causing less environmental impact compared with a baseline method. Most importantly, the IL-trained management policies are directly deployable in the real world as they use readily available information.": 0.12516051530838013, "IJCAI2023>>program>>Main Track>>2873>>title>>CROP: Towards Distributional-Shift Robust Reinforcement Learning Using Compact Reshaped Observation Processing": 0.13608598709106445}, "What is the title of the paper with ID 621 in the Main Track of IJCAI2023?": {"IJCAI2023>>program>>Main Track>>5195>>authors>>authors_3>>Thiago D. Simão": 0.09089767932891846, "IJCAI2023>>program>>Main Track>>2178>>authors>>authors_3>>Ruben Solozabal Ochoa de Retana": 0.09431082010269165, "IJCAI2023>>program>>Main Track>>1493>>authors>>authors_5>>Dong Yan": 0.0957987904548645, "IJCAI2023>>program>>Main Track>>5012>>authors>>authors_4>>Yidong Chen": 0.09678447246551514, "IJCAI2023>>program>>Main Track>>4206>>authors>>authors_2>>Hendrik Molter": 0.09983009099960327, "IJCAI2023>>program>>Journal Track>>J5939>>authors>>authors_1>>Elias Schede": 0.10047256946563721, "IJCAI2023>>program>>Main Track>>4586>>authors>>authors_5>>Shinnosuke Takamichi": 0.10155582427978516, "IJCAI2023>>program>>Main Track>>3832>>authors>>authors_6>>Felix Ulrich-Oltean": 0.10189777612686157, "IJCAI2023>>program>>Main Track>>3497>>authors>>authors_2>>Jun Yuan": 0.10190486907958984, "IJCAI2023>>program>>Main Track>>200>>authors>>authors_4>>Yiran Chen": 0.10192066431045532, "IJCAI2023>>program>>Main Track>>2230>>authors>>authors_3>>Sheila McIlraith": 0.10227745771408081, "IJCAI2023>>program>>Main Track>>3863>>authors>>authors_7>>Hua Wu": 0.10232043266296387, "IJCAI2023>>program>>Main Track>>648>>authors>>authors_8>>Yue Qi": 0.10244065523147583, "IJCAI2023>>program>>Main Track>>1621>>authors>>authors_3>>Jörg Hoffmann": 0.1024664044380188, "IJCAI2023>>program>>Main Track>>1493>>authors>>authors_3>>Xinning Zhou": 0.10260993242263794, "IJCAI2023>>program>>Main Track>>4376>>authors>>authors_1>>Zhaiming Shen": 0.10288149118423462, "IJCAI2023>>program>>Main Track>>2929>>authors>>authors_1>>Haipeng Chen": 0.10298186540603638, "IJCAI2023>>program>>Main Track>>1798>>authors>>authors_4>>Chenxi Ma": 0.10307711362838745, "IJCAI2023>>program>>Main Track>>863>>authors>>authors_5>>Kai Chen": 0.10315650701522827, "IJCAI2023>>program>>Main Track>>2705>>authors>>authors_5>>Dangyang Chen": 0.1032136082649231}, "Who are the authors of the paper 'Compositional Zero-Shot Artistic Font Synthesis'?": {"IJCAI2023>>program>>Survey Track>>SV5660>>authors>>authors_2>>Kun Zhou": 0.17548644542694092, "IJCAI2023>>program>>Special Track on AI, the Arts and Creativity>>ARTS1743>>authors>>authors_4>>Qifeng Liu": 0.1815287470817566, "IJCAI2023>>program>>Main Track>>1626>>authors>>authors_1>>Peizheng Li": 0.1826421618461609, "IJCAI2023>>program>>Main Track>>398>>authors>>authors_3>>Zhongang Qi": 0.18673264980316162, "IJCAI2023>>program>>Survey Track>>SV5569>>authors>>authors_4>>He Zhang": 0.18971872329711914, "IJCAI2023>>program>>Main Track>>4969>>authors>>authors_4>>Kai Zhao": 0.1917330026626587, "IJCAI2023>>program>>Survey Track>>SV5630>>authors>>authors_1>>Zhichun Guo": 0.19231969118118286, "IJCAI2023>>program>>Best Papers from Sister Conferences Track>>SC23>>authors>>authors_4>>Zhitang Chen": 0.19292843341827393, "IJCAI2023>>program>>Journal Track>>J5924>>authors>>authors_6>>Daniele Magazzeni": 0.1936173439025879, "IJCAI2023>>program>>Main Track>>4454>>authors>>authors_4>>Kurt Mehlhorn": 0.19403409957885742, "IJCAI2023>>program>>Special Track on AI, the Arts and Creativity>>ARTS5508>>authors>>authors_6>>Weihong Bao": 0.19455689191818237, "IJCAI2023>>program>>Main Track>>1201>>authors>>authors_1>>Bin Zhang": 0.19456052780151367, "IJCAI2023>>program>>Main Track>>2789>>authors>>authors_3>>Shuofei Qiao": 0.19472837448120117, "IJCAI2023>>program>>Survey Track>>SV5660>>authors>>authors_1>>Yifan Li": 0.19488275051116943, "IJCAI2023>>program>>Survey Track>>SV5666>>authors>>authors_3>>Junha Song": 0.19506514072418213, "IJCAI2023>>program>>Special Track on AI, the Arts and Creativity>>ARTS5672>>authors>>authors_1>>Emanuele Cosenza": 0.1951407790184021, "IJCAI2023>>calls>>Journal Track>>Accepted Papers List>>J5950>>authors>>authors_3>>Zhanhao Xiao": 0.19522690773010254, "IJCAI2023>>program>>Special Track on AI, the Arts and Creativity>>ARTS5605>>authors>>authors_5>>Florian Henkel": 0.19536292552947998}, "What is the main problem the paper 'Compositional Zero-Shot Artistic Font Synthesis' aims to address?": {"IJCAI2023>>program>>Main Track>>621>>abstract>>Recently, many researchers have made remarkable achievements in the field of artistic font synthesis, with impressive glyph style and effect style in the results. However, due to less exploration in style disentanglement, it is difficult for existing methods to envision a kind of unseen style (glyph-effect) compositions of artistic font, and thus can only learn the seen style compositions. To solve this problem, we propose a novel compositional zero-shot artistic font synthesis gan (CAFS-GAN), which allows the synthesis of unseen style compositions by exploring the visual independence and joint compatibility of encoding semantics between glyph and effect. Specifically, we propose two contrast-based style encoders to achieve style disentanglement due to glyph and effect intertwining in the image. Meanwhile, to preserve more glyph and effect detail, we propose a generator based on hierarchical dual styles AdaIN to reorganize content-styles representations from structure to texture gradually. Extensive experiments demonstrate the superiority of our model in generating high-quality artistic font images with unseen style compositions against other state-of-the-art methods. The source code and data is available at moonlight03.github.io/CAFS-GAN/.": 0.11142122745513916, "IJCAI2023>>program>>Main Track>>621>>title>>Compositional Zero-Shot Artistic Font Synthesis": 0.12394267320632935, "IJCAI2023>>program>>Main Track>>3369>>abstract>>Compositional Zero-Shot Learning (CZSL) aims to imitate the powerful generalization ability of human beings to recognize novel compositions of known primitive concepts that correspond to a state and an object, e.g., purple apple. To fully capture the intra- and inter-class correlations between compositional concepts, in this paper, we propose to learn them in a hierarchical manner. Specifically, we set up three hierarchical embedding spaces that respectively model the states, the objects, and their compositions, which serve as three “experts” that can be combined in inference for more accurate predictions. We achieve this based on the recent success of large-scale pretrained vision-language models, e.g., CLIP, which provides a strong initial knowledge of image-text relationships. To better adapt this knowledge to CZSL, we propose to learn three hierarchical prompts by explicitly fixing the unrelated word tokens in the three embedding spaces. Despite its simplicity, our proposed method consistently yields superior performance over current state-of-the-art approaches on three widely-used CZSL benchmarks.": 0.18138772249221802}, "What method is proposed in the paper 'Compositional Zero-Shot Artistic Font Synthesis' to solve the problem at hand?": {"IJCAI2023>>program>>Main Track>>621>>abstract>>Recently, many researchers have made remarkable achievements in the field of artistic font synthesis, with impressive glyph style and effect style in the results. However, due to less exploration in style disentanglement, it is difficult for existing methods to envision a kind of unseen style (glyph-effect) compositions of artistic font, and thus can only learn the seen style compositions. To solve this problem, we propose a novel compositional zero-shot artistic font synthesis gan (CAFS-GAN), which allows the synthesis of unseen style compositions by exploring the visual independence and joint compatibility of encoding semantics between glyph and effect. Specifically, we propose two contrast-based style encoders to achieve style disentanglement due to glyph and effect intertwining in the image. Meanwhile, to preserve more glyph and effect detail, we propose a generator based on hierarchical dual styles AdaIN to reorganize content-styles representations from structure to texture gradually. Extensive experiments demonstrate the superiority of our model in generating high-quality artistic font images with unseen style compositions against other state-of-the-art methods. The source code and data is available at moonlight03.github.io/CAFS-GAN/.": 0.110909104347229, "IJCAI2023>>program>>Main Track>>621>>title>>Compositional Zero-Shot Artistic Font Synthesis": 0.11942696571350098, "IJCAI2023>>program>>Main Track>>3369>>abstract>>Compositional Zero-Shot Learning (CZSL) aims to imitate the powerful generalization ability of human beings to recognize novel compositions of known primitive concepts that correspond to a state and an object, e.g., purple apple. To fully capture the intra- and inter-class correlations between compositional concepts, in this paper, we propose to learn them in a hierarchical manner. Specifically, we set up three hierarchical embedding spaces that respectively model the states, the objects, and their compositions, which serve as three “experts” that can be combined in inference for more accurate predictions. We achieve this based on the recent success of large-scale pretrained vision-language models, e.g., CLIP, which provides a strong initial knowledge of image-text relationships. To better adapt this knowledge to CZSL, we propose to learn three hierarchical prompts by explicitly fixing the unrelated word tokens in the three embedding spaces. Despite its simplicity, our proposed method consistently yields superior performance over current state-of-the-art approaches on three widely-used CZSL benchmarks.": 0.16500025987625122}, "In which fields does the paper 'Compositional Zero-Shot Artistic Font Synthesis' fall?": {"IJCAI2023>>program>>Main Track>>621>>abstract>>Recently, many researchers have made remarkable achievements in the field of artistic font synthesis, with impressive glyph style and effect style in the results. However, due to less exploration in style disentanglement, it is difficult for existing methods to envision a kind of unseen style (glyph-effect) compositions of artistic font, and thus can only learn the seen style compositions. To solve this problem, we propose a novel compositional zero-shot artistic font synthesis gan (CAFS-GAN), which allows the synthesis of unseen style compositions by exploring the visual independence and joint compatibility of encoding semantics between glyph and effect. Specifically, we propose two contrast-based style encoders to achieve style disentanglement due to glyph and effect intertwining in the image. Meanwhile, to preserve more glyph and effect detail, we propose a generator based on hierarchical dual styles AdaIN to reorganize content-styles representations from structure to texture gradually. Extensive experiments demonstrate the superiority of our model in generating high-quality artistic font images with unseen style compositions against other state-of-the-art methods. The source code and data is available at moonlight03.github.io/CAFS-GAN/.": 0.11589884757995605, "IJCAI2023>>program>>Main Track>>621>>title>>Compositional Zero-Shot Artistic Font Synthesis": 0.11601608991622925, "IJCAI2023>>program>>Main Track>>3369>>abstract>>Compositional Zero-Shot Learning (CZSL) aims to imitate the powerful generalization ability of human beings to recognize novel compositions of known primitive concepts that correspond to a state and an object, e.g., purple apple. To fully capture the intra- and inter-class correlations between compositional concepts, in this paper, we propose to learn them in a hierarchical manner. Specifically, we set up three hierarchical embedding spaces that respectively model the states, the objects, and their compositions, which serve as three “experts” that can be combined in inference for more accurate predictions. We achieve this based on the recent success of large-scale pretrained vision-language models, e.g., CLIP, which provides a strong initial knowledge of image-text relationships. To better adapt this knowledge to CZSL, we propose to learn three hierarchical prompts by explicitly fixing the unrelated word tokens in the three embedding spaces. Despite its simplicity, our proposed method consistently yields superior performance over current state-of-the-art approaches on three widely-used CZSL benchmarks.": 0.1825423240661621}, "What is the title of the paper presented in the main track of IJCAI2023?": {"IJCAI2023>>program>>Main Track>>5195>>authors>>authors_3>>Thiago D. Simão": 0.08080357313156128, "IJCAI2023>>program>>Main Track>>2178>>authors>>authors_3>>Ruben Solozabal Ochoa de Retana": 0.08126193284988403, "IJCAI2023>>program>>Main Track>>3832>>authors>>authors_6>>Felix Ulrich-Oltean": 0.08605808019638062, "IJCAI2023>>program>>Main Track>>200>>authors>>authors_4>>Yiran Chen": 0.08623206615447998, "IJCAI2023>>program>>Main Track>>1045>>authors>>authors_3>>Elisheva S. Shamash": 0.08684343099594116, "IJCAI2023>>program>>Main Track>>4586>>authors>>authors_5>>Shinnosuke Takamichi": 0.08687865734100342, "IJCAI2023>>program>>Main Track>>4376>>authors>>authors_1>>Zhaiming Shen": 0.08747780323028564, "IJCAI2023>>program>>Main Track>>1621>>authors>>authors_3>>Jörg Hoffmann": 0.08754557371139526, "IJCAI2023>>program>>Main Track>>863>>authors>>authors_5>>Kai Chen": 0.08883917331695557, "IJCAI2023>>program>>Main Track>>1798>>authors>>authors_4>>Chenxi Ma": 0.08983743190765381, "IJCAI2023>>program>>Main Track>>2276>>authors>>authors_2>>Di Jin": 0.09129822254180908, "IJCAI2023>>program>>Main Track>>2230>>authors>>authors_3>>Sheila McIlraith": 0.09148341417312622, "IJCAI2023>>program>>Main Track>>4586>>authors>>authors_1>>Takaaki Saeki": 0.09167462587356567, "IJCAI2023>>program>>Main Track>>3863>>authors>>authors_7>>Hua Wu": 0.09265697002410889, "IJCAI2023>>program>>Main Track>>1633>>authors>>authors_3>>John Dickerson": 0.0931239128112793, "IJCAI2023>>program>>Main Track>>902>>authors>>authors_1>>Tong Liu": 0.09372586011886597, "IJCAI2023>>program>>Main Track>>2705>>authors>>authors_5>>Dangyang Chen": 0.09482496976852417, "IJCAI2023>>program>>Main Track>>1883>>authors>>authors_3>>Tao Meng": 0.09486943483352661, "IJCAI2023>>program>>Main Track>>774>>authors>>authors_3>>Yabiao Wang": 0.09500634670257568, "IJCAI2023>>program>>Main Track>>4484>>authors>>authors_4>>Francesco Pasquale": 0.09519082307815552}, "Who are the authors of the paper 'Artificial Agents Inspired by Human Motivation Psychology for Teamwork in Hazardous Environments'?": {"IJCAI2023>>program>>Main Track>>1379>>authors>>authors_1>>James Kotary": 0.15719062089920044, "IJCAI2023>>program>>Journal Track>>J5924>>authors>>authors_6>>Daniele Magazzeni": 0.15795302391052246, "IJCAI2023>>program>>Survey Track>>SV5660>>authors>>authors_2>>Kun Zhou": 0.1595090627670288, "IJCAI2023>>program>>Survey Track>>SV5569>>authors>>authors_4>>He Zhang": 0.16427648067474365, "IJCAI2023>>program>>Journal Track>>J5758>>authors>>authors_2>>Matthew E. Taylor": 0.16634893417358398, "IJCAI2023>>program>>Main Track>>4454>>authors>>authors_4>>Kurt Mehlhorn": 0.1672404408454895, "IJCAI2023>>program>>Journal Track>>J5941>>authors>>authors_2>>Andrew Searns": 0.16745877265930176, "IJCAI2023>>program>>Survey Track>>SV5487>>authors>>authors_2>>Zongxiong Chen": 0.16759443283081055, "IJCAI2023>>program>>Demonstrations Track>>DM5703>>authors>>authors_5>>Manuel Gil Pérez": 0.16774219274520874, "IJCAI2023>>program>>Survey Track>>SV5660>>authors>>authors_3>>Wayne Xin Zhao": 0.16815418004989624, "IJCAI2023>>program>>Survey Track>>SV5593>>authors>>authors_1>>Chengyi Liu": 0.16854488849639893, "IJCAI2023>>program>>Journal Track>>J5919>>authors>>authors_3>>Pascal Van Hentenryck": 0.16945171356201172, "IJCAI2023>>program>>Main Track>>1834>>authors>>authors_3>>Christel Baier": 0.16970282793045044, "IJCAI2023>>program>>Main Track>>863>>authors>>authors_7>>Chang Liu": 0.16994816064834595, "IJCAI2023>>program>>Main Track>>1626>>authors>>authors_1>>Peizheng Li": 0.16997861862182617, "IJCAI2023>>program>>Special Track on AI, the Arts and Creativity>>ARTS1743>>authors>>authors_4>>Qifeng Liu": 0.17074966430664062, "IJCAI2023>>program>>Main Track>>1626>>authors>>authors_6>>Juergen Gall": 0.17094695568084717, "IJCAI2023>>program>>Main Track>>1834>>authors>>authors_2>>Simon Jantsch": 0.17117613554000854, "IJCAI2023>>program>>Survey Track>>SV5569>>authors>>authors_5>>Yunfeng Li": 0.17170196771621704}, "What are the areas of focus related to the paper 'Artificial Agents Inspired by Human Motivation Psychology for Teamwork in Hazardous Environments'?": {"IJCAI2023>>program>>Main Track>>1363>>title>>Artificial Agents Inspired by Human Motivation Psychology for Teamwork in Hazardous Environments": 0.0916748046875, "IJCAI2023>>program>>Main Track>>1363>>abstract>>Multi-agent literature explores personifying artificial agents with personality, emotions or cognitive biases to produce “typical”, believable agents. In\nthis study, we demonstrate the potential of endowing artificial agents with a motivation, using human implicit motivation psychology theory that introduces 3 motive profiles – power, achievement and affiliation, to create diverse, risk-aware agents. We first devise a framework to model these motivated agents (or agents with any inherent behavior), that can activate different strategies depending on the circumstances. We conduct experiments on a fire-fighting task domain, evaluate how motivated teams perform, and draw conclusions on appropriate team compositions to be deployed in environments with different risk levels. Our framework generates predictable agents as their resulting behaviors align with the inherent characteristics of their motives. We find that motivational diversity within teams is beneficial in dynamic collaborative environments, especially as the task risk level increases. Furthermore, we observed that the best composition in terms of the performance metrics used to evaluate team compositions, does not remain the same as the collaboration level required to achieve goals changes. These results have implications for future designs of risk-aware autonomous teams and Human-AI teams, as they highlight the prospects of creating better artificial teammates and performance gains that could be achieved through anthropomorphized motivated agents.": 0.1079549789428711, "IJCAI2023>>program>>Main Track>>1856>>abstract>>While it has long been recognized that a team of individual learning agents can be greater than the sum of its parts, recent work has shown that larger teams are not necessarily more effective than smaller ones. In this paper, we study why and under which conditions certain team structures promote effective learning for a population of individual learning agents. We show that, depending on the environment, some team structures help agents learn to specialize into specific roles, resulting in more favorable global results. However, large teams create credit assignment challenges that reduce coordination, leading to large teams performing poorly compared to smaller ones. We support our conclusions with both theoretical analysis and empirical results.": 0.1569337248802185}, "What is the key methodology used in the paper presented in the main track of IJCAI2023?": {"IJCAI2023>>program>>Main Track>>2178>>authors>>authors_3>>Ruben Solozabal Ochoa de Retana": 0.09934842586517334, "IJCAI2023>>program>>Main Track>>5195>>authors>>authors_3>>Thiago D. Simão": 0.10670393705368042, "IJCAI2023>>program>>Main Track>>4580>>authors>>authors_1>>Carlos Hernández": 0.10954099893569946, "IJCAI2023>>program>>Main Track>>1045>>authors>>authors_3>>Elisheva S. Shamash": 0.11057227849960327, "IJCAI2023>>program>>Main Track>>1633>>authors>>authors_3>>John Dickerson": 0.11110115051269531, "IJCAI2023>>program>>Main Track>>863>>authors>>authors_5>>Kai Chen": 0.11120223999023438, "IJCAI2023>>program>>Main Track>>4586>>authors>>authors_5>>Shinnosuke Takamichi": 0.11120939254760742, "IJCAI2023>>program>>Main Track>>11>>keywords>>keywords_2>>Machine Learning -> ML: Deep reinforcement learning": 0.11124223470687866, "IJCAI2023>>program>>Main Track>>1621>>authors>>authors_3>>Jörg Hoffmann": 0.11159425973892212, "IJCAI2023>>program>>Main Track>>1798>>authors>>authors_4>>Chenxi Ma": 0.11175239086151123, "IJCAI2023>>program>>Main Track>>863>>authors>>authors_8>>Jie Chen": 0.11201554536819458, "IJCAI2023>>program>>Main Track>>774>>authors>>authors_3>>Yabiao Wang": 0.11215627193450928, "IJCAI2023>>program>>Main Track>>2276>>authors>>authors_2>>Di Jin": 0.11263853311538696, "IJCAI2023>>program>>Main Track>>3832>>authors>>authors_6>>Felix Ulrich-Oltean": 0.11334562301635742, "IJCAI2023>>program>>Main Track>>435>>authors>>authors_1>>Weiyan Xie": 0.11359453201293945, "IJCAI2023>>program>>Main Track>>1593>>keywords>>keywords_2>>Computer Vision -> CV: Applications": 0.11417675018310547, "IJCAI2023>>program>>Main Track>>2230>>authors>>authors_3>>Sheila McIlraith": 0.11492979526519775, "IJCAI2023>>program>>Main Track>>1540>>authors>>authors_4>>Hao Wang": 0.11514544486999512, "IJCAI2023>>program>>Best Papers from Sister Conferences Track>>SC21>>authors>>authors_3>>Qi Zhu": 0.11545860767364502, "IJCAI2023>>program>>Main Track>>902>>authors>>authors_1>>Tong Liu": 0.11552155017852783}, "According to the paper 'Artificial Agents Inspired by Human Motivation Psychology for Teamwork in Hazardous Environments', what kind of diversity is beneficial in dynamic collaborative environments?": {"IJCAI2023>>program>>Main Track>>1363>>title>>Artificial Agents Inspired by Human Motivation Psychology for Teamwork in Hazardous Environments": 0.1341833472251892, "IJCAI2023>>program>>Main Track>>1363>>abstract>>Multi-agent literature explores personifying artificial agents with personality, emotions or cognitive biases to produce “typical”, believable agents. In\nthis study, we demonstrate the potential of endowing artificial agents with a motivation, using human implicit motivation psychology theory that introduces 3 motive profiles – power, achievement and affiliation, to create diverse, risk-aware agents. We first devise a framework to model these motivated agents (or agents with any inherent behavior), that can activate different strategies depending on the circumstances. We conduct experiments on a fire-fighting task domain, evaluate how motivated teams perform, and draw conclusions on appropriate team compositions to be deployed in environments with different risk levels. Our framework generates predictable agents as their resulting behaviors align with the inherent characteristics of their motives. We find that motivational diversity within teams is beneficial in dynamic collaborative environments, especially as the task risk level increases. Furthermore, we observed that the best composition in terms of the performance metrics used to evaluate team compositions, does not remain the same as the collaboration level required to achieve goals changes. These results have implications for future designs of risk-aware autonomous teams and Human-AI teams, as they highlight the prospects of creating better artificial teammates and performance gains that could be achieved through anthropomorphized motivated agents.": 0.13460123538970947, "IJCAI2023>>program>>Main Track>>1856>>abstract>>While it has long been recognized that a team of individual learning agents can be greater than the sum of its parts, recent work has shown that larger teams are not necessarily more effective than smaller ones. In this paper, we study why and under which conditions certain team structures promote effective learning for a population of individual learning agents. We show that, depending on the environment, some team structures help agents learn to specialize into specific roles, resulting in more favorable global results. However, large teams create credit assignment challenges that reduce coordination, leading to large teams performing poorly compared to smaller ones. We support our conclusions with both theoretical analysis and empirical results.": 0.15154796838760376, "IJCAI2023>>program>>Main Track>>4071>>title>>Generalization through Diversity: Improving Unsupervised Environment Design": 0.16771399974822998}}